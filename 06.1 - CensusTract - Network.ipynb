{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyproj import CRS\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "# set the working directory\n",
    "BASE_DIR = Path.cwd()\n",
    "# define the exported folder path\n",
    "# Check if folder exists\n",
    "folder_path = pathlib.Path(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"agg_network\"))\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "# print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nFetch SF Champ network both for year 2010, year 2016 and join the frames\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fetch SF Champ network both for year 2010, year 2016 and join the frames\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Network for YR 2010\n",
    "dfSFRd2010am = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_AM.shp\"))\n",
    "dfSFRd2010am[\"peak\"]=\"AM\"\n",
    "dfSFRd2010am[\"Tot_CAP\"]=dfSFRd2010am[\"CAP\"]*3\n",
    "\n",
    "dfSFRd2010pm = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_PM.shp\"))\n",
    "dfSFRd2010pm[\"peak\"]=\"PM\"\n",
    "dfSFRd2010pm[\"Tot_CAP\"]=dfSFRd2010pm[\"CAP\"]*3\n",
    "\n",
    "dfSFRd2010ea = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_EA.shp\"))\n",
    "dfSFRd2010ea[\"peak\"]=\"EA\"\n",
    "dfSFRd2010ea[\"Tot_CAP\"]=dfSFRd2010ea[\"CAP\"]*3\n",
    "\n",
    "dfSFRd2010ev = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_EV.shp\"))\n",
    "dfSFRd2010ev[\"peak\"]=\"EV\"\n",
    "dfSFRd2010ev[\"Tot_CAP\"]=dfSFRd2010ev[\"CAP\"]*8.5\n",
    "\n",
    "dfSFRd2010md = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_MD.shp\"))\n",
    "dfSFRd2010md[\"peak\"]=\"MD\"\n",
    "dfSFRd2010md[\"Tot_CAP\"]=dfSFRd2010md[\"CAP\"]*6.5\n",
    "\n",
    "dfSFRdNtwrk2010 = pd.concat([dfSFRd2010am,dfSFRd2010pm,dfSFRd2010ea,dfSFRd2010ev,dfSFRd2010md])\n",
    "# create empty columns: OOS, PUDO, Tot_Vol\n",
    "dfSFRdNtwrk2010 = dfSFRdNtwrk2010.assign(OOS=0, PUDO=0, Tot_Vol=0, TNC_Tot_Vol=0)\n",
    "dfSFRdNtwrk2010[\"A_B\"] = dfSFRdNtwrk2010[\"A\"].astype(str)  + \"_\" + dfSFRdNtwrk2010[\"B\"].astype(str)\n",
    "dfSFRdNtwrk2010[\"A\"] = dfSFRdNtwrk2010[\"A\"].astype(str)\n",
    "dfSFRdNtwrk2010[\"B\"] = dfSFRdNtwrk2010[\"B\"].astype(str)\n",
    "# get the columns which together formm Tot_Vol\n",
    "add_2010 = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1',\n",
    "            'BUSVOL_AM','BUSVOL_PM','BUSVOL_EA','BUSVOL_MD','BUSVOL_EV','OOS']\n",
    "# add them up\n",
    "dfSFRdNtwrk2010[\"Tot_Vol\"] = dfSFRdNtwrk2010[add_2010].sum(axis=1)\n",
    "\n",
    "# Keep only FT types representing real road-network\n",
    "# 1: Fwy-Fwy Connector; 2: Freeway; 3: Expressway; 4: Collector; 5: Ramp; 6: Centroid Connector;\n",
    "# 7: Major Arterial; 8: ; 9: Alley (only for DTA); 10: ; 11: Local; 12: Minor Arterial; 13: Bike only;\n",
    "# 14: ; 15: Super Arterial\n",
    "dfSFRdNtwrk2010=dfSFRdNtwrk2010[dfSFRdNtwrk2010.FT.isin([1,2,3,4,5,7,9,10,11,12,13,15])]\n",
    "# dfSFRdNtwrk2010=dfSFRdNtwrk2010[dfSFRdNtwrk2010.FT.isin([4,7,10,11,12,15])]\n",
    "\n",
    "# convert back to geopandas dataframe\n",
    "dfSFRdNtwrk2010 = gpd.GeoDataFrame(dfSFRdNtwrk2010, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfSFRdNtwrk2010=dfSFRdNtwrk2010.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # read the SF_Boundary file\n",
    "dfSFBoundary = gpd.read_file(BASE_DIR.parent.joinpath(\"Data\",\"SF_County\",\"SFBay_Boundary.shp\"))\n",
    "dfSFBoundary=dfSFBoundary.to_crs(\"EPSG:4326\")\n",
    "\n",
    "dfSFRdNtwrk2010.reset_index(drop=True,inplace=True)\n",
    "# # Overlay and select only links which are within the SF Bay Area Polygon\n",
    "dfSFRdNtwrk2010 = gpd.clip(dfSFRdNtwrk2010, dfSFBoundary)\n",
    "\n",
    "# dfSFRdNtwrk2010.to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb122022\",\"Raw_SFRdNtwrk_2010_cores.csv\"))\n",
    "\n",
    "# aggregate the dataframe using A_B\n",
    "wt_avg = lambda x: np.ma.average(x, weights = dfSFRdNtwrk2010.loc[x.index, \"Tot_Vol\"])\n",
    "\n",
    "lst_col = [\"SPEED\",\"TIME\",\"TIME_1\",\"CSPD_1\"]\n",
    "# average the columns\n",
    "avg_col = [ 'DISTANCE',\n",
    "            \"FT\",\"AT\",\n",
    "            'TIMESEED',\n",
    "            'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "            'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3',\n",
    "            'TOLLEA_DA', 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA',\n",
    "            'TOLLEV_SR2', 'TOLLEV_SR3',\"USE\",]\n",
    "# custom aggregate function: weighted average, summation or concatanation\n",
    "def agg_func(df):\n",
    "    d = {}\n",
    "    for col in df.select_dtypes(np.number).columns:\n",
    "        if col in lst_col:\n",
    "            d[col] = wt_avg\n",
    "        elif col in avg_col:\n",
    "            d[col]=\"mean\"\n",
    "        else:\n",
    "            d[col] = \"sum\"\n",
    "    for col in df.select_dtypes(object).columns:\n",
    "        d[col] = \"first\"\n",
    "    d[\"geometry\"] = \"first\"\n",
    "    return d\n",
    "\n",
    "dfSFRdNtwrk2010_agg = dfSFRdNtwrk2010.groupby(['A_B'],as_index=False).aggregate(agg_func(dfSFRdNtwrk2010.copy())).copy()\n",
    "# above merge converts the geo-dataframe to pandas dataframe. So re-convert it into geodataframe\n",
    "dfSFRdNtwrk2010_agg = gpd.GeoDataFrame(dfSFRdNtwrk2010_agg, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfSFRdNtwrk2010_agg=dfSFRdNtwrk2010_agg.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # Overlay and select only links which are within the SF Bay Area Polygon\n",
    "dfSFRdNtwrk2010_agg = gpd.clip(dfSFRdNtwrk2010_agg, dfSFBoundary)\n",
    "\n",
    "# export the geodataframe\n",
    "# dfSFRdNtwrk2010_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb122022\",\"SFChamp_2010_agg.geojson\"), driver='GeoJSON')\n",
    "# dfSFRdNtwrk2010_agg.to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb122022\",\"SFChamp_2010_agg.csv\"))\n",
    "\n",
    "# reproject the geodataframe to EPSG:3857\n",
    "dfSFRdNtwrk2010_agg = dfSFRdNtwrk2010_agg.to_crs(\"EPSG:3857\")\n",
    "dfSFRdNtwrk2010_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFChamp_2010_PCS.geojson\"), driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# read 2016 file\n",
    "dfSFRd2016am = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_AM.shp\"))\n",
    "dfSFRd2016am[\"peak\"]=\"AM\"\n",
    "dfSFRd2016am[\"Tot_CAP\"]=dfSFRd2016am[\"CAP\"]*3\n",
    "dfSFRd2016pm = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_PM.shp\"))\n",
    "dfSFRd2016pm[\"peak\"]=\"PM\"\n",
    "dfSFRd2016pm[\"Tot_CAP\"]=dfSFRd2016pm[\"CAP\"]*3\n",
    "dfSFRd2016ea = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_EA.shp\"))\n",
    "dfSFRd2016ea[\"peak\"]=\"EA\"\n",
    "dfSFRd2016ea[\"Tot_CAP\"]=dfSFRd2016ea[\"CAP\"]*3\n",
    "dfSFRd2016ev = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_EV.shp\"))\n",
    "dfSFRd2016ev[\"peak\"]=\"EV\"\n",
    "dfSFRd2016ev[\"Tot_CAP\"]=dfSFRd2016ev[\"CAP\"]*8.5\n",
    "dfSFRd2016md = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_MD.shp\"))\n",
    "dfSFRd2016md[\"peak\"]=\"MD\"\n",
    "dfSFRd2016md[\"Tot_CAP\"]=dfSFRd2016md[\"CAP\"]*6.5\n",
    "\n",
    "dfSFRdNtwrk2016 = pd.concat([dfSFRd2016am,dfSFRd2016pm,dfSFRd2016ea,dfSFRd2016ev,dfSFRd2016md])\n",
    "\n",
    "# create empty columns: OOS, PUDO, Tot_Vol\n",
    "dfSFRdNtwrk2016 = dfSFRdNtwrk2016.assign(Tot_Vol=0)\n",
    "dfSFRdNtwrk2016[\"A_B\"] = dfSFRdNtwrk2016[\"A\"].astype(str)  + \"_\" + dfSFRdNtwrk2016[\"B\"].astype(str)\n",
    "dfSFRdNtwrk2016[\"A\"] = dfSFRdNtwrk2016[\"A\"].astype(str)\n",
    "dfSFRdNtwrk2016[\"B\"] = dfSFRdNtwrk2016[\"B\"].astype(str)\n",
    "# get the columns which together formm Tot_Vol\n",
    "add_2016 = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1',\n",
    "            'V13_1','V14_1','V15_1','V16_1','V17_1','V18_1','V19_1',\n",
    "            'BUSVOL_AM','BUSVOL_PM','BUSVOL_EA','BUSVOL_MD','BUSVOL_EV','OOS']\n",
    "# add them up\n",
    "dfSFRdNtwrk2016[\"Tot_Vol\"] = dfSFRdNtwrk2016[add_2016].sum(axis=1)\n",
    "\n",
    "# add to TNC Vol\n",
    "# TNC_2016 = ['V16_1','V17_1','V18_1','OOS'] # updated 26 Jan 2022 to reflect TNC volumes which were mis-represented\n",
    "TNC_2016 = ['V13_1','OOS'] # V13_1 is TNC_Volumes plying on the road segment\n",
    "dfSFRdNtwrk2016[\"TNC_Tot_Vol\"] = dfSFRdNtwrk2016[TNC_2016].sum(axis=1)\n",
    "\n",
    "# Keep only FT types representing real road-network\n",
    "dfSFRdNtwrk2016=dfSFRdNtwrk2016[dfSFRdNtwrk2016.FT.isin([1,2,3,4,5,7,9,10,11,12,13,15])]\n",
    "# dfSFRdNtwrk2016=dfSFRdNtwrk2016[dfSFRdNtwrk2016.FT.isin([4,7,10,11,12,15])]\n",
    "\n",
    "# convert back to geopandas dataframe\n",
    "dfSFRdNtwrk2016 = gpd.GeoDataFrame(dfSFRdNtwrk2016, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfSFRdNtwrk2016=dfSFRdNtwrk2016.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # read the SF_Boundary file\n",
    "dfSFBoundary = gpd.read_file(BASE_DIR.parent.joinpath(\"Data\",\"SF_County\",\"SFBay_Boundary.shp\"))\n",
    "dfSFBoundary=dfSFBoundary.to_crs(\"EPSG:4326\")\n",
    "\n",
    "dfSFRdNtwrk2016.reset_index(drop=True,inplace=True)\n",
    "# # Overlay and select only links which are within the SF Bay Area Polygon\n",
    "dfSFRdNtwrk2016 = gpd.clip(dfSFRdNtwrk2016, dfSFBoundary)\n",
    "\n",
    "# dfSFRdNtwrk2016.to_csv(BASE_DIR.parent.joinpath(folder_path,\"Raw_SFRdNtwrk_2016_cores.csv\"))\n",
    "\n",
    "# aggregate the dataframe using A_B\n",
    "wt_avg = lambda x: np.ma.average(x, weights = dfSFRdNtwrk2016.loc[x.index, \"Tot_Vol\"])\n",
    "\n",
    "lst_col = [\"SPEED\",\"TIME\",\"TIME_1\",\"CSPD_1\"]\n",
    "# average the columns\n",
    "avg_col = [ 'DISTANCE',\n",
    "            \"FT\",\"AT\",\n",
    "            'TIMESEED',\n",
    "            'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "            'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3',\n",
    "            'TOLLEA_DA', 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA',\n",
    "            'TOLLEV_SR2', 'TOLLEV_SR3',\"USE\"]\n",
    "\n",
    "def agg_func(df):\n",
    "    d = {}\n",
    "    for col in df.select_dtypes(np.number).columns:\n",
    "        if col in lst_col:\n",
    "            d[col] = wt_avg\n",
    "        elif col in avg_col:\n",
    "            d[col]=\"mean\"\n",
    "        else:\n",
    "            d[col] = \"sum\"\n",
    "    for col in df.select_dtypes(object).columns:\n",
    "        d[col] = \"first\"\n",
    "    d[\"geometry\"] = \"first\"\n",
    "    return d\n",
    "\n",
    "dfSFRdNtwrk2016_agg = dfSFRdNtwrk2016.groupby(['A_B'],as_index=False).aggregate(agg_func(dfSFRdNtwrk2016.copy())).copy()\n",
    "# above merge converts the geo-dataframe to pandas dataframe. So re-convert it into geodataframe\n",
    "dfSFRdNtwrk2016_agg = gpd.GeoDataFrame(dfSFRdNtwrk2016_agg, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfSFRdNtwrk2016_agg=dfSFRdNtwrk2016_agg.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # Overlay and select only links which are within the SF Bay Area Polygon\n",
    "dfSFRdNtwrk2016_agg = gpd.clip(dfSFRdNtwrk2016_agg, dfSFBoundary)\n",
    "# export the geodataframe\n",
    "# dfSFRdNtwrk2016_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_agg.geojson\"), driver='GeoJSON')\n",
    "# dfSFRdNtwrk2016_agg.to_csv(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_agg.csv\"))\n",
    "\n",
    "# reproject the geodataframe to EPSG:3857\n",
    "dfSFRdNtwrk2016_agg = dfSFRdNtwrk2016_agg.to_crs(\"EPSG:3857\")\n",
    "dfSFRdNtwrk2016_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFChamp_2016_PCS.geojson\"), driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# # Also reproject the Census Tract file to EPSG:3857\n",
    "# dfSF_CensusTract = gpd.read_file(BASE_DIR.parent.joinpath(\"CensusTract\",\"SF_CT_2010.geojson\"))\n",
    "# dfSF_CensusTract = dfSF_CensusTract.to_crs(\"EPSG:3857\")\n",
    "# dfSF_CensusTract[\"tractce10\"]=dfSF_CensusTract[\"tractce10\"].astype(str)\n",
    "# dfSF_CensusTract.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_CensusTract_PCS.geojson\"), driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Check in QGIS if the re-projection is successful .i.e.\n",
    "# 1. Both SFChamp_2010_agg.geojson and SFChamp_2016_agg.geojson into EPSG:3857, are named with _PCS suffix\n",
    "# 2. The SF_CensusTract to EPSG:3857, is also name with _PCS suffix\n",
    "\n",
    "# After the above process do the following in QGIS\n",
    "# 3. Road Crash for each year i.e. 2010 and 2016, convert it to EPSG:3857, name it SFCrash_2010_PCS.geojson & SFCrash_2016_PCS.geojson\n",
    "# 4. Perform spatial intersection:\n",
    "        # Intersect with SF_CensusTract and RoadNetwork and name the output as SFChamp_201x_agg_CT_PCS.geojson\n",
    "        # Intersect road crashes with SF_Census Tract and name the output as SFCrash_201x_CT_PCS.geojson\n",
    "        # for both, keep \"tractce\" column from SF_Census Tract in the output file\n",
    "# This ends QGIS manipulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Fetch SF_Census Tract, SF_RoadNetwork and SF_RoadCrash\n",
    "SF_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_CensusTract_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "# road network\n",
    "dfSFRdNtwrk2010_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFChamp_2010_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2010_CT.fillna(dfSFRdNtwrk2010_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFRdNtwrk2010_CT[\"tractce10\"]=dfSFRdNtwrk2010_CT[\"tractce10\"].astype(str)\n",
    "# dfSFRdNtwrk2010_CT[\"ACCIDENT_YEAR\"] = 2010\n",
    "dfSFRdNtwrk2016_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFChamp_2016_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2016_CT.fillna(dfSFRdNtwrk2016_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFRdNtwrk2016_CT[\"tractce10\"]=dfSFRdNtwrk2016_CT[\"tractce10\"].astype(str)\n",
    "# dfSFRdNtwrk2016_CT[\"ACCIDENT_YEAR\"] = 2016\n",
    "# road crashes\n",
    "dfSFCrash2010_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFCrash_2010_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2010_CT.fillna(dfSFCrash2010_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFCrash2010_CT[\"tractce10\"]=dfSFCrash2010_CT[\"tractce10\"].astype(str)\n",
    "dfSFCrash2010_CT[\"ACCIDENT_YEAR\"] = 2010\n",
    "dfSFCrash2016_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFCrash_2016_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2016_CT.fillna(dfSFCrash2016_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFCrash2016_CT[\"ACCIDENT_YEAR\"] = 2016\n",
    "dfSFCrash2016_CT[\"tractce10\"]=dfSFCrash2016_CT[\"tractce10\"].astype(str)\n",
    "\n",
    "def add_length_columns(_df):\n",
    "    #remember to rename DISTANCE variable (as this is no longer the actual distance (in miles), given that feature is split-up)\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[\"Length_meters\"] = _df.geometry.length\n",
    "        d[\"Length_miles\"] = d[\"Length_meters\"]* 0.000621371\n",
    "\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "dfSFRdNtwrk2010_CT.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2010_CT = add_length_columns(dfSFRdNtwrk2010_CT.copy())\n",
    "\n",
    "dfSFRdNtwrk2016_CT.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2016_CT = add_length_columns(dfSFRdNtwrk2016_CT.copy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# aggregate the SFChamp network files by Census Tract\n",
    "def reqd_colmns(_df):\n",
    "    df = _df.copy()\n",
    "    d = {}\n",
    "    reqd_colmns = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                   'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                   'OOS', 'PUDO','Tot_Vol',\"TNC_Tot_Vol\",\n",
    "                   'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',\n",
    "                   ]\n",
    "    for col in reqd_colmns:\n",
    "        if col not in df.columns:\n",
    "            d[col]=0\n",
    "            # df = (df.assign(txt=0))\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "# create supplemental fields for estimation\n",
    "def get_required_fields(_df):\n",
    "    fields = [\"Tot_TNC_Vol\", \"Tot_Non_TNC_Vol\", \"Tot_VMT\",\"Tot_TNC_VMT\",\"Tot_Non_TNC_VMT\", \"Congested_Speed\",\"Tot_Vol\",\"PUDO\",\"OOS\"]\n",
    "\n",
    "    d = {}\n",
    "    for fld in fields:\n",
    "        if fld == \"Tot_TNC_Vol\":# TNC Tot Vol\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_Vol\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[\"Tot_Vol\"] - _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_VMT\":\n",
    "            d[fld] = _df[\"Tot_Vol\"]*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[\"Tot_Vol\"] - _df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Congested_Speed\":\n",
    "            d[\"Congested_Speed\"] = (((_df[\"Length_meters\"]*0.000621371).divide(_df[\"TIME_1\"]))*60)\n",
    "            d[\"Congested_Speed_yr\"] = d[\"Congested_Speed\"]\n",
    "        elif fld == \"Tot_Vol\":\n",
    "            d[\"Tot_Vol_yr\"] = (_df[\"Tot_Vol\"]*365)\n",
    "            d[\"Tot_Vol_mil\"] = _df[\"Tot_Vol\"].divide(1000000)\n",
    "            d[\"Tot_Vol_mil_yr\"] = d[\"Tot_Vol_yr\"].divide(1000000)\n",
    "            d[\"log_Tot_Vol\"] = np.log(_df[\"Tot_Vol\"]+1)\n",
    "            d[\"log_Tot_Vol_yr\"] = np.log(d[\"Tot_Vol_yr\"]+1)\n",
    "            d[\"log_Tot_Vol_mil\"] = np.log(d[\"Tot_Vol_mil\"]+1)\n",
    "            d[\"log_Tot_Vol_mil_yr\"] = np.log(d[\"Tot_Vol_mil_yr\"]+1)\n",
    "        elif fld == \"PUDO\":\n",
    "            d[\"PUDO_yr\"] = (_df[\"PUDO\"]*365)\n",
    "            d[\"PUDO_thousands\"] = _df[\"PUDO\"].divide(1000)\n",
    "            d[\"PUDO_thousands_yr\"] = d[\"PUDO_thousands\"]*365\n",
    "            d[\"PUDO_mil\"] = _df[\"PUDO\"].divide(1000000)\n",
    "            d[\"PUDO_mil_yr\"] = d[\"PUDO_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_PUDO\"] = np.log(_df[\"PUDO\"]+1)\n",
    "            d[\"log_PUDO_yr\"] = np.log(d[\"PUDO_yr\"]+1)\n",
    "            d[\"log_PUDO_thousands\"] = np.log(d[\"PUDO_thousands\"]+1)\n",
    "            d[\"log_PUDO_thousands_yr\"] = np.log(d[\"PUDO_thousands_yr\"]+1)\n",
    "            d[\"log_PUDO_mil\"] = np.log(d[\"PUDO_mil\"]+1)\n",
    "            d[\"log_PUDO_mil_yr\"] = np.log(d[\"PUDO_mil_yr\"]+1)\n",
    "        elif fld == \"OOS\":\n",
    "            d[\"OOS_yr\"] = (_df[\"OOS\"]*365)\n",
    "            d[\"OOS_thousands\"] = _df[\"OOS\"].divide(1000)\n",
    "            d[\"OOS_thousands_yr\"] = d[\"OOS_thousands\"]*365\n",
    "            d[\"OOS_mil\"] = _df[\"OOS\"].divide(1000000)\n",
    "            d[\"OOS_mil_yr\"] = d[\"OOS_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_OOS\"] = np.log(_df[\"OOS\"]+1)\n",
    "            d[\"log_OOS_yr\"] = np.log(d[\"OOS_yr\"]+1)\n",
    "            d[\"log_OOS_thousands\"] = np.log(d[\"OOS_thousands\"]+1)\n",
    "            d[\"log_OOS_thousands_yr\"] = np.log(d[\"OOS_thousands_yr\"]+1)\n",
    "            d[\"log_OOS_mil\"] = np.log(d[\"OOS_mil\"]+1)\n",
    "            d[\"log_OOS_mil_yr\"] = np.log(d[\"OOS_mil_yr\"]+1)\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "# aggregate the fields by CensusTract\n",
    "def agg_network_CT(_df):\n",
    "    _df = reqd_colmns(_df.copy())\n",
    "    _df [\"A\"] = _df[\"A\"].astype(str)\n",
    "    _df[\"B\"] = _df[\"B\"].astype(str)\n",
    "    _df [\"A_B\"] = _df[\"A_B\"].astype(str)\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    _df[\"tractce10\"] = _df[\"tractce10\"].astype(str)\n",
    "    #     _df[\"category\"] = _df[\"category\"].astype(str)\n",
    "\n",
    "    # aggregate the dataframe using A_B\n",
    "    wt_avg = lambda x: np.ma.average(x, weights = _df.loc[x.index, \"Tot_Vol\"])\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            if col in wt_col:\n",
    "                d[col] = wt_avg\n",
    "            else:\n",
    "                d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "\n",
    "    wt_col = [\"SPEED\",\"TIME\",\"CSPD_1\", 'VDT_1', 'VHT_1','VC_1',]\n",
    "    sum_col = [ \"CAP\", \"DISTANCE_MILES\",'Length_meters', 'Length_miles',\n",
    "                'V_1',\n",
    "                'V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                'VT_1',\n",
    "                'V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1',\n",
    "                'V11T_1','V12T_1',\"V13T_1\",'V14T_1', 'V15T_1',\"V16T_1\",'V17T_1', 'V18T_1',\"V19T_1\",\n",
    "                'OOS', 'PUDO',\n",
    "                'Tot_Vol',\"TNC_Tot_Vol\",\n",
    "                'TIMESEED',\"TIME_1\",\n",
    "                \"Tot_CAP\",\n",
    "                'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',]\n",
    "\n",
    "    str_col = ['tractce10']\n",
    "    concat_col = [\"A_B\",\"FT\",\"AT\"]\n",
    "    drop_col = [ 'A', 'B',\"USE\",'PER_RISE', 'ONEWAY',\"TOLL\",'PROJ', 'ACTION', 'AB','peak',\n",
    "                 'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3', 'TOLLEA_DA',\n",
    "                 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA', 'TOLLEV_SR2', 'TOLLEV_SR3',\n",
    "                 'DTA_EDIT_F', 'TOLLTIME', 'PHASE', 'AMBUSSAVE', 'MDBUSSAVE', 'PMBUSSAVE', 'EVBUSSAVE', 'EABUSSAVE', 'SPDC', 'CAPC',\n",
    "                 'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "                 'STREETNAME', 'TYPE', 'MTYPE','TSIN',\n",
    "                 'VALUETOLL_', 'PASSTHRU', 'BUSTPS_AM', 'BUSTPS_OP', 'BUSTPS_PM', 'TSVA', 'BIKE_CLASS', 'PER_RISE', 'ONEWAY',\n",
    "                 'TOLL',\n",
    "                 'TIMESEED',\n",
    "                 ]\n",
    "\n",
    "    _df.drop(drop_col,axis=1,inplace=True)\n",
    "    df = _df.groupby(['tractce10'],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "\n",
    "    return get_required_fields(df)\n",
    "\n",
    "dfSFRdNtwrk2010_CT_agg = agg_network_CT(dfSFRdNtwrk2010_CT.copy())\n",
    "dfSFRdNtwrk2010_CT_agg[\"Crash_Year\"] = 2010\n",
    "dfSFRdNtwrk2016_CT_agg = agg_network_CT(dfSFRdNtwrk2016_CT.copy())\n",
    "dfSFRdNtwrk2016_CT_agg[\"Crash_Year\"] = 2016\n",
    "\n",
    "#unwanted columns\n",
    "# unwanted_cols = ['V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1',\n",
    "#                  'V11T_1','V12T_1',\"V13T_1\",'V14T_1', 'V15T_1',\"V16T_1\",'V17T_1', 'V18T_1',\"V19T_1\",\n",
    "#                  \"FT\",\"AT\",\"A_B\"]\n",
    "\n",
    "dfSFRdNtwrk2010_CT_agg.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFChamp_2010_CT_agg_PCS.csv\"))\n",
    "dfSFRdNtwrk2016_CT_agg.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFChamp_2016_CT_agg_PCS.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# aggregate crashes along the CensusTract\n",
    "def agg_crash_CT(_df):\n",
    "    drop_fld = ['CASE_ID', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'REPORTING_DISTRICT', 'DAY_OF_WEEK', 'CHP_SHIFT', 'POPULATION', 'CNTY_CITY_LOC', 'SPECIAL_COND', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'CITY_DIVISION_LAPD', 'CHP_BEAT_CLASS', 'BEAT_NUMBER', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'WEATHER_1', 'WEATHER_2', 'STATE_HWY_IND', 'CALTRANS_COUNTY', 'CALTRANS_DISTRICT', 'STATE_ROUTE', 'ROUTE_SUFFIX', 'POSTMILE_PREFIX', 'POSTMILE', 'LOCATION_TYPE', 'RAMP_INTERSECTION', 'SIDE_OF_HWY', 'TOW_AWAY', 'COLLISION_SEVERITY','PARTY_COUNT', 'PRIMARY_COLL_FACTOR', 'PCF_CODE_OF_VIOL', 'PCF_VIOL_CATEGORY', 'PCF_VIOLATION', 'PCF_VIOL_SUBSECTION', 'HIT_AND_RUN', 'TYPE_OF_COLLISION', 'MVIW', 'PED_ACTION', 'ROAD_SURFACE', 'ROAD_COND_1', 'ROAD_COND_2', 'LIGHTING', 'CONTROL_DEVICE', 'CHP_ROAD_TYPE', 'PEDESTRIAN_ACCIDENT', 'BICYCLE_ACCIDENT', 'MOTORCYCLE_ACCIDENT', 'TRUCK_ACCIDENT', 'NOT_PRIVATE_PROPERTY', 'ALCOHOL_INVOLVED', 'STWD_VEHTYPE_AT_FAULT', 'CHP_VEHTYPE_AT_FAULT', 'PRIMARY_RAMP', 'SECONDARY_RAMP', 'LATITUDE', 'LONGITUDE', 'COUNTY', 'CITY', 'POINT_X', 'POINT_Y', 'PRIMARY_RD_3', 'SECONDARY_RD_3', ]\n",
    "    _df.drop(columns=drop_fld,inplace=True)\n",
    "    _df[\"tractce10\"] = _df[\"tractce10\"].astype(str)\n",
    "    str_col = ['tractce10']\n",
    "    sum_col = ['NUMBER_KILLED', 'NUMBER_INJURED',\n",
    "               'COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED', 'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED',\n",
    "               'Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', 'COUNT_PDO',]\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "    df = _df.groupby(['tractce10'],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "    return df\n",
    "\n",
    "dfSFCrash2010_CT_agg = agg_crash_CT(dfSFCrash2010_CT.copy())\n",
    "dfSFCrash2010_CT_agg[\"ACCIDENT_YEAR\"]=2010\n",
    "\n",
    "dfSFCrash2016_CT_agg = agg_crash_CT(dfSFCrash2016_CT.copy())\n",
    "dfSFCrash2016_CT_agg[\"ACCIDENT_YEAR\"]=2016\n",
    "\n",
    "dfSFCrash2010_CT_agg.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFCrash_2010_CT_agg_PCS.csv\"))\n",
    "dfSFCrash2016_CT_agg.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFCrash_2016_CT_agg_PCS.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "dfSF_RdNtwrk_Crash_2010 = pd.merge(dfSFRdNtwrk2010_CT_agg.copy(),dfSFCrash2010_CT_agg.copy(), left_on=\"tractce10\",right_on=\"tractce10\",how=\"left\")\n",
    "dfSF_RdNtwrk_Crash_2016 = pd.merge(dfSFRdNtwrk2016_CT_agg.copy(),dfSFCrash2016_CT_agg.copy(), left_on=\"tractce10\",right_on=\"tractce10\",how=\"left\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "SF_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_CensusTract_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "drop_cols = ['statefp10', 'mtfcc10', 'name10', 'intptlat10', 'awater10', 'namelsad10', 'funcstat10', 'aland10', 'geoid10','intptlon10', 'countyfp10','FT', 'A_B', \"AT\",]\n",
    "SF_CT[\"tractce10\"] = SF_CT[\"tractce10\"].astype(str)\n",
    "\n",
    "dfSF2010 = SF_CT.merge(dfSF_RdNtwrk_Crash_2010,on=\"tractce10\",how=\"left\")\n",
    "dfSF2010.drop(columns=drop_cols,inplace=True)\n",
    "dfSF2010[[\"Crash_Year\",\"ACCIDENT_YEAR\"]]=2010\n",
    "\n",
    "dfSF2016 = SF_CT.merge(dfSF_RdNtwrk_Crash_2016,on=\"tractce10\",how=\"left\")\n",
    "dfSF2016.drop(columns=drop_cols,inplace=True)\n",
    "dfSF2016[[\"Crash_Year\",\"ACCIDENT_YEAR\"]]=2016\n",
    "\n",
    "# dfSF2010.loc[:,~dfSF2010.columns.isin([\"geometry\"])].sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF2010_CT_agg_PCS.csv\"))\n",
    "# dfSF2016.loc[:,~dfSF2016.columns.isin([\"geometry\"])].sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF2016_CT_agg_PCS.csv\"))\n",
    "\n",
    "dfSF2010 = dfSF2010.to_crs(3857)\n",
    "dfSF2016 = dfSF2016.to_crs(3857)\n",
    "\n",
    "dfSF2010.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_2010_CT_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")\n",
    "dfSF2016.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_2016_CT_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")\n",
    "\n",
    "dfSFmerged = gpd.GeoDataFrame(pd.concat([dfSF2010,dfSF2016],ignore_index=True),crs=dfSF2010.crs)\n",
    "dfSFmerged.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "dfSFmerged.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "dfSFmerged.fillna(0, inplace=True)\n",
    "\n",
    "dfSFmerged = dfSFmerged.to_crs(3857)\n",
    "dfSFmerged.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFmerged_CT_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")\n",
    "dfSFmerged.loc[:,~dfSFmerged.columns.isin([\"geometry\"])].sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFmerged_CT_PCS.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Create difference and pct_change columns\n",
    "gdSFdb = pd.read_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFmerged_CT_PCS.csv\"))\n",
    "# Five types of crashes: 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', \"COUNT_PDO\"\n",
    "gdSFdb[\"COUNT_Fatal_and_Injury\"] = gdSFdb[\"COUNT_Fatal\"] + gdSFdb[\"COUNT_Visible_Injury\"] + gdSFdb[\"COUNT_Severe_Injury\"]+ gdSFdb[\"COUNT_Other_Injury\"]\n",
    "\n",
    "gdSFdb2010 = gdSFdb[gdSFdb[\"ACCIDENT_YEAR\"]==2010].add_suffix(\"_2010\").copy()\n",
    "gdSFdb2016 = gdSFdb[gdSFdb[\"ACCIDENT_YEAR\"]==2016].add_suffix(\"_2016\").copy()\n",
    "\n",
    "gdSFdb2010[\"tractce10_2010\"] = gdSFdb2010[\"tractce10_2010\"].astype(str)\n",
    "gdSFdb2016[\"tractce10_2016\"] = gdSFdb2016[\"tractce10_2016\"].astype(str)\n",
    "\n",
    "dfSFJoined = pd.merge(gdSFdb2010,gdSFdb2016, left_on=\"tractce10_2010\",right_on=\"tractce10_2016\",how=\"inner\")\n",
    "dfSFJoined.rename(columns={\"tractce10_2010\":\"tractce10\"},inplace=True)\n",
    "dfSFJoined[\"tractce10\"]=dfSFJoined[\"tractce10\"].astype(str)\n",
    "dfSFJoined = dfSFJoined.sort_index(axis=1)\n",
    "dfSFJoined.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_joined.csv\"))\n",
    "\n",
    "def add_column(df):\n",
    "    cols = ['CAP', 'SPEED', 'TIME', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA', 'V_1', 'TIME_1', 'VC_1', 'CSPD_1', 'VDT_1', 'VHT_1', 'V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1', 'VT_1', 'V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1', 'V11T_1', 'V12T_1', 'Tot_CAP', 'OOS', 'PUDO', 'Tot_Vol', 'TNC_Tot_Vol', 'V13_1', 'V14_1', 'V15_1', 'V16_1', 'V17_1', 'V18_1', 'V19_1', 'Tot_TNC_Vol', 'Tot_TNC_Vol_mil', 'Tot_TNC_Vol_yr', 'Tot_TNC_Vol_mil_yr', 'log_Tot_TNC_Vol', 'log_Tot_TNC_Vol_mil', 'log_Tot_TNC_Vol_mil_yr', 'Tot_Non_TNC_Vol', 'Tot_Non_TNC_Vol_mil', 'Tot_Non_TNC_Vol_yr', 'Tot_Non_TNC_Vol_mil_yr', 'log_Tot_Non_TNC_Vol', 'log_Tot_Non_TNC_Vol_mil', 'log_Tot_Non_TNC_Vol_mil_yr', 'Tot_VMT', 'Tot_VMT_mil', 'Tot_VMT_yr', 'Tot_VMT_mil_yr', 'log_Tot_VMT', 'log_Tot_VMT_mil', 'log_Tot_VMT_mil_yr', 'Tot_TNC_VMT', 'Tot_TNC_VMT_mil', 'Tot_TNC_VMT_yr', 'Tot_TNC_VMT_mil_yr', 'log_Tot_TNC_VMT', 'log_Tot_TNC_VMT_mil', 'log_Tot_TNC_VMT_mil_yr', 'Tot_Non_TNC_VMT', 'Tot_Non_TNC_VMT_mil', 'Tot_Non_TNC_VMT_yr', 'Tot_Non_TNC_VMT_mil_yr', 'log_Tot_Non_TNC_VMT', 'log_Tot_Non_TNC_VMT_mil', 'log_Tot_Non_TNC_VMT_mil_yr', 'Congested_Speed', 'Congested_Speed_yr', 'Tot_Vol_yr', 'Tot_Vol_mil', 'Tot_Vol_mil_yr', 'log_Tot_Vol', 'log_Tot_Vol_yr', 'log_Tot_Vol_mil', 'log_Tot_Vol_mil_yr', 'PUDO_yr', 'PUDO_thousands', 'PUDO_thousands_yr', 'PUDO_mil', 'PUDO_mil_yr', 'log_PUDO','log_PUDO_yr', 'log_PUDO_thousands', 'log_PUDO_thousands_yr', 'log_PUDO_mil', 'log_PUDO_mil_yr', 'OOS_yr', 'OOS_thousands', 'OOS_thousands_yr', 'OOS_mil', 'OOS_mil_yr', 'log_OOS','log_OOS_yr', 'log_OOS_thousands', 'log_OOS_thousands_yr', 'log_OOS_mil', 'log_OOS_mil_yr','NUMBER_KILLED', 'NUMBER_INJURED', 'COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED', 'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED', 'Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', 'COUNT_PDO', \"COUNT_Fatal_and_Injury\",'V13T_1', 'V14T_1', 'V15T_1', 'V16T_1', 'V17T_1', 'V18T_1', 'V19T_1']\n",
    "    d = {}\n",
    "    for col in cols:\n",
    "        d[f'{col}_{\"diff\"}'] = df[f'{col}_{\"2016\"}'] - df[f'{col}_{\"2010\"}']\n",
    "        d[f'{col}_{\"pct_change\"}'] = d[f'{col}_{\"diff\"}'].divide(df[f'{col}_{\"2010\"}'])\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "dfSFJoined = add_column(dfSFJoined)\n",
    "dfSFJoined.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "dfSFJoined.fillna(0,inplace=True)\n",
    "\n",
    "dfSFJoined.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_joined_diff_pct_chnge.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "dfSFJoined = pd.read_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_joined_diff_pct_chnge.csv\"))\n",
    "SF_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_CensusTract_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "# cols = ['statefp10', 'mtfcc10', 'name10', 'intptlat10', 'awater10', 'namelsad10', 'funcstat10', 'aland10', 'geoid10', 'intptlon10', 'countyfp10',]\n",
    "# SF_CT.drop(columns=cols,inplace=True)\n",
    "SF_CT[\"tractce10\"]=SF_CT[\"tractce10\"].astype(int).astype(str)\n",
    "# SF_CT[\"tractce10\"]=SF_CT[\"tractce10\"].astype(str)\n",
    "# dfSFJoined.rename(columns={\"tractce10_2010\":\"tractce10\"},inplace=True)\n",
    "# clmn = dfSFJoined.columns.to_list()\n",
    "# dfSFJoined[clmn] = dfSFJoined[clmn].apply(pd.to_numeric, errors='coerce')\n",
    "dfSFJoined[\"tractce10\"]=dfSFJoined[\"tractce10\"].astype(str)\n",
    "# dfSFJoined[[\"tractce10\",\"tractce10_2016\"]]=dfSFJoined[[\"tractce10\",\"tractce10_2016\"]].astype(str)\n",
    "SF_CT = SF_CT.to_crs(3857)\n",
    "SF_CT.merge(dfSFJoined,left_on=\"tractce10\",right_on=\"tractce10\",how=\"left\").to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_joined_diff_pct_chnge_CT_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nPerform analysis based on Facility Type\\nFT = 1:Fwy-Fwy Connector, 2:Freeway, 3:Expressway, 4:Collector, 5:Ramp, 6:Centroid Connector, 7:Major Arterial, 8:Not used,\\n9:Alley, 10:Metered Ramp, 11:Local, 12:Minor Arterial,13:Bike-Only!, 14:Not used, 15:Super Arterial,\\nSegregate the road network into three categories\\n1. Category 1 = contains FT = [1, 2, 3, 5]\\n2. Category 2 = contains FT = [4,7,12,,13,15]\\n3. Category 3 = contains FT = [9,11]\\n'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform analysis based on Facility Type\n",
    "FT = 1:Fwy-Fwy Connector, 2:Freeway, 3:Expressway, 4:Collector, 5:Ramp, 6:Centroid Connector, 7:Major Arterial, 8:Not used,\n",
    "9:Alley, 10:Metered Ramp, 11:Local, 12:Minor Arterial,13:Bike-Only!, 14:Not used, 15:Super Arterial,\n",
    "Segregate the road network into three categories\n",
    "1. Category 1 = contains FT = [1, 2, 3, 5]\n",
    "2. Category 2 = contains FT = [4,7,12,13,15]\n",
    "3. Category 3 = contains FT = [9,11]\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# modify the dataframe to create a new column \"CATEGORY\" using \"FacilityType\"\n",
    "def label_df_by_road_category(_df,fld):\n",
    "    _df[\"category\"]=0\n",
    "    _df.loc[_df[fld].isin([1, 2, 3, 5,13]),'category']=1\n",
    "    _df.loc[_df[fld].isin([4,7,12,15]),'category']=2\n",
    "    _df.loc[_df[fld].isin([9,11 ]),'category']=3\n",
    "    return _df\n",
    "\n",
    "# Fetch SF_Census Tract\n",
    "SF_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SF_CensusTract_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "SF_CT = SF_CT.to_crs(3857)\n",
    "\n",
    "# road network\n",
    "dfSFRdNtwrk2010_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFChamp_2010_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2010_CT.fillna(dfSFRdNtwrk2010_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFRdNtwrk2010_cat_CT =  label_df_by_road_category(dfSFRdNtwrk2010_CT.copy(),\"FT\")\n",
    "dfSFRdNtwrk2010_cat_CT = dfSFRdNtwrk2010_cat_CT.to_crs(3857)\n",
    "\n",
    "# dfSFRdNtwrk2010_CT[\"ACCIDENT_YEAR\"] = 2010\n",
    "dfSFRdNtwrk2016_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFChamp_2016_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2016_CT.fillna(dfSFRdNtwrk2016_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFRdNtwrk2016_cat_CT =  label_df_by_road_category(dfSFRdNtwrk2016_CT.copy(),\"FT\")\n",
    "dfSFRdNtwrk2016_cat_CT = dfSFRdNtwrk2016_cat_CT.to_crs(3857)\n",
    "\n",
    "def add_column(df):\n",
    "    d = {}\n",
    "    d['COUNT_Fatal_and_Injury'] = df['COUNT_Fatal'] + df['COUNT_Visible_Injury'] + df['COUNT_Severe_Injury'] + df['COUNT_Other_Injury']\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "# road crashes\n",
    "dfSFCrash2010_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"NN_SFCrash_SFChamp_2010_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2010_CT.fillna(dfSFCrash2010_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFCrash2010_CT = add_column(dfSFCrash2010_CT)\n",
    "dfSFCrash2010_CT[\"ACCIDENT_YEAR\"] = 2010\n",
    "dfSFCrash2010_CT= dfSFCrash2010_CT.loc[dfSFCrash2010_CT[\"D2NL\"]<10,:]\n",
    "dfSFCrash2010_cat_CT =  label_df_by_road_category(dfSFCrash2010_CT.copy(),\"FT\")\n",
    "dfSFCrash2010_cat_CT = dfSFCrash2010_cat_CT.to_crs(3857)\n",
    "\n",
    "dfSFCrash2016_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"NN_SFCrash_SFChamp_2016_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2016_CT.fillna(dfSFCrash2016_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFCrash2016_CT = add_column(dfSFCrash2016_CT)\n",
    "dfSFCrash2016_CT[\"ACCIDENT_YEAR\"] = 2016\n",
    "dfSFCrash2016_CT= dfSFCrash2016_CT.loc[dfSFCrash2016_CT[\"D2NL\"]<10,:]\n",
    "dfSFCrash2016_cat_CT =  label_df_by_road_category(dfSFCrash2016_CT.copy(),\"FT\")\n",
    "dfSFCrash2016_cat_CT = dfSFCrash2016_cat_CT.to_crs(3857)\n",
    "\n",
    "#remember to rename DISTANCE variable (as this is no longer the actual distance (in miles), given that feature is split-up)\n",
    "def add_length_columns(_df):\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[\"Length_meters\"] = _df.geometry.length\n",
    "        d[\"Length_miles\"] = d[\"Length_meters\"]* 0.000621371\n",
    "\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "dfSFRdNtwrk2010_cat_CT.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2010_cat_CT = add_length_columns(dfSFRdNtwrk2010_cat_CT.copy())\n",
    "dfSFRdNtwrk2016_cat_CT.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2016_cat_CT = add_length_columns(dfSFRdNtwrk2016_cat_CT.copy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# aggregate the SFChamp network files by TAZ by category\n",
    "def reqd_colmns(_df):# road network\n",
    "    df = _df.copy()\n",
    "    d = {}\n",
    "    reqd_colmns = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                   'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                   'OOS', 'PUDO','Tot_Vol',\"TNC_Tot_Vol\",\n",
    "                   'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',\n",
    "                   ]\n",
    "    for col in reqd_colmns:\n",
    "        if col not in df.columns:\n",
    "            d[col]=0\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "def get_req_fields(_df):\n",
    "    fields = [\"Tot_TNC_Vol\", \"Tot_Non_TNC_Vol\", \"Tot_VMT\",\"Tot_TNC_VMT\",\"Tot_Non_TNC_VMT\", \"Congested_Speed\",\"Tot_Vol\",\"PUDO\",\"OOS\"]\n",
    "    d = {}\n",
    "    for fld in fields:\n",
    "        if fld == \"Tot_TNC_Vol\":# TNC Tot Vol\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_Vol\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[\"Tot_Vol\"] - _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_VMT\":\n",
    "            d[fld] = _df[\"Tot_Vol\"]*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[\"Tot_Vol\"] - _df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Congested_Speed\":\n",
    "            d[\"Congested_Speed\"] = (((_df[\"Length_meters\"]*0.000621371).divide(_df[\"TIME_1\"]))*60)\n",
    "            d[\"Congested_Speed_yr\"] = d[\"Congested_Speed\"]\n",
    "        elif fld == \"Tot_Vol\":\n",
    "            d[\"Tot_Vol_yr\"] = (_df[\"Tot_Vol\"]*365)\n",
    "            d[\"Tot_Vol_mil\"] = _df[\"Tot_Vol\"].divide(1000000)\n",
    "            d[\"Tot_Vol_mil_yr\"] = d[\"Tot_Vol_yr\"].divide(1000000)\n",
    "            d[\"log_Tot_Vol\"] = np.log(_df[\"Tot_Vol\"]+1)\n",
    "            d[\"log_Tot_Vol_yr\"] = np.log(d[\"Tot_Vol_yr\"]+1)\n",
    "            d[\"log_Tot_Vol_mil\"] = np.log(d[\"Tot_Vol_mil\"]+1)\n",
    "            d[\"log_Tot_Vol_mil_yr\"] = np.log(d[\"Tot_Vol_mil_yr\"]+1)\n",
    "        elif fld == \"PUDO\":\n",
    "            d[\"PUDO_yr\"] = (_df[\"PUDO\"]*365)\n",
    "            d[\"PUDO_thousands\"] = _df[\"PUDO\"].divide(1000)\n",
    "            d[\"PUDO_thousands_yr\"] = d[\"PUDO_thousands\"]*365\n",
    "            d[\"PUDO_mil\"] = _df[\"PUDO\"].divide(1000000)\n",
    "            d[\"PUDO_mil_yr\"] = d[\"PUDO_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_PUDO\"] = np.log(_df[\"PUDO\"]+1)\n",
    "            d[\"log_PUDO_yr\"] = np.log(d[\"PUDO_yr\"]+1)\n",
    "            d[\"log_PUDO_thousands\"] = np.log(d[\"PUDO_thousands\"]+1)\n",
    "            d[\"log_PUDO_thousands_yr\"] = np.log(d[\"PUDO_thousands_yr\"]+1)\n",
    "            d[\"log_PUDO_mil\"] = np.log(d[\"PUDO_mil\"]+1)\n",
    "            d[\"log_PUDO_mil_yr\"] = np.log(d[\"PUDO_mil_yr\"]+1)\n",
    "        elif fld == \"OOS\":\n",
    "            d[\"OOS_yr\"] = (_df[\"OOS\"]*365)\n",
    "            d[\"OOS_thousands\"] = _df[\"OOS\"].divide(1000)\n",
    "            d[\"OOS_thousands_yr\"] = d[\"OOS_thousands\"]*365\n",
    "            d[\"OOS_mil\"] = _df[\"OOS\"].divide(1000000)\n",
    "            d[\"OOS_mil_yr\"] = d[\"OOS_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_OOS\"] = np.log(_df[\"OOS\"]+1)\n",
    "            d[\"log_OOS_yr\"] = np.log(d[\"OOS_yr\"]+1)\n",
    "            d[\"log_OOS_thousands\"] = np.log(d[\"OOS_thousands\"]+1)\n",
    "            d[\"log_OOS_thousands_yr\"] = np.log(d[\"OOS_thousands_yr\"]+1)\n",
    "            d[\"log_OOS_mil\"] = np.log(d[\"OOS_mil\"]+1)\n",
    "            d[\"log_OOS_mil_yr\"] = np.log(d[\"OOS_mil_yr\"]+1)\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "def agg_network(_df):\n",
    "    _df = reqd_colmns(_df.copy())\n",
    "    _df [\"A\"] = _df[\"A\"].astype(str)\n",
    "    _df[\"B\"] = _df[\"B\"].astype(str)\n",
    "    _df [\"A_B\"] = _df[\"A_B\"].astype(str)\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    _df[\"tractce10\"] = _df[\"tractce10\"].astype(str)\n",
    "    # aggregate the dataframe using A_B\n",
    "    wt_avg = lambda x: np.ma.average(x, weights = _df.loc[x.index, \"Tot_Vol\"])\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "\n",
    "    def agg_func_rdntwrk(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            if col in wt_col:\n",
    "                d[col] = wt_avg\n",
    "            else:\n",
    "                d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "\n",
    "    wt_col = [\"SPEED\",\"TIME\",\"CSPD_1\", 'VDT_1', 'VHT_1','VC_1', ]\n",
    "    sum_col = [ \"CAP\", \"DISTANCE_MILES\",'Length_meters', 'Length_miles',\n",
    "                'V_1',\n",
    "                'V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                'VT_1',\n",
    "                'V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1',\n",
    "                'V11T_1','V12T_1',\"V13T_1\",'V14T_1', 'V15T_1',\"V16T_1\",'V17T_1', 'V18T_1',\"V19T_1\",\n",
    "                'OOS', 'PUDO',\n",
    "                'Tot_Vol',\"TNC_Tot_Vol\",\n",
    "               'TIMESEED',\"TIME_1\",\n",
    "                \"Tot_CAP\",\n",
    "                'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',]\n",
    "\n",
    "    str_col = ['tractce10']\n",
    "    concat_col = [\"FT\",]\n",
    "    drop_col = [ 'A', 'B',\"USE\",'PER_RISE', 'ONEWAY',\"TOLL\",'PROJ', 'ACTION', 'AB','peak',\n",
    "                 'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3', 'TOLLEA_DA',\n",
    "                 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA', 'TOLLEV_SR2', 'TOLLEV_SR3',\n",
    "                 'DTA_EDIT_F', 'TOLLTIME', 'PHASE', 'AMBUSSAVE', 'MDBUSSAVE', 'PMBUSSAVE', 'EVBUSSAVE', 'EABUSSAVE', 'SPDC', 'CAPC',\n",
    "                 'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "                 'STREETNAME', 'TYPE', 'MTYPE','TSIN',\n",
    "                 'VALUETOLL_', 'PASSTHRU', 'BUSTPS_AM', 'BUSTPS_OP', 'BUSTPS_PM', 'TSVA', 'BIKE_CLASS', 'PER_RISE', 'ONEWAY',\n",
    "                 'TOLL',\n",
    "                 'TIMESEED',\"A_B\",\"AT\"\n",
    "                 ]\n",
    "\n",
    "    _df.drop(drop_col,axis=1,inplace=True)\n",
    "    df = _df.groupby(['tractce10'],as_index=False).aggregate(agg_func_rdntwrk(_df.copy())).copy()\n",
    "\n",
    "    return get_req_fields(df)\n",
    "\n",
    "def agg_crash(_df):\n",
    "    drop_fld = ['CASE_ID', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'REPORTING_DISTRICT', 'DAY_OF_WEEK', 'CHP_SHIFT', 'POPULATION', 'CNTY_CITY_LOC', 'SPECIAL_COND', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'CITY_DIVISION_LAPD', 'CHP_BEAT_CLASS', 'BEAT_NUMBER', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'WEATHER_1', 'WEATHER_2', 'STATE_HWY_IND', 'CALTRANS_COUNTY', 'CALTRANS_DISTRICT', 'STATE_ROUTE', 'ROUTE_SUFFIX', 'POSTMILE_PREFIX', 'POSTMILE', 'LOCATION_TYPE', 'RAMP_INTERSECTION', 'SIDE_OF_HWY', 'TOW_AWAY', 'COLLISION_SEVERITY','PARTY_COUNT', 'PRIMARY_COLL_FACTOR', 'PCF_CODE_OF_VIOL', 'PCF_VIOL_CATEGORY', 'PCF_VIOLATION', 'PCF_VIOL_SUBSECTION', 'HIT_AND_RUN', 'TYPE_OF_COLLISION', 'MVIW', 'PED_ACTION', 'ROAD_SURFACE', 'ROAD_COND_1', 'ROAD_COND_2', 'LIGHTING', 'CONTROL_DEVICE', 'CHP_ROAD_TYPE', 'PEDESTRIAN_ACCIDENT', 'BICYCLE_ACCIDENT', 'MOTORCYCLE_ACCIDENT', 'TRUCK_ACCIDENT', 'NOT_PRIVATE_PROPERTY', 'ALCOHOL_INVOLVED', 'STWD_VEHTYPE_AT_FAULT', 'CHP_VEHTYPE_AT_FAULT', 'PRIMARY_RAMP', 'SECONDARY_RAMP', 'LATITUDE', 'LONGITUDE', 'COUNTY', 'CITY', 'POINT_X', 'POINT_Y', 'PRIMARY_RD_3', 'SECONDARY_RD_3',]\n",
    "    _df.drop(columns=drop_fld,inplace=True)\n",
    "    str_col = [\"FT\",\"join_tractce10\",\"tractce10\"]\n",
    "    sum_col = ['NUMBER_KILLED', 'NUMBER_INJURED',\n",
    "               'COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED', 'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED',\n",
    "               'Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', 'COUNT_PDO',\"COUNT_Fatal_and_Injury\"]\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    _df[\"join_tractce10\"] = _df[\"join_tractce10\"].astype(str)\n",
    "    _df[\"tractce10\"] = _df[\"tractce10\"].astype(str)\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "    df = _df.groupby(['tractce10'],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "    return df\n",
    "\n",
    "def perform_merge(_dfrdntwrk,_dfcrash,_dfSFTAZ,crash_yr):\n",
    "    dfrdntwrk = agg_network(_dfrdntwrk)\n",
    "    dfcrash = agg_crash(_dfcrash)\n",
    "    dfcrash[\"ACCIDENT_YEAR\"]=crash_yr\n",
    "\n",
    "    dfrdntwrk_crash = pd.merge(dfrdntwrk,dfcrash, left_on=\"tractce10\",right_on=\"join_tractce10\",how=\"left\")\n",
    "    dfrdntwrk_crash.rename(columns={\"tractce10_x\":\"tractce10\"},inplace=True)\n",
    "\n",
    "    col_drop = ['statefp10', 'mtfcc10', 'name10', 'intptlat10', 'awater10', 'namelsad10', 'funcstat10', 'aland10', 'geoid10','intptlon10', 'countyfp10',]\n",
    "\n",
    "    dfSFTAZ = _dfSFTAZ.copy()\n",
    "    dfSFTAZ[\"tractce10\"]=dfSFTAZ[\"tractce10\"].astype(str)\n",
    "\n",
    "    dfrdntwrk_crash_taz = dfSFTAZ.merge(dfrdntwrk_crash,left_on=\"tractce10\",right_on=\"tractce10\",how=\"left\")\n",
    "    dfrdntwrk_crash_taz.drop(columns=col_drop,inplace=True)\n",
    "    dfrdntwrk_crash_taz.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # Fill NaN for all strings column with NaN and for all numeric columns with 0\n",
    "    d = {**dict.fromkeys(dfrdntwrk_crash_taz.select_dtypes(np.number).columns, 0),\n",
    "         **dict.fromkeys(dfrdntwrk_crash_taz.select_dtypes(exclude=np.number).columns, '')}\n",
    "    dfrdntwrk_crash_taz = dfrdntwrk_crash_taz.fillna(d)\n",
    "\n",
    "    drop_clmn = ['category_x', 'category_y', 'tractce10_y', 'FT_y', 'join_tractce10', 'FT_x', ]\n",
    "    dfrdntwrk_crash_taz.drop(columns=drop_clmn, inplace=True)\n",
    "    dfrdntwrk_crash_taz[\"tractce10\"]=dfrdntwrk_crash_taz[\"tractce10\"].astype(\"int\")\n",
    "    # dfrdntwrk_crash_taz = dfrdntwrk_crash_taz.loc[dfrdntwrk_crash_taz[\"tractce10\"].between(0,981)]\n",
    "    dfrdntwrk_crash_taz[\"ACCIDENT_YEAR\"]=crash_yr\n",
    "    dfrdntwrk_crash_taz[\"Crash_Year\"]=crash_yr\n",
    "    # dfrdntwrk_crash_taz.loc[:,~dfrdntwrk_crash_taz.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFmerged_TAZ_cat_1_PCS.csv\"))\n",
    "\n",
    "    return dfrdntwrk_crash_taz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def add_column(df):\n",
    "    cols = ['CAP', 'SPEED', 'TIME', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA', 'V_1', 'TIME_1', 'VC_1', 'CSPD_1', 'VDT_1', 'VHT_1', 'V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1', 'VT_1', 'V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1', 'V11T_1', 'V12T_1', 'Tot_CAP', 'OOS', 'PUDO', 'Tot_Vol', 'TNC_Tot_Vol', 'V13_1', 'V14_1', 'V15_1', 'V16_1', 'V17_1', 'V18_1', 'V19_1', 'Tot_TNC_Vol', 'Tot_TNC_Vol_mil', 'Tot_TNC_Vol_yr', 'Tot_TNC_Vol_mil_yr', 'log_Tot_TNC_Vol', 'log_Tot_TNC_Vol_mil', 'log_Tot_TNC_Vol_mil_yr', 'Tot_Non_TNC_Vol', 'Tot_Non_TNC_Vol_mil', 'Tot_Non_TNC_Vol_yr', 'Tot_Non_TNC_Vol_mil_yr', 'log_Tot_Non_TNC_Vol', 'log_Tot_Non_TNC_Vol_mil', 'log_Tot_Non_TNC_Vol_mil_yr', 'Tot_VMT', 'Tot_VMT_mil', 'Tot_VMT_yr', 'Tot_VMT_mil_yr', 'log_Tot_VMT', 'log_Tot_VMT_mil', 'log_Tot_VMT_mil_yr', 'Tot_TNC_VMT', 'Tot_TNC_VMT_mil', 'Tot_TNC_VMT_yr', 'Tot_TNC_VMT_mil_yr', 'log_Tot_TNC_VMT', 'log_Tot_TNC_VMT_mil', 'log_Tot_TNC_VMT_mil_yr', 'Tot_Non_TNC_VMT', 'Tot_Non_TNC_VMT_mil', 'Tot_Non_TNC_VMT_yr', 'Tot_Non_TNC_VMT_mil_yr', 'log_Tot_Non_TNC_VMT', 'log_Tot_Non_TNC_VMT_mil', 'log_Tot_Non_TNC_VMT_mil_yr', 'Congested_Speed', 'Congested_Speed_yr', 'Tot_Vol_yr', 'Tot_Vol_mil', 'Tot_Vol_mil_yr', 'log_Tot_Vol', 'log_Tot_Vol_yr', 'log_Tot_Vol_mil', 'log_Tot_Vol_mil_yr', 'PUDO_yr', 'PUDO_thousands', 'PUDO_thousands_yr', 'PUDO_mil', 'PUDO_mil_yr', 'log_PUDO_yr', 'log_PUDO_thousands', 'log_PUDO_thousands_yr', 'log_PUDO_mil', 'log_PUDO_mil_yr', 'OOS_yr', 'OOS_thousands', 'OOS_thousands_yr', 'OOS_mil', 'OOS_mil_yr', 'log_OOS_yr', 'log_OOS_thousands', 'log_OOS_thousands_yr', 'log_OOS_mil', 'log_OOS_mil_yr','NUMBER_KILLED', 'NUMBER_INJURED', 'COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED', 'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED', 'Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', 'COUNT_PDO', \"COUNT_Fatal_and_Injury\",'V13T_1', 'V14T_1', 'V15T_1', 'V16T_1', 'V17T_1', 'V18T_1', 'V19T_1',]\n",
    "    d = {}\n",
    "    for col in cols:\n",
    "        d[f'{col}_{\"diff\"}'] = df[f'{col}_{\"2016\"}'] - df[f'{col}_{\"2010\"}']\n",
    "        d[f'{col}_{\"pct_change\"}'] = d[f'{col}_{\"diff\"}'].divide(df[f'{col}_{\"2010\"}'])\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "def create_pct_change_files(_dfmerged,_dfSFTAZ,cat):\n",
    "    dfmerged = _dfmerged\n",
    "    dfSFTAZ = _dfSFTAZ\n",
    "\n",
    "    gdSFdb2010 = dfmerged[dfmerged[\"ACCIDENT_YEAR\"]==2010].add_suffix(\"_2010\").copy()\n",
    "    gdSFdb2016 = dfmerged[dfmerged[\"ACCIDENT_YEAR\"]==2016].add_suffix(\"_2016\").copy()\n",
    "\n",
    "    df_joined = pd.merge(gdSFdb2010, gdSFdb2016, left_on=\"tractce10_2010\", right_on=\"tractce10_2016\", how=\"inner\")\n",
    "    df_joined.rename(columns={\"tractce10_2010\": \"tractce10\"}, inplace=True)\n",
    "    df_joined[\"tractce10\"]=df_joined[\"tractce10\"].astype(str)\n",
    "    df_joined = df_joined.sort_index(axis=1)\n",
    "    df_joined.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path, \"Feb162022\", f\"SF_joined_cat_{cat}.csv\"))\n",
    "\n",
    "    df_joined = add_column(df_joined)\n",
    "    df_joined.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_joined.fillna(0, inplace=True)\n",
    "    df_joined.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path, \"Feb162022\", f\"SF_joined_cat_{cat}_diff_pct_chnge.csv\"))\n",
    "\n",
    "    df_joined = pd.read_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",f\"SF_joined_cat_{cat}_diff_pct_chnge.csv\"))\n",
    "\n",
    "    # cols = ['statefp10', 'mtfcc10', 'name10', 'intptlat10', 'awater10', 'namelsad10', 'funcstat10', 'aland10', 'geoid10', 'intptlon10', 'countyfp10',]\n",
    "    # SF_CT.drop(columns=cols,inplace=True)\n",
    "    dfSFTAZ[\"tractce10\"]=dfSFTAZ[\"tractce10\"].astype(int).astype(str)\n",
    "\n",
    "    # dfSFJoined.rename(columns={\"tractce10_2010\":\"tractce10\"},inplace=True)\n",
    "    # clmn = dfSFJoined.columns.to_list()\n",
    "    # dfSFJoined[clmn] = dfSFJoined[clmn].apply(pd.to_numeric, errors='coerce')\n",
    "    df_joined[\"tractce10\"]=df_joined[\"tractce10\"].astype(str)\n",
    "    # dfSFJoined[[\"tractce10\",\"tractce10_2016\"]]=dfSFJoined[[\"tractce10\",\"tractce10_2016\"]].astype(str)\n",
    "    dfSFTAZ.merge(df_joined,left_on=\"tractce10\",right_on=\"tractce10\",how=\"left\").to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",f\"SF_joined_cat_{cat}_diff_pct_chnge_CT_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\numpy\\ma\\extras.py:623: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\numpy\\ma\\extras.py:623: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\numpy\\ma\\extras.py:623: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "def perform_manipulation_by_category(_dfrdntwrk2010,_dfrdntwrk2016,_dfcrash2010,_dfcrash2016,_dfSF_CT,cat):\n",
    "    dfSF2010 = perform_merge(_dfrdntwrk2010,_dfcrash2010,_dfSF_CT,2010)\n",
    "    dfSF2016 = perform_merge(_dfrdntwrk2016,_dfcrash2016,_dfSF_CT,2016)\n",
    "    dfSFmerged = gpd.GeoDataFrame(pd.concat([dfSF2010,dfSF2016],ignore_index=True),crs=dfSF2010.crs)\n",
    "    dfSFmerged = dfSFmerged.set_crs(3857)\n",
    "    dfSFmerged.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",f\"SFmerged_CT_cat_{cat}_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")\n",
    "    dfSFmerged.loc[:,~dfSFmerged.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",f\"SFmerged_CT_cat_{cat}_PCS.csv\"))\n",
    "    create_pct_change_files(dfSFmerged.loc[:,~dfSFmerged.columns.isin([\"geometry\"])],_dfSF_CT,cat)\n",
    "    # return dfSF2010,dfSF2016\n",
    "\n",
    "for cat in [1,2,3]:\n",
    "    dfMerged = perform_manipulation_by_category(dfSFRdNtwrk2010_cat_CT.loc[dfSFRdNtwrk2010_cat_CT[\"category\"]==cat,:].copy(),\n",
    "                                                dfSFRdNtwrk2016_cat_CT.loc[dfSFRdNtwrk2016_cat_CT[\"category\"]==cat,:].copy(),\n",
    "                                                dfSFCrash2010_cat_CT.loc[dfSFCrash2010_cat_CT[\"category\"]==cat,:].copy(),\n",
    "                                                dfSFCrash2016_cat_CT.loc[dfSFCrash2016_cat_CT[\"category\"]==cat,:].copy(),\n",
    "                                                SF_CT.copy(),\n",
    "                                                cat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}