{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyproj import CRS\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "# set the working directory\n",
    "BASE_DIR = Path.cwd()\n",
    "# define the exported folder path\n",
    "# Check if folder exists\n",
    "folder_path = pathlib.Path(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"agg_network\"))\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "# print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nThis notebook is for undertaking TAZ level of analysis.\\nIt assumes that QGIS level of analysis is been completed and following files are made available\\n1. The projection is EPSG: 3857\\n2. RdNetwork files are named in the SFChamp_201x_PCS.geojson format\\n3. RoadCrash files are named in the SFCrash_201x_PCS.geojson format\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook is for undertaking TAZ level of analysis.\n",
    "It assumes that QGIS level of analysis is been completed and following files are made available\n",
    "1. The projection is EPSG: 3857\n",
    "2. RdNetwork files are named in the SFChamp_201x_PCS.geojson format\n",
    "3. RoadCrash files are named in the SFCrash_201x_PCS.geojson format\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\geodataframe.py:1351: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\geodataframe.py:1351: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\geodataframe.py:1351: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "# Fetch SF_Census Tract, SF_RoadNetwork and SF_RoadCrash\n",
    "SF_TAZ = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "# road network\n",
    "dfSFRdNtwrk2010_taz = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFChamp_2010_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2010_taz.fillna(dfSFRdNtwrk2010_taz.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFRdNtwrk2010_taz[\"TAZ\"]=dfSFRdNtwrk2010_taz[\"TAZ\"].astype(str)\n",
    "\n",
    "# dfSFRdNtwrk2010_CT[\"ACCIDENT_YEAR\"] = 2010\n",
    "dfSFRdNtwrk2016_taz = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFChamp_2016_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2016_taz.fillna(dfSFRdNtwrk2016_taz.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFRdNtwrk2016_taz[\"TAZ\"]=dfSFRdNtwrk2016_taz[\"TAZ\"].astype(str)\n",
    "\n",
    "# dfSFRdNtwrk2016_CT[\"ACCIDENT_YEAR\"] = 2016\n",
    "# road crashes\n",
    "dfSFCrash2010_taz = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFCrash_2010_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2010_taz.fillna(dfSFCrash2010_taz.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFCrash2010_taz[\"TAZ\"]=dfSFCrash2010_taz[\"TAZ\"].astype(str)\n",
    "dfSFCrash2010_taz[\"ACCIDENT_YEAR\"] = 2010\n",
    "\n",
    "dfSFCrash2016_taz = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFCrash_2016_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2016_taz.fillna(dfSFCrash2016_taz.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFCrash2016_taz[\"ACCIDENT_YEAR\"] = 2016\n",
    "dfSFCrash2016_taz[\"TAZ\"]=dfSFCrash2016_taz[\"TAZ\"].astype(str)\n",
    "\n",
    "#remember to rename DISTANCE variable (as this is no longer the actual distance (in miles), given that feature is split-up)\n",
    "dfSFRdNtwrk2010_taz.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2010_taz[\"Length_meters\"] = dfSFRdNtwrk2010_taz.geometry.length\n",
    "dfSFRdNtwrk2010_taz[\"Length_miles\"] = dfSFRdNtwrk2010_taz[\"Length_meters\"]* 0.000621371\n",
    "dfSFRdNtwrk2016_taz.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2016_taz[\"Length_meters\"] = dfSFRdNtwrk2016_taz.geometry.length\n",
    "dfSFRdNtwrk2010_taz[\"Length_miles\"] = dfSFRdNtwrk2016_taz[\"Length_meters\"] * 0.000621371"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# aggregate the SFChamp network files by TAZ\n",
    "def reqd_colmns(_df):\n",
    "    df = _df.copy()\n",
    "    d = {}\n",
    "    reqd_colmns = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                   'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                   'OOS', 'PUDO','Tot_Vol',\"TNC_Tot_Vol\",\n",
    "                   'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',\n",
    "                   ]\n",
    "    for col in reqd_colmns:\n",
    "        if col not in df.columns:\n",
    "            txt= col\n",
    "            d[col]=0\n",
    "            # df = (df.assign(txt=0))\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "def get_required_fields(_df):\n",
    "    fields = [\"Tot_TNC_Vol\", \"Tot_Non_TNC_Vol\", \"Tot_VMT\",\"Tot_TNC_VMT\",\"Tot_Non_TNC_VMT\", \"Congested_Speed\",\"Tot_Vol\",\"PUDO\",\"OOS\"]\n",
    "    d = {}\n",
    "    for fld in fields:\n",
    "        if fld == \"Tot_TNC_Vol\":# TNC Tot Vol\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_Vol\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[\"Tot_Vol\"] - _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_VMT\":\n",
    "            d[fld] = _df[\"Tot_Vol\"]*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[\"Tot_Vol\"] - _df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Congested_Speed\":\n",
    "            d[\"Congested_Speed\"] = (((_df[\"Length_meters\"]*0.000621371).divide(_df[\"TIME_1\"]))*60)\n",
    "            d[\"Congested_Speed_yr\"] = d[\"Congested_Speed\"]\n",
    "        elif fld == \"Tot_Vol\":\n",
    "            d[\"Tot_Vol_yr\"] = (_df[\"Tot_Vol\"]*365)\n",
    "            d[\"Tot_Vol_mil\"] = _df[\"Tot_Vol\"].divide(1000000)\n",
    "            d[\"Tot_Vol_mil_yr\"] = d[\"Tot_Vol_yr\"].divide(1000000)\n",
    "            d[\"log_Tot_Vol\"] = np.log(_df[\"Tot_Vol\"]+1)\n",
    "            d[\"log_Tot_Vol_yr\"] = np.log(d[\"Tot_Vol_yr\"]+1)\n",
    "            d[\"log_Tot_Vol_mil\"] = np.log(d[\"Tot_Vol_mil\"]+1)\n",
    "            d[\"log_Tot_Vol_mil_yr\"] = np.log(d[\"Tot_Vol_mil_yr\"]+1)\n",
    "        elif fld == \"PUDO\":\n",
    "            d[\"PUDO_yr\"] = (_df[\"PUDO\"]*365)\n",
    "            d[\"PUDO_thousands\"] = _df[\"PUDO\"].divide(1000)\n",
    "            d[\"PUDO_thousands_yr\"] = d[\"PUDO_thousands\"]*365\n",
    "            d[\"PUDO_mil\"] = _df[\"PUDO\"].divide(1000000)\n",
    "            d[\"PUDO_mil_yr\"] = d[\"PUDO_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_PUDO_yr\"] = np.log(d[\"PUDO_yr\"]+1)\n",
    "            d[\"log_PUDO_thousands\"] = np.log(d[\"PUDO_thousands\"]+1)\n",
    "            d[\"log_PUDO_thousands_yr\"] = np.log(d[\"PUDO_thousands_yr\"]+1)\n",
    "            d[\"log_PUDO_mil\"] = np.log(d[\"PUDO_mil\"]+1)\n",
    "            d[\"log_PUDO_mil_yr\"] = np.log(d[\"PUDO_mil_yr\"]+1)\n",
    "        elif fld == \"OOS\":\n",
    "            d[\"OOS_yr\"] = (_df[\"OOS\"]*365)\n",
    "            d[\"OOS_thousands\"] = _df[\"OOS\"].divide(1000)\n",
    "            d[\"OOS_thousands_yr\"] = d[\"OOS_thousands\"]*365\n",
    "            d[\"OOS_mil\"] = _df[\"OOS\"].divide(1000000)\n",
    "            d[\"OOS_mil_yr\"] = d[\"OOS_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_OOS_yr\"] = np.log(d[\"OOS_yr\"]+1)\n",
    "            d[\"log_OOS_thousands\"] = np.log(d[\"OOS_thousands\"]+1)\n",
    "            d[\"log_OOS_thousands_yr\"] = np.log(d[\"OOS_thousands_yr\"]+1)\n",
    "            d[\"log_OOS_mil\"] = np.log(d[\"OOS_mil\"]+1)\n",
    "            d[\"log_OOS_mil_yr\"] = np.log(d[\"OOS_mil_yr\"]+1)\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "def agg_network_CT(_df):\n",
    "    _df = reqd_colmns(_df.copy())\n",
    "    _df [\"A\"] = _df[\"A\"].astype(str)\n",
    "    _df[\"B\"] = _df[\"B\"].astype(str)\n",
    "    _df [\"A_B\"] = _df[\"A_B\"].astype(str)\n",
    "    #     _df[\"AT\"] = _df[\"AT\"].astype(str)\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    #     _df[\"category\"] = _df[\"category\"].astype(str)\n",
    "    # aggregate the dataframe using A_B\n",
    "    wt_avg = lambda x: np.ma.average(x, weights = _df.loc[x.index, \"Tot_Vol\"])\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            if col in wt_col:\n",
    "                d[col] = wt_avg\n",
    "            else:\n",
    "                d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "\n",
    "    wt_col = [\"SPEED\",\"TIME\",\"CSPD_1\", 'VDT_1', 'VHT_1','VC_1',]\n",
    "    sum_col = [ \"CAP\", \"DISTANCE_MILES\",'Length_meters', 'Length_miles',\n",
    "                'V_1',\n",
    "                'V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                'VT_1',\n",
    "                'V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1',\n",
    "                'V11T_1','V12T_1',\"V13T_1\",'V14T_1', 'V15T_1',\"V16T_1\",'V17T_1', 'V18T_1',\"V19T_1\",\n",
    "                'OOS', 'PUDO',\n",
    "                'Tot_Vol',\"TNC_Tot_Vol\",\n",
    "                \"TIME_1\",'TIMESEED',\n",
    "                \"Tot_CAP\",\n",
    "                'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',]\n",
    "\n",
    "    str_col = ['TAZ']\n",
    "    concat_col = [\"A_B\",\"FT\",\"AT\"]\n",
    "    drop_col = [ 'A', 'B',\"USE\",'PER_RISE', 'ONEWAY',\"TOLL\",'PROJ', 'ACTION', 'AB','peak',\n",
    "                 'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3', 'TOLLEA_DA',\n",
    "                 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA', 'TOLLEV_SR2', 'TOLLEV_SR3',\n",
    "                 'DTA_EDIT_F', 'TOLLTIME', 'PHASE', 'AMBUSSAVE', 'MDBUSSAVE', 'PMBUSSAVE', 'EVBUSSAVE', 'EABUSSAVE', 'SPDC', 'CAPC',\n",
    "                 'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "                 'STREETNAME', 'TYPE', 'MTYPE','TSIN',\n",
    "                 'VALUETOLL_', 'PASSTHRU', 'BUSTPS_AM', 'BUSTPS_OP', 'BUSTPS_PM', 'TSVA', 'BIKE_CLASS', 'PER_RISE', 'ONEWAY',\n",
    "                 'TOLL',\n",
    "                 'TIMESEED',\n",
    "                 ]\n",
    "\n",
    "    _df.drop(drop_col,axis=1,inplace=True)\n",
    "    df = _df.groupby(['TAZ'],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "\n",
    "    return get_required_fields(df)\n",
    "\n",
    "dfSFRdNtwrk2010_TAZ_agg = agg_network_CT(dfSFRdNtwrk2010_taz.copy())\n",
    "dfSFRdNtwrk2010_TAZ_agg[\"Crash_Year\"] = 2010\n",
    "dfSFRdNtwrk2016_TAZ_agg = agg_network_CT(dfSFRdNtwrk2016_taz.copy())\n",
    "dfSFRdNtwrk2016_TAZ_agg[\"Crash_Year\"] = 2016\n",
    "\n",
    "dfSFRdNtwrk2010_TAZ_agg.to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFChamp_2010_TAZ_agg_PCS.csv\"))\n",
    "dfSFRdNtwrk2016_TAZ_agg.to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFChamp_2016_TAZ_agg_PCS.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# aggregate crashes along the CensusTract\n",
    "def agg_crash_CT(_df):\n",
    "    drop_fld = ['CASE_ID', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'REPORTING_DISTRICT', 'DAY_OF_WEEK', 'CHP_SHIFT', 'POPULATION', 'CNTY_CITY_LOC', 'SPECIAL_COND', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'CITY_DIVISION_LAPD', 'CHP_BEAT_CLASS', 'BEAT_NUMBER', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'WEATHER_1', 'WEATHER_2', 'STATE_HWY_IND', 'CALTRANS_COUNTY', 'CALTRANS_DISTRICT', 'STATE_ROUTE', 'ROUTE_SUFFIX', 'POSTMILE_PREFIX', 'POSTMILE', 'LOCATION_TYPE', 'RAMP_INTERSECTION', 'SIDE_OF_HWY', 'TOW_AWAY', 'COLLISION_SEVERITY','PARTY_COUNT', 'PRIMARY_COLL_FACTOR', 'PCF_CODE_OF_VIOL', 'PCF_VIOL_CATEGORY', 'PCF_VIOLATION', 'PCF_VIOL_SUBSECTION', 'HIT_AND_RUN', 'TYPE_OF_COLLISION', 'MVIW', 'PED_ACTION', 'ROAD_SURFACE', 'ROAD_COND_1', 'ROAD_COND_2', 'LIGHTING', 'CONTROL_DEVICE', 'CHP_ROAD_TYPE', 'PEDESTRIAN_ACCIDENT', 'BICYCLE_ACCIDENT', 'MOTORCYCLE_ACCIDENT', 'TRUCK_ACCIDENT', 'NOT_PRIVATE_PROPERTY', 'ALCOHOL_INVOLVED', 'STWD_VEHTYPE_AT_FAULT', 'CHP_VEHTYPE_AT_FAULT', 'PRIMARY_RAMP', 'SECONDARY_RAMP', 'LATITUDE', 'LONGITUDE', 'COUNTY', 'CITY', 'POINT_X', 'POINT_Y', 'PRIMARY_RD_3', 'SECONDARY_RD_3', ]\n",
    "    _df.drop(columns=drop_fld,inplace=True)\n",
    "    str_col = ['TAZ']\n",
    "    sum_col = ['NUMBER_KILLED', 'NUMBER_INJURED',\n",
    "               'COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED', 'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED',\n",
    "               'Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', 'COUNT_PDO',]\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "    df = _df.groupby(['TAZ'],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "    return df\n",
    "dfSFCrash2010_TAZ_agg = agg_crash_CT(dfSFCrash2010_taz.copy())\n",
    "dfSFCrash2010_TAZ_agg[\"ACCIDENT_YEAR\"]=2010\n",
    "\n",
    "dfSFCrash2016_TAZ_agg = agg_crash_CT(dfSFCrash2016_taz.copy())\n",
    "dfSFCrash2016_TAZ_agg[\"ACCIDENT_YEAR\"]=2016\n",
    "\n",
    "dfSFCrash2010_TAZ_agg.to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFCrash_2010_TAZ_agg_PCS.csv\"))\n",
    "dfSFCrash2016_TAZ_agg.to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFCrash_2016_TAZ_agg_PCS.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dfSF_RdNtwrk_Crash_2010 = pd.merge(dfSFRdNtwrk2010_TAZ_agg.copy(),dfSFCrash2010_TAZ_agg.copy(), left_on=\"TAZ\",right_on=\"TAZ\",how=\"left\")\n",
    "dfSF_RdNtwrk_Crash_2016 = pd.merge(dfSFRdNtwrk2016_TAZ_agg.copy(),dfSFCrash2016_TAZ_agg.copy(), left_on=\"TAZ\",right_on=\"TAZ\",how=\"left\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "SF_TAZ = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "drop_cols = ['SUPERD', 'TAZ1454', 'AREALAND', 'AREAWATR', 'WATRACRE', 'SD', 'COUNTY', 'SFBLKGRP', 'X_COORD', 'Y_COORD',\n",
    "             'Nhood', 'nhood_num', 'lg_nhood', 'lg_nhd_num', 'SQ_MILE', 'DIST20',]\n",
    "SF_TAZ[\"TAZ\"]=SF_TAZ[\"TAZ\"].astype(str)\n",
    "dfSF2010 = SF_TAZ.merge(dfSF_RdNtwrk_Crash_2010,on=\"TAZ\",how=\"left\")\n",
    "dfSF2010.drop(columns=drop_cols,inplace=True)\n",
    "dfSF2010[[\"Crash_Year\",\"ACCIDENT_YEAR\"]]=2010\n",
    "dfSF2016 = SF_TAZ.merge(dfSF_RdNtwrk_Crash_2016,on=\"TAZ\",how=\"left\")\n",
    "dfSF2016.drop(columns=drop_cols,inplace=True)\n",
    "dfSF2016[[\"Crash_Year\",\"ACCIDENT_YEAR\"]]=2016\n",
    "dfSF2010.loc[:,~dfSF2010.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF2010_TAZ_agg_PCS.csv\"))\n",
    "dfSF2016.loc[:,~dfSF2016.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF2016_TAZ_agg_PCS.csv\"))\n",
    "\n",
    "dfSF2010.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_2010_TAZ_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")\n",
    "dfSF2016.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_2016_TAZ_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")\n",
    "\n",
    "dfSFmerged = gpd.GeoDataFrame(pd.concat([dfSF2010,dfSF2016],ignore_index=True),crs=dfSF2010.crs)\n",
    "dfSFmerged.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "dfSFmerged.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"SFmerged_TAZ_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")\n",
    "dfSFmerged.loc[:,~dfSFmerged.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFmerged_TAZ_PCS.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "gdSFdb = pd.read_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFmerged_TAZ_PCS.csv\"))\n",
    "gdSFdb2010 = gdSFdb[gdSFdb[\"ACCIDENT_YEAR\"]==2010].add_suffix(\"_2010\").copy()\n",
    "gdSFdb2016 = gdSFdb[gdSFdb[\"ACCIDENT_YEAR\"]==2016].add_suffix(\"_2016\").copy()\n",
    "\n",
    "dfSFJoined = pd.merge(gdSFdb2010,gdSFdb2016, left_on=\"TAZ_2010\",right_on=\"TAZ_2016\",how=\"inner\")\n",
    "dfSFJoined.rename(columns={\"TAZ_2010\":\"TAZ\"},inplace=True)\n",
    "dfSFJoined[\"TAZ\"]=dfSFJoined[\"TAZ\"].astype(str)\n",
    "dfSFJoined = dfSFJoined.sort_index(axis=1)\n",
    "dfSFJoined.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_TAZ_joined.csv\"))\n",
    "\n",
    "def add_column(df):\n",
    "    cols = ['CAP', 'SPEED', 'TIME', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA', 'V_1', 'TIME_1', 'VC_1', 'CSPD_1', 'VDT_1', 'VHT_1', 'V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1', 'VT_1', 'V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1', 'V11T_1', 'V12T_1', 'Tot_CAP', 'OOS', 'PUDO', 'Tot_Vol', 'TNC_Tot_Vol', 'V13_1', 'V14_1', 'V15_1', 'V16_1', 'V17_1', 'V18_1', 'V19_1', 'Tot_TNC_Vol', 'Tot_TNC_Vol_mil', 'Tot_TNC_Vol_yr', 'Tot_TNC_Vol_mil_yr', 'log_Tot_TNC_Vol', 'log_Tot_TNC_Vol_mil', 'log_Tot_TNC_Vol_mil_yr', 'Tot_Non_TNC_Vol', 'Tot_Non_TNC_Vol_mil', 'Tot_Non_TNC_Vol_yr', 'Tot_Non_TNC_Vol_mil_yr', 'log_Tot_Non_TNC_Vol', 'log_Tot_Non_TNC_Vol_mil', 'log_Tot_Non_TNC_Vol_mil_yr', 'Tot_VMT', 'Tot_VMT_mil', 'Tot_VMT_yr', 'Tot_VMT_mil_yr', 'log_Tot_VMT', 'log_Tot_VMT_mil', 'log_Tot_VMT_mil_yr', 'Tot_TNC_VMT', 'Tot_TNC_VMT_mil', 'Tot_TNC_VMT_yr', 'Tot_TNC_VMT_mil_yr', 'log_Tot_TNC_VMT', 'log_Tot_TNC_VMT_mil', 'log_Tot_TNC_VMT_mil_yr', 'Tot_Non_TNC_VMT', 'Tot_Non_TNC_VMT_mil', 'Tot_Non_TNC_VMT_yr', 'Tot_Non_TNC_VMT_mil_yr', 'log_Tot_Non_TNC_VMT', 'log_Tot_Non_TNC_VMT_mil', 'log_Tot_Non_TNC_VMT_mil_yr', 'Congested_Speed', 'Congested_Speed_yr', 'Tot_Vol_yr', 'Tot_Vol_mil', 'Tot_Vol_mil_yr', 'log_Tot_Vol', 'log_Tot_Vol_yr', 'log_Tot_Vol_mil', 'log_Tot_Vol_mil_yr', 'PUDO_yr', 'PUDO_thousands', 'PUDO_thousands_yr', 'PUDO_mil', 'PUDO_mil_yr', 'log_PUDO_yr', 'log_PUDO_thousands', 'log_PUDO_thousands_yr', 'log_PUDO_mil', 'log_PUDO_mil_yr', 'OOS_yr', 'OOS_thousands', 'OOS_thousands_yr', 'OOS_mil', 'OOS_mil_yr', 'log_OOS_yr', 'log_OOS_thousands', 'log_OOS_thousands_yr', 'log_OOS_mil', 'log_OOS_mil_yr','NUMBER_KILLED', 'NUMBER_INJURED', 'COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED', 'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED', 'Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', 'COUNT_PDO', 'V13T_1', 'V14T_1', 'V15T_1', 'V16T_1', 'V17T_1', 'V18T_1', 'V19T_1']\n",
    "    d = {}\n",
    "    for col in cols:\n",
    "        d[f'{col}_{\"diff\"}'] = df[f'{col}_{\"2016\"}'] - df[f'{col}_{\"2010\"}']\n",
    "        d[f'{col}_{\"pct_change\"}'] = d[f'{col}_{\"diff\"}'].divide(df[f'{col}_{\"2010\"}'])\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "dfSFJoined = add_column(dfSFJoined)\n",
    "dfSFJoined.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "dfSFJoined.fillna(0,inplace=True)\n",
    "dfSFJoined.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_TAZ_joined_diff_pct_chnge.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "dfSFJoined = pd.read_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_TAZ_joined_diff_pct_chnge.csv\"))\n",
    "dfSFJoined[\"TAZ\"]=dfSFJoined[\"TAZ\"].astype(str)\n",
    "SF_TAZ = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "SF_TAZ[\"TAZ\"]=SF_TAZ[\"TAZ\"].astype(str)\n",
    "# clmn = dfSFJoined.columns.to_list()\n",
    "# dfSFJoined[clmn] = dfSFJoined[clmn].apply(pd.to_numeric, errors='coerce')\n",
    "SF_TAZ.merge(dfSFJoined,left_on=\"TAZ\",right_on=\"TAZ\",how=\"left\").to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_TAZ_joined_diff_pct_chnge_TAZ_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nPerform analysis based on Facility Type\\nFT = 1:Fwy-Fwy Connector, 2:Freeway, 3:Expressway, 4:Collector, 5:Ramp, 6:Centroid Connector, 7:Major Arterial, 8:Not used,\\n9:Alley, 10:Metered Ramp, 11:Local, 12:Minor Arterial,13:Bike-Only!, 14:Not used, 15:Super Arterial,\\nSegregate the road network into three categories\\n1. Category 1 = contains FT = [1, 2, 3, 5]\\n2. Category 2 = contains FT = [7,12,,13,15]\\n3. Category 3 = contains FT = [4,9,11]\\n'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform analysis based on Facility Type\n",
    "FT = 1:Fwy-Fwy Connector, 2:Freeway, 3:Expressway, 4:Collector, 5:Ramp, 6:Centroid Connector, 7:Major Arterial, 8:Not used,\n",
    "9:Alley, 10:Metered Ramp, 11:Local, 12:Minor Arterial,13:Bike-Only!, 14:Not used, 15:Super Arterial,\n",
    "Segregate the road network into three categories\n",
    "1. Category 1 = contains FT = [1, 2, 3, 5]\n",
    "2. Category 2 = contains FT = [7,12,,13,15]\n",
    "3. Category 3 = contains FT = [4,9,11]\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# modify the dataframe to create a new column \"CATEGORY\" using \"FacilityType\"\n",
    "def label_df_by_road_category(_df,fld):\n",
    "    _df[\"category\"]=0\n",
    "    _df.loc[_df[fld].isin([1, 2, 3, 5,13]),'category']=1\n",
    "    _df.loc[_df[fld].isin([7,12,15]),'category']=2\n",
    "    _df.loc[_df[fld].isin([4,9,11 ]),'category']=3\n",
    "\n",
    "    return _df\n",
    "\n",
    "# Fetch SF_Census Tract\n",
    "SF_TAZ = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SF_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "\n",
    "# road network\n",
    "dfSFRdNtwrk2010_taz = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFChamp_2010_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2010_taz.fillna(dfSFRdNtwrk2010_taz.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFRdNtwrk2010_cat_taz =  label_df_by_road_category(dfSFRdNtwrk2010_taz.copy(),\"FT\")\n",
    "\n",
    "# dfSFRdNtwrk2010_CT[\"ACCIDENT_YEAR\"] = 2010\n",
    "dfSFRdNtwrk2016_taz = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFChamp_2016_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2016_taz.fillna(dfSFRdNtwrk2016_taz.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFRdNtwrk2016_cat_taz =  label_df_by_road_category(dfSFRdNtwrk2016_taz.copy(),\"FT\")\n",
    "\n",
    "# road crashes\n",
    "dfSFCrash2010_taz = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"NN_SFCrash_2010_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2010_taz.fillna(dfSFCrash2010_taz.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFCrash2010_taz[\"ACCIDENT_YEAR\"] = 2010\n",
    "dfSFCrash2010_cat_taz =  label_df_by_road_category(dfSFCrash2010_taz.copy(),\"FT\")\n",
    "\n",
    "dfSFCrash2016_taz = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"NN_SFCrash_2016_TAZ_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2016_taz.fillna(dfSFCrash2016_taz.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFCrash2016_taz[\"ACCIDENT_YEAR\"] = 2016\n",
    "dfSFCrash2016_cat_taz =  label_df_by_road_category(dfSFCrash2016_taz.copy(),\"FT\")\n",
    "\n",
    "\n",
    "#remember to rename DISTANCE variable (as this is no longer the actual distance (in miles), given that feature is split-up)\n",
    "def add_length_columns(_df):\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[\"Length_meters\"] = _df.geometry.length\n",
    "        d[\"Length_miles\"] = d[\"Length_meters\"]* 0.000621371\n",
    "\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "dfSFRdNtwrk2010_cat_taz.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2010_cat_taz = add_length_columns(dfSFRdNtwrk2010_cat_taz.copy())\n",
    "dfSFRdNtwrk2016_cat_taz.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2016_cat_taz = add_length_columns(dfSFRdNtwrk2016_cat_taz.copy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# aggregate the SFChamp network files by TAZ by category\n",
    "def reqd_colmns(_df):# road network\n",
    "    df = _df.copy()\n",
    "    d = {}\n",
    "    reqd_colmns = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                   'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                   'OOS', 'PUDO','Tot_Vol',\"TNC_Tot_Vol\",\n",
    "                   'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',\n",
    "                   ]\n",
    "    for col in reqd_colmns:\n",
    "        if col not in df.columns:\n",
    "            d[col]=0\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "def get_req_fields(_df):\n",
    "    fields = [\"Tot_TNC_Vol\", \"Tot_Non_TNC_Vol\", \"Tot_VMT\",\"Tot_TNC_VMT\",\"Tot_Non_TNC_VMT\", \"Congested_Speed\",\"Tot_Vol\",\"PUDO\",\"OOS\"]\n",
    "    d = {}\n",
    "    for fld in fields:\n",
    "        if fld == \"Tot_TNC_Vol\":# TNC Tot Vol\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_Vol\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[\"Tot_Vol\"] - _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_VMT\":\n",
    "            d[fld] = _df[\"Tot_Vol\"]*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[\"Tot_Vol\"] - _df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Congested_Speed\":\n",
    "            d[\"Congested_Speed\"] = (((_df[\"Length_meters\"]*0.000621371).divide(_df[\"TIME_1\"]))*60)\n",
    "            d[\"Congested_Speed_yr\"] = d[\"Congested_Speed\"]\n",
    "        elif fld == \"Tot_Vol\":\n",
    "            d[\"Tot_Vol_yr\"] = (_df[\"Tot_Vol\"]*365)\n",
    "            d[\"Tot_Vol_mil\"] = _df[\"Tot_Vol\"].divide(1000000)\n",
    "            d[\"Tot_Vol_mil_yr\"] = d[\"Tot_Vol_yr\"].divide(1000000)\n",
    "            d[\"log_Tot_Vol\"] = np.log(_df[\"Tot_Vol\"]+1)\n",
    "            d[\"log_Tot_Vol_yr\"] = np.log(d[\"Tot_Vol_yr\"]+1)\n",
    "            d[\"log_Tot_Vol_mil\"] = np.log(d[\"Tot_Vol_mil\"]+1)\n",
    "            d[\"log_Tot_Vol_mil_yr\"] = np.log(d[\"Tot_Vol_mil_yr\"]+1)\n",
    "        elif fld == \"PUDO\":\n",
    "            d[\"PUDO_yr\"] = (_df[\"PUDO\"]*365)\n",
    "            d[\"PUDO_thousands\"] = _df[\"PUDO\"].divide(1000)\n",
    "            d[\"PUDO_thousands_yr\"] = d[\"PUDO_thousands\"]*365\n",
    "            d[\"PUDO_mil\"] = _df[\"PUDO\"].divide(1000000)\n",
    "            d[\"PUDO_mil_yr\"] = d[\"PUDO_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_PUDO_yr\"] = np.log(d[\"PUDO_yr\"]+1)\n",
    "            d[\"log_PUDO_thousands\"] = np.log(d[\"PUDO_thousands\"]+1)\n",
    "            d[\"log_PUDO_thousands_yr\"] = np.log(d[\"PUDO_thousands_yr\"]+1)\n",
    "            d[\"log_PUDO_mil\"] = np.log(d[\"PUDO_mil\"]+1)\n",
    "            d[\"log_PUDO_mil_yr\"] = np.log(d[\"PUDO_mil_yr\"]+1)\n",
    "        elif fld == \"OOS\":\n",
    "            d[\"OOS_yr\"] = (_df[\"OOS\"]*365)\n",
    "            d[\"OOS_thousands\"] = _df[\"OOS\"].divide(1000)\n",
    "            d[\"OOS_thousands_yr\"] = d[\"OOS_thousands\"]*365\n",
    "            d[\"OOS_mil\"] = _df[\"OOS\"].divide(1000000)\n",
    "            d[\"OOS_mil_yr\"] = d[\"OOS_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_OOS_yr\"] = np.log(d[\"OOS_yr\"]+1)\n",
    "            d[\"log_OOS_thousands\"] = np.log(d[\"OOS_thousands\"]+1)\n",
    "            d[\"log_OOS_thousands_yr\"] = np.log(d[\"OOS_thousands_yr\"]+1)\n",
    "            d[\"log_OOS_mil\"] = np.log(d[\"OOS_mil\"]+1)\n",
    "            d[\"log_OOS_mil_yr\"] = np.log(d[\"OOS_mil_yr\"]+1)\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "def agg_network(_df):\n",
    "    _df = reqd_colmns(_df.copy())\n",
    "    _df [\"A\"] = _df[\"A\"].astype(str)\n",
    "    _df[\"B\"] = _df[\"B\"].astype(str)\n",
    "    _df [\"A_B\"] = _df[\"A_B\"].astype(str)\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    _df[\"TAZ\"] = _df[\"TAZ\"].astype(str)\n",
    "    # aggregate the dataframe using A_B\n",
    "    wt_avg = lambda x: np.ma.average(x, weights = _df.loc[x.index, \"Tot_Vol\"])\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "\n",
    "    def agg_func_rdntwrk(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            if col in wt_col:\n",
    "                d[col] = wt_avg\n",
    "            else:\n",
    "                d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "\n",
    "    wt_col = [\"SPEED\",\"TIME\",\"CSPD_1\", 'VDT_1', 'VHT_1','VC_1',]\n",
    "    sum_col = [ \"CAP\", \"DISTANCE_MILES\",'Length_meters', 'Length_miles',\n",
    "                'V_1',\n",
    "                'V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                'VT_1',\n",
    "                'V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1',\n",
    "                'V11T_1','V12T_1',\"V13T_1\",'V14T_1', 'V15T_1',\"V16T_1\",'V17T_1', 'V18T_1',\"V19T_1\",\n",
    "                'OOS', 'PUDO',\n",
    "                'Tot_Vol',\"TNC_Tot_Vol\",\n",
    "                \"TIME_1\",'TIMESEED',\n",
    "                \"Tot_CAP\",\n",
    "                'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',]\n",
    "\n",
    "    str_col = ['TAZ']\n",
    "    concat_col = [\"FT\",]\n",
    "    drop_col = [ 'A', 'B',\"USE\",'PER_RISE', 'ONEWAY',\"TOLL\",'PROJ', 'ACTION', 'AB','peak',\n",
    "                 'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3', 'TOLLEA_DA',\n",
    "                 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA', 'TOLLEV_SR2', 'TOLLEV_SR3',\n",
    "                 'DTA_EDIT_F', 'TOLLTIME', 'PHASE', 'AMBUSSAVE', 'MDBUSSAVE', 'PMBUSSAVE', 'EVBUSSAVE', 'EABUSSAVE', 'SPDC', 'CAPC',\n",
    "                 'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "                 'STREETNAME', 'TYPE', 'MTYPE','TSIN',\n",
    "                 'VALUETOLL_', 'PASSTHRU', 'BUSTPS_AM', 'BUSTPS_OP', 'BUSTPS_PM', 'TSVA', 'BIKE_CLASS', 'PER_RISE', 'ONEWAY',\n",
    "                 'TOLL',\n",
    "                 'TIMESEED',\"A_B\",\"AT\"\n",
    "                 ]\n",
    "\n",
    "    _df.drop(drop_col,axis=1,inplace=True)\n",
    "    df = _df.groupby(['TAZ'],as_index=False).aggregate(agg_func_rdntwrk(_df.copy())).copy()\n",
    "\n",
    "    return get_req_fields(df)\n",
    "def agg_crash(_df):\n",
    "    drop_fld = ['CASE_ID', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'REPORTING_DISTRICT', 'DAY_OF_WEEK', 'CHP_SHIFT', 'POPULATION', 'CNTY_CITY_LOC', 'SPECIAL_COND', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'CITY_DIVISION_LAPD', 'CHP_BEAT_CLASS', 'BEAT_NUMBER', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'WEATHER_1', 'WEATHER_2', 'STATE_HWY_IND', 'CALTRANS_COUNTY', 'CALTRANS_DISTRICT', 'STATE_ROUTE', 'ROUTE_SUFFIX', 'POSTMILE_PREFIX', 'POSTMILE', 'LOCATION_TYPE', 'RAMP_INTERSECTION', 'SIDE_OF_HWY', 'TOW_AWAY', 'COLLISION_SEVERITY','PARTY_COUNT', 'PRIMARY_COLL_FACTOR', 'PCF_CODE_OF_VIOL', 'PCF_VIOL_CATEGORY', 'PCF_VIOLATION', 'PCF_VIOL_SUBSECTION', 'HIT_AND_RUN', 'TYPE_OF_COLLISION', 'MVIW', 'PED_ACTION', 'ROAD_SURFACE', 'ROAD_COND_1', 'ROAD_COND_2', 'LIGHTING', 'CONTROL_DEVICE', 'CHP_ROAD_TYPE', 'PEDESTRIAN_ACCIDENT', 'BICYCLE_ACCIDENT', 'MOTORCYCLE_ACCIDENT', 'TRUCK_ACCIDENT', 'NOT_PRIVATE_PROPERTY', 'ALCOHOL_INVOLVED', 'STWD_VEHTYPE_AT_FAULT', 'CHP_VEHTYPE_AT_FAULT', 'PRIMARY_RAMP', 'SECONDARY_RAMP', 'LATITUDE', 'LONGITUDE', 'COUNTY', 'CITY', 'POINT_X', 'POINT_Y', 'PRIMARY_RD_3', 'SECONDARY_RD_3',]\n",
    "    _df.drop(columns=drop_fld,inplace=True)\n",
    "    str_col = [\"FT\",\"join_TAZ\",\"TAZ\"]\n",
    "    sum_col = ['NUMBER_KILLED', 'NUMBER_INJURED',\n",
    "               'COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED', 'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED',\n",
    "               'Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', 'COUNT_PDO',]\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    _df[\"join_TAZ\"] = _df[\"join_TAZ\"].astype(str)\n",
    "    _df[\"TAZ\"] = _df[\"TAZ\"].astype(str)\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "    df = _df.groupby(['TAZ'],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def perform_merge(_dfrdntwrk,_dfcrash,_dfSFTAZ,crash_yr):\n",
    "    dfrdntwrk = agg_network(_dfrdntwrk)\n",
    "    dfcrash = agg_crash(_dfcrash)\n",
    "    dfcrash[\"ACCIDENT_YEAR\"]=crash_yr\n",
    "\n",
    "    dfrdntwrk_crash = pd.merge(dfrdntwrk,dfcrash, left_on=\"TAZ\",right_on=\"join_TAZ\",how=\"left\")\n",
    "    dfrdntwrk_crash.rename(columns={\"TAZ_x\":\"TAZ\"},inplace=True)\n",
    "\n",
    "    col_drop = ['SUPERD', 'TAZ1454', 'AREALAND', 'AREAWATR', 'WATRACRE', 'SD', 'COUNTY', 'SFBLKGRP', 'X_COORD', 'Y_COORD',\n",
    "                'Nhood', 'nhood_num', 'lg_nhood', 'lg_nhd_num', 'SQ_MILE', 'DIST20',]\n",
    "\n",
    "    dfSFTAZ = _dfSFTAZ.copy()\n",
    "    dfSFTAZ[\"TAZ\"]=dfSFTAZ[\"TAZ\"].astype(str)\n",
    "\n",
    "    dfrdntwrk_crash_taz = dfSFTAZ.merge(dfrdntwrk_crash,left_on=\"TAZ\",right_on=\"TAZ\",how=\"left\")\n",
    "    dfrdntwrk_crash_taz.drop(columns=col_drop,inplace=True)\n",
    "    dfrdntwrk_crash_taz.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # Fill NaN for all strings column with NaN and for all numeric columns with 0\n",
    "    d = {**dict.fromkeys(dfrdntwrk_crash_taz.select_dtypes(np.number).columns, 0),\n",
    "         **dict.fromkeys(dfrdntwrk_crash_taz.select_dtypes(exclude=np.number).columns, '')}\n",
    "    dfrdntwrk_crash_taz = dfrdntwrk_crash_taz.fillna(d)\n",
    "\n",
    "    drop_clmn = ['category_x', 'Unnamed: 0', 'category_y', 'TAZ_y', 'FT_y', 'join_TAZ', 'FT_x', ]\n",
    "    dfrdntwrk_crash_taz.drop(columns=drop_clmn, inplace=True)\n",
    "    dfrdntwrk_crash_taz[\"TAZ\"]=dfrdntwrk_crash_taz[\"TAZ\"].astype(\"int\")\n",
    "    dfrdntwrk_crash_taz = dfrdntwrk_crash_taz.loc[dfrdntwrk_crash_taz[\"TAZ\"].between(0,981)]\n",
    "    dfrdntwrk_crash_taz[\"ACCIDENT_YEAR\"]=crash_yr\n",
    "    dfrdntwrk_crash_taz[\"Crash_Year\"]=crash_yr\n",
    "    # dfrdntwrk_crash_taz.loc[:,~dfrdntwrk_crash_taz.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",\"SFmerged_TAZ_cat_1_PCS.csv\"))\n",
    "\n",
    "    return dfrdntwrk_crash_taz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def add_column(df):\n",
    "    cols = ['CAP', 'SPEED', 'TIME', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA', 'V_1', 'TIME_1', 'VC_1', 'CSPD_1', 'VDT_1', 'VHT_1', 'V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1', 'VT_1', 'V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1', 'V11T_1', 'V12T_1', 'Tot_CAP', 'OOS', 'PUDO', 'Tot_Vol', 'TNC_Tot_Vol', 'V13_1', 'V14_1', 'V15_1', 'V16_1', 'V17_1', 'V18_1', 'V19_1', 'Tot_TNC_Vol', 'Tot_TNC_Vol_mil', 'Tot_TNC_Vol_yr', 'Tot_TNC_Vol_mil_yr', 'log_Tot_TNC_Vol', 'log_Tot_TNC_Vol_mil', 'log_Tot_TNC_Vol_mil_yr', 'Tot_Non_TNC_Vol', 'Tot_Non_TNC_Vol_mil', 'Tot_Non_TNC_Vol_yr', 'Tot_Non_TNC_Vol_mil_yr', 'log_Tot_Non_TNC_Vol', 'log_Tot_Non_TNC_Vol_mil', 'log_Tot_Non_TNC_Vol_mil_yr', 'Tot_VMT', 'Tot_VMT_mil', 'Tot_VMT_yr', 'Tot_VMT_mil_yr', 'log_Tot_VMT', 'log_Tot_VMT_mil', 'log_Tot_VMT_mil_yr', 'Tot_TNC_VMT', 'Tot_TNC_VMT_mil', 'Tot_TNC_VMT_yr', 'Tot_TNC_VMT_mil_yr', 'log_Tot_TNC_VMT', 'log_Tot_TNC_VMT_mil', 'log_Tot_TNC_VMT_mil_yr', 'Tot_Non_TNC_VMT', 'Tot_Non_TNC_VMT_mil', 'Tot_Non_TNC_VMT_yr', 'Tot_Non_TNC_VMT_mil_yr', 'log_Tot_Non_TNC_VMT', 'log_Tot_Non_TNC_VMT_mil', 'log_Tot_Non_TNC_VMT_mil_yr', 'Congested_Speed', 'Congested_Speed_yr', 'Tot_Vol_yr', 'Tot_Vol_mil', 'Tot_Vol_mil_yr', 'log_Tot_Vol', 'log_Tot_Vol_yr', 'log_Tot_Vol_mil', 'log_Tot_Vol_mil_yr', 'PUDO_yr', 'PUDO_thousands', 'PUDO_thousands_yr', 'PUDO_mil', 'PUDO_mil_yr', 'log_PUDO_yr', 'log_PUDO_thousands', 'log_PUDO_thousands_yr', 'log_PUDO_mil', 'log_PUDO_mil_yr', 'OOS_yr', 'OOS_thousands', 'OOS_thousands_yr', 'OOS_mil', 'OOS_mil_yr', 'log_OOS_yr', 'log_OOS_thousands', 'log_OOS_thousands_yr', 'log_OOS_mil', 'log_OOS_mil_yr','NUMBER_KILLED', 'NUMBER_INJURED', 'COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED', 'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED', 'Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', 'COUNT_PDO', 'V13T_1', 'V14T_1', 'V15T_1', 'V16T_1', 'V17T_1', 'V18T_1', 'V19T_1']\n",
    "    d = {}\n",
    "    for col in cols:\n",
    "        d[f'{col}_{\"diff\"}'] = df[f'{col}_{\"2016\"}'] - df[f'{col}_{\"2010\"}']\n",
    "        d[f'{col}_{\"pct_change\"}'] = d[f'{col}_{\"diff\"}'].divide(df[f'{col}_{\"2010\"}'])\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "def create_pct_change_files(_dfmerged,_dfSFTAZ,cat):\n",
    "    dfmerged = _dfmerged\n",
    "    dfSFTAZ = _dfSFTAZ\n",
    "\n",
    "    gdSFdb2010 = dfmerged[dfmerged[\"ACCIDENT_YEAR\"]==2010].add_suffix(\"_2010\").copy()\n",
    "    gdSFdb2016 = dfmerged[dfmerged[\"ACCIDENT_YEAR\"]==2016].add_suffix(\"_2016\").copy()\n",
    "\n",
    "    df_joined = pd.merge(gdSFdb2010, gdSFdb2016, left_on=\"TAZ_2010\", right_on=\"TAZ_2016\", how=\"inner\")\n",
    "    df_joined.rename(columns={\"TAZ_2010\": \"TAZ\"}, inplace=True)\n",
    "    df_joined[\"TAZ\"]=df_joined[\"TAZ\"].astype(str)\n",
    "    df_joined = df_joined.sort_index(axis=1)\n",
    "    df_joined.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path, \"Feb162022\", \"TAZ\", f\"SF_joined_cat_{cat}.csv\"))\n",
    "\n",
    "    df_joined = add_column(df_joined)\n",
    "    df_joined.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_joined.fillna(0, inplace=True)\n",
    "    df_joined.sort_index(axis=1).to_csv(BASE_DIR.parent.joinpath(folder_path, \"Feb162022\", \"TAZ\", f\"SF_joined_cat_{cat}_diff_pct_chnge.csv\"))\n",
    "\n",
    "    df_joined = pd.read_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",f\"SF_joined_cat_{cat}_diff_pct_chnge.csv\"))\n",
    "\n",
    "    # cols = ['statefp10', 'mtfcc10', 'name10', 'intptlat10', 'awater10', 'namelsad10', 'funcstat10', 'aland10', 'geoid10', 'intptlon10', 'countyfp10',]\n",
    "    # SF_CT.drop(columns=cols,inplace=True)\n",
    "    dfSFTAZ[\"TAZ\"]=dfSFTAZ[\"TAZ\"].astype(int).astype(str)\n",
    "\n",
    "    # dfSFJoined.rename(columns={\"tractce10_2010\":\"tractce10\"},inplace=True)\n",
    "    # clmn = dfSFJoined.columns.to_list()\n",
    "    # dfSFJoined[clmn] = dfSFJoined[clmn].apply(pd.to_numeric, errors='coerce')\n",
    "    df_joined[\"TAZ\"]=df_joined[\"TAZ\"].astype(str)\n",
    "    # dfSFJoined[[\"tractce10\",\"tractce10_2016\"]]=dfSFJoined[[\"tractce10\",\"tractce10_2016\"]].astype(str)\n",
    "    dfSFTAZ.merge(df_joined,left_on=\"TAZ\",right_on=\"TAZ\",how=\"left\").to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",f\"SF_joined_cat_{cat}_diff_pct_chnge_TAZ_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\numpy\\ma\\extras.py:623: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\numpy\\ma\\extras.py:623: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\numpy\\ma\\extras.py:623: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\numpy\\ma\\extras.py:623: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "def perform_manipulation_by_category(_dfrdntwrk2010,_dfrdntwrk2016,_dfcrash2010,_dfcrash2016,_dfSF_TAZ,cat):\n",
    "    dfSF2010 = perform_merge(_dfrdntwrk2010,_dfcrash2010,_dfSF_TAZ,2010)\n",
    "    dfSF2016 = perform_merge(_dfrdntwrk2016,_dfcrash2016,_dfSF_TAZ,2016)\n",
    "    dfSFmerged = gpd.GeoDataFrame(pd.concat([dfSF2010,dfSF2016],ignore_index=True),crs=dfSF2010.crs)\n",
    "    # dfSFmerged.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    dfSFmerged.to_file(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",f\"SFmerged_TAZ_cat_{cat}_PCS.geojson\"), driver='GeoJSON', crs = \"EPSG:3857\")\n",
    "    dfSFmerged.loc[:,~dfSFmerged.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(folder_path,\"Feb162022\",\"TAZ\",f\"SFmerged_TAZ_cat_{cat}_PCS.csv\"))\n",
    "\n",
    "    create_pct_change_files(dfSFmerged.loc[:,~dfSFmerged.columns.isin([\"geometry\"])],_dfSF_TAZ,cat)\n",
    "\n",
    "for cat in [1,2,3]:\n",
    "    perform_manipulation_by_category(dfSFRdNtwrk2010_cat_taz.loc[dfSFRdNtwrk2010_cat_taz[\"category\"]==cat,:].copy(),\n",
    "                                     dfSFRdNtwrk2016_cat_taz.loc[dfSFRdNtwrk2016_cat_taz[\"category\"]==cat,:].copy(),\n",
    "                                     dfSFCrash2010_cat_taz.loc[dfSFCrash2010_cat_taz[\"category\"]==cat,:].copy(),\n",
    "                                     dfSFCrash2016_cat_taz.loc[dfSFCrash2016_cat_taz[\"category\"]==cat,:].copy(),\n",
    "                                     SF_TAZ.copy(),\n",
    "                                     cat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}