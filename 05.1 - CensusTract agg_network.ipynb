{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304cfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyproj import CRS\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import codecs\n",
    "import osm2geojson\n",
    "from shapely import wkt\n",
    "\n",
    "import gzip\n",
    "from xml.etree.ElementTree import Element, SubElement, Comment, tostring\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import os\n",
    "\n",
    "# set the working directory\n",
    "BASE_DIR = Path.cwd()\n",
    "# define the exported folder path\n",
    "# Check if folder exists\n",
    "folder_path = pathlib.Path(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"agg_network\"))\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "# print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e9dec",
   "metadata": {},
   "source": [
    "## Fetch SF Champ network for 2010 and join the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3663c8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goyal\\.virtualenvs\\overpass_turbo-zwxzihl_\\lib\\site-packages\\numpy\\ma\\extras.py:624: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n"
     ]
    }
   ],
   "source": [
    "# Network for YR 2010\n",
    "dfSFRd2010am = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_AM.shp\"))\n",
    "dfSFRd2010am[\"peak\"]=\"AM\"\n",
    "dfSFRd2010am[\"Tot_CAP\"]=dfSFRd2010am[\"CAP\"]*3\n",
    "\n",
    "dfSFRd2010pm = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_PM.shp\"))\n",
    "dfSFRd2010pm[\"peak\"]=\"PM\"\n",
    "dfSFRd2010pm[\"Tot_CAP\"]=dfSFRd2010pm[\"CAP\"]*3\n",
    "\n",
    "dfSFRd2010ea = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_EA.shp\"))\n",
    "dfSFRd2010ea[\"peak\"]=\"EA\"\n",
    "dfSFRd2010ea[\"Tot_CAP\"]=dfSFRd2010ea[\"CAP\"]*3\n",
    "\n",
    "dfSFRd2010ev = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_EV.shp\"))\n",
    "dfSFRd2010ev[\"peak\"]=\"EV\"\n",
    "dfSFRd2010ev[\"Tot_CAP\"]=dfSFRd2010ev[\"CAP\"]*8.5\n",
    "\n",
    "dfSFRd2010md = gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_MD.shp\"))\n",
    "dfSFRd2010md[\"peak\"]=\"MD\"\n",
    "dfSFRd2010md[\"Tot_CAP\"]=dfSFRd2010md[\"CAP\"]*6.5\n",
    "\n",
    "dfSFRdNtwrk2010 = pd.concat([dfSFRd2010am,dfSFRd2010pm,dfSFRd2010ea,dfSFRd2010ev,dfSFRd2010md])\n",
    "# create empty columns: OOS, PUDO, Tot_Vol\n",
    "dfSFRdNtwrk2010 = dfSFRdNtwrk2010.assign(OOS=0, PUDO=0, Tot_Vol=0, TNC_Tot_Vol=0)\n",
    "dfSFRdNtwrk2010[\"A_B\"] = dfSFRdNtwrk2010[\"A\"].astype(str)  + \"_\" + dfSFRdNtwrk2010[\"B\"].astype(str)\n",
    "dfSFRdNtwrk2010[\"A\"] = dfSFRdNtwrk2010[\"A\"].astype(str)\n",
    "dfSFRdNtwrk2010[\"B\"] = dfSFRdNtwrk2010[\"B\"].astype(str)\n",
    "# get the columns which together formm Tot_Vol\n",
    "add_2010 = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1',\n",
    "            'BUSVOL_AM','BUSVOL_PM','BUSVOL_EA','BUSVOL_MD','BUSVOL_EV','OOS']\n",
    "# add them up\n",
    "dfSFRdNtwrk2010[\"Tot_Vol\"] = dfSFRdNtwrk2010[add_2010].sum(axis=1)\n",
    "\n",
    "# Keep only FT types representing real road-network\n",
    "# 1: Fwy-Fwy Connector; 2: Freeway; 3: Expressway; 4: Collector; 5: Ramp; 6: Centroid Connector;\n",
    "# 7: Major Arterial; 8: ; 9: Alley (only for DTA); 10: ; 11: Local; 12: Minor Arterial; 13: Bike only;\n",
    "# 14: ; 15: Super Arterial\n",
    "dfSFRdNtwrk2010=dfSFRdNtwrk2010[dfSFRdNtwrk2010.FT.isin([1,2,3,4,5,7,9,10,11,12,13,15])]\n",
    "# dfSFRdNtwrk2010=dfSFRdNtwrk2010[dfSFRdNtwrk2010.FT.isin([4,7,10,11,12,15])]\n",
    "\n",
    "# convert back to geopandas dataframe\n",
    "dfSFRdNtwrk2010 = gpd.GeoDataFrame(dfSFRdNtwrk2010, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfSFRdNtwrk2010=dfSFRdNtwrk2010.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # read the SF_Boundary file\n",
    "dfSFBoundary = gpd.read_file(BASE_DIR.parent.joinpath(\"Data\",\"SF_County\",\"SFBay_Boundary.shp\"))\n",
    "dfSFBoundary=dfSFBoundary.to_crs(\"EPSG:4326\")\n",
    "\n",
    "dfSFRdNtwrk2010.reset_index(drop=True,inplace=True)\n",
    "# # Overlay and select only links which are within the SF Bay Area Polygon\n",
    "dfSFRdNtwrk2010 = gpd.clip(dfSFRdNtwrk2010, dfSFBoundary)\n",
    "\n",
    "dfSFRdNtwrk2010.to_csv(BASE_DIR.parent.joinpath(folder_path,\"Raw_SFRdNtwrk_2010_cores.csv\"))\n",
    "\n",
    "# aggregate the dataframe using A_B\n",
    "wt_avg = lambda x: np.ma.average(x, weights = dfSFRdNtwrk2010.loc[x.index, \"Tot_Vol\"])\n",
    "\n",
    "lst_col = [\"SPEED\",\"TIME\",\"TIME_1\",\"CSPD_1\"]\n",
    "# average the columns\n",
    "avg_col = [ 'DISTANCE',\n",
    "            'CAP', \"FT\",\"AT\",\n",
    "            'TIMESEED',\n",
    "            'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM', \n",
    "            'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3', \n",
    "            'TOLLEA_DA', 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA',\n",
    "            'TOLLEV_SR2', 'TOLLEV_SR3',\"USE\",\"Tot_CAP\"]\n",
    "\n",
    "def agg_func(df):    \n",
    "    d = {}\n",
    "    for col in df.select_dtypes(np.number).columns:\n",
    "        if col in lst_col:\n",
    "            d[col] = wt_avg\n",
    "        elif col in avg_col:\n",
    "            d[col]=\"mean\"\n",
    "        else:\n",
    "            d[col] = \"sum\"\n",
    "    for col in df.select_dtypes(object).columns:\n",
    "        d[col] = \"first\"    \n",
    "    d[\"geometry\"] = \"first\"\n",
    "    return d\n",
    "\n",
    "dfSFRdNtwrk2010_agg = dfSFRdNtwrk2010.groupby(['A_B'],as_index=False).aggregate(agg_func(dfSFRdNtwrk2010.copy())).copy()\n",
    "# above merge converts the geo-dataframe to pandas dataframe. So re-convert it into geodataframe\n",
    "dfSFRdNtwrk2010_agg = gpd.GeoDataFrame(dfSFRdNtwrk2010_agg, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfSFRdNtwrk2010_agg=dfSFRdNtwrk2010_agg.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # Overlay and select only links which are within the SF Bay Area Polygon\n",
    "dfSFRdNtwrk2010_agg = gpd.clip(dfSFRdNtwrk2010_agg, dfSFBoundary)\n",
    "\n",
    "# export the geodataframe\n",
    "dfSFRdNtwrk2010_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_agg.geojson\"), driver='GeoJSON')\n",
    "dfSFRdNtwrk2010_agg.to_csv(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_agg.csv\"))\n",
    "\n",
    "# reproject the geodataframe to EPSG:3857\n",
    "dfSFRdNtwrk2010_agg = dfSFRdNtwrk2010_agg.to_crs(\"EPSG:3857\")\n",
    "dfSFRdNtwrk2010_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_agg_PCS.geojson\"), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20aa3eb",
   "metadata": {},
   "source": [
    "## Fetch SF Champ network for 2016 and join the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901adbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goyal\\.virtualenvs\\overpass_turbo-zwxzihl_\\lib\\site-packages\\numpy\\ma\\extras.py:624: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n"
     ]
    }
   ],
   "source": [
    "# read 2016 file\n",
    "dfSFRd2016am = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_AM.shp\"))\n",
    "dfSFRd2016am[\"peak\"]=\"AM\"\n",
    "dfSFRd2016am[\"Tot_CAP\"]=dfSFRd2016am[\"CAP\"]*3\n",
    "dfSFRd2016pm = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_PM.shp\"))\n",
    "dfSFRd2016pm[\"peak\"]=\"PM\"\n",
    "dfSFRd2016pm[\"Tot_CAP\"]=dfSFRd2016pm[\"CAP\"]*3\n",
    "dfSFRd2016ea = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_EA.shp\"))\n",
    "dfSFRd2016ea[\"peak\"]=\"EA\"\n",
    "dfSFRd2016ea[\"Tot_CAP\"]=dfSFRd2016ea[\"CAP\"]*3\n",
    "dfSFRd2016ev = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_EV.shp\"))\n",
    "dfSFRd2016ev[\"peak\"]=\"EV\"\n",
    "dfSFRd2016ev[\"Tot_CAP\"]=dfSFRd2016ev[\"CAP\"]*8.5\n",
    "dfSFRd2016md = gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_MD.shp\"))\n",
    "dfSFRd2016md[\"peak\"]=\"MD\"\n",
    "dfSFRd2016md[\"Tot_CAP\"]=dfSFRd2016md[\"CAP\"]*6.5\n",
    "\n",
    "dfSFRdNtwrk2016 = pd.concat([dfSFRd2016am,dfSFRd2016pm,dfSFRd2016ea,dfSFRd2016ev,dfSFRd2016md])\n",
    "\n",
    "# create empty columns: OOS, PUDO, Tot_Vol\n",
    "dfSFRdNtwrk2016 = dfSFRdNtwrk2016.assign(Tot_Vol=0)\n",
    "dfSFRdNtwrk2016[\"A_B\"] = dfSFRdNtwrk2016[\"A\"].astype(str)  + \"_\" + dfSFRdNtwrk2016[\"B\"].astype(str)\n",
    "dfSFRdNtwrk2016[\"A\"] = dfSFRdNtwrk2016[\"A\"].astype(str)\n",
    "dfSFRdNtwrk2016[\"B\"] = dfSFRdNtwrk2016[\"B\"].astype(str)\n",
    "# get the columns which together formm Tot_Vol\n",
    "add_2016 = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1',\n",
    "            'V13_1','V14_1','V15_1','V16_1','V17_1','V18_1','V19_1',\n",
    "            'BUSVOL_AM','BUSVOL_PM','BUSVOL_EA','BUSVOL_MD','BUSVOL_EV','OOS']\n",
    "# add them up\n",
    "dfSFRdNtwrk2016[\"Tot_Vol\"] = dfSFRdNtwrk2016[add_2016].sum(axis=1)\n",
    "\n",
    "# add to TNC Vol\n",
    "# TNC_2016 = ['V16_1','V17_1','V18_1','OOS'] # updated 26 Jan 2022 to reflect TNC volumes which were mis-represented\n",
    "TNC_2016 = ['V13_1','OOS'] # V13_1 is TNC_Volumnes plying on the road segment\n",
    "dfSFRdNtwrk2016[\"TNC_Tot_Vol\"] = dfSFRdNtwrk2016[TNC_2016].sum(axis=1)\n",
    "\n",
    "# Keep only FT types representing real road-network\n",
    "dfSFRdNtwrk2016=dfSFRdNtwrk2016[dfSFRdNtwrk2016.FT.isin([1,2,3,4,5,7,9,10,11,12,13,15])]\n",
    "# dfSFRdNtwrk2016=dfSFRdNtwrk2016[dfSFRdNtwrk2016.FT.isin([4,7,10,11,12,15])]\n",
    "                                \n",
    "# convert back to geopandas dataframe\n",
    "dfSFRdNtwrk2016 = gpd.GeoDataFrame(dfSFRdNtwrk2016, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfSFRdNtwrk2016=dfSFRdNtwrk2016.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # read the SF_Boundary file\n",
    "dfSFBoundary = gpd.read_file(BASE_DIR.parent.joinpath(\"Data\",\"SF_County\",\"SFBay_Boundary.shp\"))\n",
    "dfSFBoundary=dfSFBoundary.to_crs(\"EPSG:4326\")\n",
    "\n",
    "dfSFRdNtwrk2016.reset_index(drop=True,inplace=True)\n",
    "# # Overlay and select only links which are within the SF Bay Area Polygon\n",
    "dfSFRdNtwrk2016 = gpd.clip(dfSFRdNtwrk2016, dfSFBoundary)\n",
    "\n",
    "dfSFRdNtwrk2016.to_csv(BASE_DIR.parent.joinpath(folder_path,\"Raw_SFRdNtwrk_2016_cores.csv\"))\n",
    "\n",
    "# aggregate the dataframe using A_B\n",
    "wt_avg = lambda x: np.ma.average(x, weights = dfSFRdNtwrk2016.loc[x.index, \"Tot_Vol\"])\n",
    "\n",
    "lst_col = [\"SPEED\",\"TIME\",\"TIME_1\",\"CSPD_1\"]\n",
    "# average the columns\n",
    "avg_col = [ 'DISTANCE',\n",
    "            'CAP', \"FT\",\"AT\",\n",
    "            'TIMESEED',\n",
    "            'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM', \n",
    "            'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3', \n",
    "            'TOLLEA_DA', 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA',\n",
    "            'TOLLEV_SR2', 'TOLLEV_SR3',\"USE\",\"Tot_CAP\"]\n",
    "\n",
    "def agg_func(df):    \n",
    "    d = {}\n",
    "    for col in df.select_dtypes(np.number).columns:\n",
    "        if col in lst_col:\n",
    "            d[col] = wt_avg\n",
    "        elif col in avg_col:\n",
    "            d[col]=\"mean\"\n",
    "        else:\n",
    "            d[col] = \"sum\"\n",
    "    for col in df.select_dtypes(object).columns:\n",
    "        d[col] = \"first\"    \n",
    "    d[\"geometry\"] = \"first\"\n",
    "    return d\n",
    "\n",
    "dfSFRdNtwrk2016_agg = dfSFRdNtwrk2016.groupby(['A_B'],as_index=False).aggregate(agg_func(dfSFRdNtwrk2016.copy())).copy()\n",
    "# above merge converts the geo-dataframe to pandas dataframe. So re-convert it into geodataframe\n",
    "dfSFRdNtwrk2016_agg = gpd.GeoDataFrame(dfSFRdNtwrk2016_agg, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfSFRdNtwrk2016_agg=dfSFRdNtwrk2016_agg.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # Overlay and select only links which are within the SF Bay Area Polygon\n",
    "dfSFRdNtwrk2016_agg = gpd.clip(dfSFRdNtwrk2016_agg, dfSFBoundary)\n",
    "# export the geodataframe\n",
    "dfSFRdNtwrk2016_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_agg.geojson\"), driver='GeoJSON')\n",
    "dfSFRdNtwrk2016_agg.to_csv(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_agg.csv\"))\n",
    "\n",
    "# reproject the geodataframe to EPSG:3857\n",
    "dfSFRdNtwrk2016_agg = dfSFRdNtwrk2016_agg.to_crs(\"EPSG:3857\")\n",
    "dfSFRdNtwrk2016_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_agg_PCS.geojson\"), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Also reproject the Census Tract file to EPSG:3857\n",
    "dfSF_CensusTract = gpd.read_file(BASE_DIR.parent.joinpath(\"CensusTract\",\"SF_CT_2010.geojson\"))\n",
    "dfSF_CensusTract = dfSF_CensusTract.to_crs(\"EPSG:3857\")\n",
    "dfSF_CensusTract.to_file(BASE_DIR.parent.joinpath(folder_path,\"SF_CensusTract_PCS.geojson\"), driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check in QGIS if the re-projection is successful .i.e.\n",
    "# 1. Both SFChamp_2010_agg.geojson and SFChamp_2016_agg.geojson into EPSG:3857, are named with _PCS suffix\n",
    "# 2. The SF_CensusTract to EPSG:3857, is also name with _PCS suffix\n",
    "\n",
    "# After the above process do the following in QGIS\n",
    "# 3. Intersect with SF_CensusTract and RoadNetwork and save the output as SFChamp_201x_agg_CT_PCS.geojson\n",
    "# 4. Road Crash for each year i.e. 2010 and 2016, convert it to EPSG:3857, name it SFCrash_2010_PCS.geojson & SFCrash_2016_PCS.geojson\n",
    "# 5. Perform NearestNeighbour join of respective year road crash with respective year road network. Name them as NN_SFCrash_2010_PCS & NN_SFCrash_2016_PCS.\n",
    "#    Keep only FT, A_B, A, B attribute from roadnetwork in the output file\n",
    "# 6. Intersect NN_SFCrash_2010_PCS & NN_SFCrash_2016_PCS with SF_CensusTract, name the file as NN_SFCrash_CT_2010_PCS and NN_SFCrash_CT_2016_PCS.\n",
    "#    Keep only tractce10 attribute from SF_CensusTract in the output file\n",
    "# This ends QGIS manipulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Fetch SF_Census Tract, SF_RoadNetwork and SF_RoadCrash\n",
    "SF_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SF_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "# road network\n",
    "dfSFRdNtwrk2010_agg_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_agg_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2010_agg_CT.fillna(dfSFRdNtwrk2010_agg_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFRdNtwrk2016_agg_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_agg_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk2016_agg_CT.fillna(dfSFRdNtwrk2016_agg_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "# road crashes\n",
    "dfSFCrash2010_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"NN_SFCrash_CT_2010_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2010_CT.fillna(dfSFCrash2010_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "dfSFCrash2016_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"NN_SFCrash_CT_2016_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash2016_CT.fillna(dfSFCrash2016_CT.dtypes.replace({'float64': 0.0, 'O': 'NULL'}),downcast='infer', inplace=True)\n",
    "\n",
    "#remember to rename DISTANCE variable (as this is no longer the actual distance (in miles), given that feature is split-up)\n",
    "dfSFRdNtwrk2010_agg_CT.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2010_agg_CT[\"Length_meters\"] = dfSFRdNtwrk2010_agg_CT.geometry.length\n",
    "dfSFRdNtwrk2016_agg_CT.rename(columns={\"DISTANCE\":\"DISTANCE_MILES\"},inplace=True)\n",
    "dfSFRdNtwrk2016_agg_CT[\"Length_meters\"] = dfSFRdNtwrk2016_agg_CT.geometry.length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## attaching TMCEquiv for future usage, possible\n",
    "# read the TMC file to map A_B <--> TMCEquiv\n",
    "dfTMCEquiv = pd.read_csv(BASE_DIR.parent.joinpath(\"TMC\", 'CHAMP_ModifiedTMC_Equiv.csv'))\n",
    "dfEST2010 = pd.read_csv(BASE_DIR.parent.joinpath(\"TMC\",'ESTFILE_2010.csv'))\n",
    "dfEST2016 = pd.read_csv(BASE_DIR.parent.joinpath(\"TMC\", 'ESTFILE_2016.csv'))\n",
    "\n",
    "# Map the road network \"A_B\" with \"TMCLink\" using a lambda\n",
    "TMCEquiv_dict = dict(zip(dfTMCEquiv[\"A_B\"],dfTMCEquiv[\"ModifiedTMC\"]))\n",
    "dfSFRdNtwrk2010_agg_CT[\"TMCEquiv\"] = dfSFRdNtwrk2010_agg_CT[\"A_B\"].map(TMCEquiv_dict) # this geodataframe only contains \"A_B\" & \"TMC_Equiv\" fields\n",
    "dfSFRdNtwrk2010_agg_CT[\"TMCEquiv\"] = dfSFRdNtwrk2010_agg_CT[\"A_B\"].map(TMCEquiv_dict) # this geodataframe only contains \"A_B\" & \"TMC_Equiv\" fields"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "### drop all columns which have information related to links except A_B, FT, AT.\n",
    "# Also drop all columns which have information related to Census Tract except \"tractce\"\n",
    "# on crash feature class\n",
    "rename_col = { \"join_A_B\": \"A_B\",\n",
    "               \"join_FT\": \"FT\"}\n",
    "dfSFCrash2010_CT.rename(columns=rename_col,inplace=True)\n",
    "unwanted = dfSFCrash2010_CT.columns[dfSFCrash2010_CT.columns.str.startswith('join_')]\n",
    "dfSFCrash2010_CT.drop(unwanted, axis=1, inplace=True)\n",
    "dfSFCrash2010_CT.drop(columns=\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "dfSFCrash2016_CT.rename(columns=rename_col,inplace=True)\n",
    "unwanted = dfSFCrash2016_CT.columns[dfSFCrash2016_CT.columns.str.startswith('join_')]\n",
    "dfSFCrash2016_CT.drop(unwanted, axis=1, inplace=True)\n",
    "dfSFCrash2016_CT.drop(columns=\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "### create different categories of the network and crashes - in our case full network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# for road network\n",
    "# save full network as a featureclass\n",
    "# for cat in [1,2,3]:\n",
    "# filter the road network by category value\n",
    "dfSFRdNtwrk2010_agg_CT.to_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2010_fullnetwork.geojson\"),driver=\"GeoJSON\")\n",
    "dfSFRdNtwrk2016_agg_CT.to_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2016_fullnetwork.geojson\"),driver=\"GeoJSON\")\n",
    "# filter the road crash by category value\n",
    "dfSFCrash2010_CT[(dfSFCrash2010_CT[\"D2NL\"]<=10)].to_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFCrash_CT_2010_fullnetwork.geojson\"),driver=\"GeoJSON\")\n",
    "dfSFCrash2016_CT[(dfSFCrash2016_CT[\"D2NL\"]<=10)].to_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFCrash_CT_2016_fullnetwork.geojson\"),driver=\"GeoJSON\")\n",
    "\n",
    "# export it to CSV\n",
    "# for cat in [1,2,3]:\n",
    "# export to CSV\n",
    "dfSFRdNtwrk2010_agg_CT.loc[:, ~dfSFRdNtwrk2010_agg_CT.columns.isin(['geometry'])].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2010_fullnetwork.csv\"))\n",
    "dfSFRdNtwrk2016_agg_CT.loc[:, ~dfSFRdNtwrk2016_agg_CT.columns.isin(['geometry'])].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2016_fullnetwork.csv\"))\n",
    "dfSFCrash2010_CT[(dfSFCrash2010_CT[\"D2NL\"]<=10)].loc[:,~dfSFCrash2010_CT.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFCrash_CT_2010_fullnetwork.csv\"))\n",
    "dfSFCrash2016_CT[(dfSFCrash2016_CT[\"D2NL\"]<=10)].loc[:,~dfSFCrash2016_CT.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFCrash_CT_2016_fullnetwork.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "### Above exported dataframes are exactly what we want.\n",
    "# Now aggregate variables in each dataframe according to \"censustract\" it belongs\n",
    "# merge them with Census Tract shapefile to get geodataframe\n",
    "# merge both crash and road network with Census Tract to help explore visually"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### aggregate the road network along the census_tract IDs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def agg_network_CT(_df):\n",
    "    _df [\"A\"] = _df[\"A\"].astype(str)\n",
    "    _df[\"B\"] = _df[\"B\"].astype(str)\n",
    "    _df [\"A_B\"] = _df[\"A_B\"].astype(str)\n",
    "    #     _df[\"AT\"] = _df[\"AT\"].astype(str)\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    #     _df[\"category\"] = _df[\"category\"].astype(str)\n",
    "    # aggregate the dataframe using A_B\n",
    "    wt_avg = lambda x: np.ma.average(x, weights = _df.loc[x.index, \"Tot_Vol\"])\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            if col in wt_col:\n",
    "                d[col] = wt_avg\n",
    "            else:\n",
    "                d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "\n",
    "    wt_col = [\"SPEED\",\"TIME\",\"CSPD_1\"]\n",
    "    sum_col = [ \"CAP\", \"DISTANCE_MILES\",'Length_meters','V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                'VT_1', 'V1T_1', 'V2T_1', 'V3T_1', 'V4T_1', 'V5T_1', 'V6T_1', 'V7T_1', 'V8T_1', 'V9T_1', 'V10T_1', 'V11T_1','V12T_1',\"V13T_1\",'V14T_1', 'V15T_1',\"V16T_1\",'V17T_1', 'V18T_1',\"V19T_1\",\n",
    "                'OOS', 'PUDO', 'Tot_Vol',\"TNC_Tot_Vol\",\"TIME_1\",'TIMESEED',\"Tot_CAP\"]\n",
    "\n",
    "    str_col = ['tractce10',\"TMCEquiv\"]\n",
    "    concat_col = [\"A_B\",\"FT\"]\n",
    "    drop_col = [ 'A', 'B',\"USE\",'PER_RISE', 'ONEWAY',\"TOLL\",'PROJ', 'ACTION', 'AB','peak',\n",
    "                 'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3', 'TOLLEA_DA',\n",
    "                 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA', 'TOLLEV_SR2', 'TOLLEV_SR3',\n",
    "                 'DTA_EDIT_F', 'TOLLTIME', 'PHASE', 'AMBUSSAVE', 'MDBUSSAVE', 'PMBUSSAVE', 'EVBUSSAVE', 'EABUSSAVE', 'SPDC', 'CAPC',\n",
    "                 'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',\n",
    "                 'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "                 'STREETNAME', 'TYPE', 'MTYPE','TSIN']\n",
    "    #                  'statefp10','mtfcc10','name10','intptlat10','awater10','namelsad10','funcstat10','aland10','geoid10','intptlon10','countyfp10']\n",
    "\n",
    "    _df.drop(drop_col,axis=1,inplace=True)\n",
    "    df = _df.groupby(['tractce10'],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "    return df\n",
    "\n",
    "# aggregate the road network information according to the CT it belongs\n",
    "# for cat in [1,2,3]:\n",
    "# census tract #\n",
    "SF_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SF_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "gdfRdNtwrk = gpd.read_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2010_fullnetwork.geojson\"))\n",
    "gdfRdNtwrk = gdfRdNtwrk.assign(V13_1=0, V14_1=0, V15_1=0, V16_1=0, V17_1=0, V18_1=0, V19_1=0,\n",
    "                               V13T_1=0, V14T_1=0, V15T_1=0, V16T_1=0, V17T_1=0, V18T_1=0, V19T_1=0,)\n",
    "gdfRdNtwrk_agg = agg_network_CT(gdfRdNtwrk)\n",
    "# calculate some additional fields\n",
    "gdfRdNtwrk_agg[\"Tot_Vol_yr\"] = gdfRdNtwrk_agg[\"Tot_Vol\"]*365\n",
    "# Calculate Vehicle Distance Travelled\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"Tot_Vol\"]*0.000621371\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_yr\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"Tot_Vol\"]*0.000621371*365\n",
    "# Calculate congested speed\n",
    "gdfRdNtwrk_agg[\"CONGESTED_SPEED\"] = (((gdfRdNtwrk_agg[\"Length_meters\"]*0.000621371).divide(gdfRdNtwrk_agg[\"TIME_1\"]))*60)\n",
    "gdfRdNtwrk_agg[\"CONGESTED_SPEED_yr\"] = (((gdfRdNtwrk_agg[\"Length_meters\"]*0.000621371).divide(gdfRdNtwrk_agg[\"TIME_1\"]))*60)\n",
    "# Calculate TNC Volume\n",
    "cols = ['V13_1',\"OOS\"]\n",
    "gdfRdNtwrk_agg[\"TNC_VOLUME\"] = gdfRdNtwrk_agg[cols].sum(axis=1)\n",
    "gdfRdNtwrk_agg[\"TNC_VOLUME_yr\"] = gdfRdNtwrk_agg[\"TNC_VOLUME\"]*365\n",
    "\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_TNC\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"TNC_VOLUME\"]*0.000621371\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_TNC_yr\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"TNC_VOLUME\"]*0.000621371*365\n",
    "\n",
    "gdfRdNtwrk_agg[\"NON_TNC_VOLUME\"] = gdfRdNtwrk_agg[\"Tot_Vol\"] - gdfRdNtwrk_agg[\"TNC_VOLUME\"]\n",
    "gdfRdNtwrk_agg[\"NON_TNC_VOLUME_yr\"] = gdfRdNtwrk_agg[\"Tot_Vol_yr\"] - gdfRdNtwrk_agg[\"TNC_VOLUME_yr\"]\n",
    "\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_NON_TNC\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"NON_TNC_VOLUME\"]*0.000621371\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_NON_TNC_yr\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"NON_TNC_VOLUME\"]*0.000621371*365\n",
    "\n",
    "gdfRdNtwrk_agg[\"PUDO_yr\"] = gdfRdNtwrk_agg[\"PUDO\"]*365\n",
    "\n",
    "# Calculate - PCT of the TNC Vol, Pick-up & Drop-off and VC Ratio on the segment\n",
    "gdfRdNtwrk_agg[\"PCT_TNC_VOL\"] = 0\n",
    "gdfRdNtwrk_agg[\"PCT_TNC_PUDO\"] = 0\n",
    "gdfRdNtwrk_agg[\"PCT_TNC_VOL\"] = gdfRdNtwrk_agg[\"TNC_VOLUME\"].divide(gdfRdNtwrk_agg[\"Tot_Vol\"])\n",
    "gdfRdNtwrk_agg[\"PCT_TNC_PUDO\"] = gdfRdNtwrk_agg[\"PUDO\"].divide(gdfRdNtwrk_agg[\"Tot_Vol\"])\n",
    "gdfRdNtwrk_agg[\"VC_ratio\"] = gdfRdNtwrk_agg[\"Tot_Vol\"].divide(gdfRdNtwrk_agg[\"Tot_CAP\"])\n",
    "gdfRdNtwrk_agg.to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2010_fullnetwork_agg.csv\"))\n",
    "# convert the db2010 and db2016 as geojson files\n",
    "SF_CT_2010 = SF_CT.merge(gdfRdNtwrk_agg,on='tractce10',how=\"left\")\n",
    "# save the file\n",
    "unwanted_cols = ['statefp10','mtfcc10','name10','intptlat10','awater10','namelsad10','funcstat10','aland10','geoid10','intptlon10','countyfp10']\n",
    "SF_CT_2010.loc[:,~SF_CT_2010.columns.isin(unwanted_cols)].to_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2010_fullnetwork_agg.geojson\"),driver=\"GeoJSON\")\n",
    "# save the file as csv\n",
    "SF_CT_2010.loc[:,~SF_CT_2010.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2010_fullnetwork_agg_merged.csv\"))\n",
    "\n",
    "# for cat in [1,2,3]:\n",
    "# census tract #\n",
    "SF_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SF_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "gdfRdNtwrk = gpd.read_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2016_fullnetwork.geojson\"))\n",
    "gdfRdNtwrk_agg = agg_network_CT(gdfRdNtwrk)\n",
    "# calculate some additional fields\n",
    "gdfRdNtwrk_agg[\"Tot_Vol_yr\"] = gdfRdNtwrk_agg[\"Tot_Vol\"]*365\n",
    "# Calculate Vehicle Distance Travelled\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"Tot_Vol\"]*0.000621371\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_yr\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"Tot_Vol\"]*0.000621371*365\n",
    "# Calculate congested speed\n",
    "gdfRdNtwrk_agg[\"CONGESTED_SPEED\"] = (((gdfRdNtwrk_agg[\"Length_meters\"]*0.000621371).divide(gdfRdNtwrk_agg[\"TIME_1\"]))*60)\n",
    "gdfRdNtwrk_agg[\"CONGESTED_SPEED_yr\"] = (((gdfRdNtwrk_agg[\"Length_meters\"]*0.000621371).divide(gdfRdNtwrk_agg[\"TIME_1\"]))*60)\n",
    "# Calculate TNC Volume\n",
    "cols = ['V13_1',\"OOS\"]\n",
    "gdfRdNtwrk_agg[\"TNC_VOLUME\"] = gdfRdNtwrk_agg[cols].sum(axis=1)\n",
    "gdfRdNtwrk_agg[\"TNC_VOLUME_yr\"] = gdfRdNtwrk_agg[\"TNC_VOLUME\"]*365\n",
    "\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_TNC\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"TNC_VOLUME\"]*0.000621371\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_TNC_yr\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"TNC_VOLUME\"]*0.000621371*365\n",
    "\n",
    "gdfRdNtwrk_agg[\"NON_TNC_VOLUME\"] = gdfRdNtwrk_agg[\"Tot_Vol\"] - gdfRdNtwrk_agg[\"TNC_VOLUME\"]\n",
    "gdfRdNtwrk_agg[\"NON_TNC_VOLUME_yr\"] = gdfRdNtwrk_agg[\"Tot_Vol_yr\"] - gdfRdNtwrk_agg[\"TNC_VOLUME_yr\"]\n",
    "\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_NON_TNC\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"NON_TNC_VOLUME\"]*0.000621371\n",
    "gdfRdNtwrk_agg[\"Veh_Dist_Travel_NON_TNC_yr\"] = gdfRdNtwrk_agg[\"Length_meters\"]*gdfRdNtwrk_agg[\"NON_TNC_VOLUME\"]*0.000621371*365\n",
    "\n",
    "gdfRdNtwrk_agg[\"PUDO_yr\"] = gdfRdNtwrk_agg[\"PUDO\"]*365\n",
    "\n",
    "# Calculate - PCT of the TNC Vol, Pick-up & Drop-off and VC Ratio on the segment\n",
    "gdfRdNtwrk_agg[\"PCT_TNC_VOL\"] = 0\n",
    "gdfRdNtwrk_agg[\"PCT_TNC_PUDO\"] = 0\n",
    "gdfRdNtwrk_agg[\"PCT_TNC_VOL\"] = gdfRdNtwrk_agg[\"TNC_VOLUME\"].divide(gdfRdNtwrk_agg[\"Tot_Vol\"])\n",
    "gdfRdNtwrk_agg[\"PCT_TNC_PUDO\"] = gdfRdNtwrk_agg[\"PUDO\"].divide(gdfRdNtwrk_agg[\"Tot_Vol\"])\n",
    "gdfRdNtwrk_agg[\"VC_ratio\"] = gdfRdNtwrk_agg[\"Tot_Vol\"].divide(gdfRdNtwrk_agg[\"Tot_CAP\"])\n",
    "gdfRdNtwrk_agg.to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2016_fullnetwork_agg.csv\"))\n",
    "# convert the db2010 and db2016 as geojson files\n",
    "SF_CT_2016 = SF_CT.merge(gdfRdNtwrk_agg,on='tractce10',how=\"left\")\n",
    "# save the file as geodataframe\n",
    "unwanted_cols = ['statefp10','mtfcc10','name10','intptlat10','awater10','namelsad10','funcstat10','aland10','geoid10','intptlon10','countyfp10']\n",
    "SF_CT_2016.loc[:,~SF_CT_2016.columns.isin(unwanted_cols)].to_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2016_fullnetwork_agg.geojson\"),driver=\"GeoJSON\")\n",
    "# save the file as csv\n",
    "SF_CT_2016.loc[:,~SF_CT_2016.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2016_fullnetwork_agg.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "### aggregate the road CRASH along the census_tract IDs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# aggregate crashes along the CensusTract\n",
    "def agg_crash_CT(_df):\n",
    "    _df [\"A_B\"] = _df[\"A_B\"].astype(str)\n",
    "    #     _df[\"category\"] = _df[\"category\"].astype(str)\n",
    "    _df[\"CASE_ID\"] = _df[\"CASE_ID\"].astype(str)\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    concat_col = [\"A_B\",\"CASE_ID\",\"FT\"]\n",
    "    str_col = ['tractce10']\n",
    "    sum_col = ['NUMBER_KILLED', 'NUMBER_INJURED','COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED',\n",
    "               'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED','Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury',\n",
    "               'COUNT_Visible_Injury', 'COUNT_Other_Injury']\n",
    "    drop_col = [ 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'REPORTING_DISTRICT', 'DAY_OF_WEEK', 'CHP_SHIFT', 'POPULATION',\n",
    "                 'CNTY_CITY_LOC', 'SPECIAL_COND', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'CITY_DIVISION_LAPD', 'CHP_BEAT_CLASS', 'BEAT_NUMBER', 'PRIMARY_RD', 'SECONDARY_RD',\n",
    "                 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'WEATHER_1', 'WEATHER_2', 'STATE_HWY_IND', 'CALTRANS_COUNTY', 'CALTRANS_DISTRICT', 'STATE_ROUTE', 'ROUTE_SUFFIX',\n",
    "                 'POSTMILE_PREFIX', 'POSTMILE', 'LOCATION_TYPE', 'RAMP_INTERSECTION', 'SIDE_OF_HWY', 'TOW_AWAY', 'COLLISION_SEVERITY', 'PARTY_COUNT', 'PRIMARY_COLL_FACTOR',\n",
    "                 'PCF_CODE_OF_VIOL', 'PCF_VIOL_CATEGORY', 'PCF_VIOLATION', 'PCF_VIOL_SUBSECTION', 'HIT_AND_RUN', 'TYPE_OF_COLLISION', 'MVIW', 'PED_ACTION', 'ROAD_SURFACE',\n",
    "                 'ROAD_COND_1', 'ROAD_COND_2', 'LIGHTING', 'CONTROL_DEVICE', 'CHP_ROAD_TYPE', 'PEDESTRIAN_ACCIDENT', 'BICYCLE_ACCIDENT', 'MOTORCYCLE_ACCIDENT',\n",
    "                 'TRUCK_ACCIDENT', 'NOT_PRIVATE_PROPERTY', 'ALCOHOL_INVOLVED', 'STWD_VEHTYPE_AT_FAULT', 'CHP_VEHTYPE_AT_FAULT',  'PRIMARY_RAMP', 'SECONDARY_RAMP',\n",
    "                 'LATITUDE', 'LONGITUDE', 'COUNTY', 'CITY', 'POINT_X', 'POINT_Y', 'PRIMARY_RD_3', 'SECONDARY_RD_3','rdntwrk_tractce10', 'D2NL']\n",
    "\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "\n",
    "    _df.drop(drop_col,axis=1,inplace=True)\n",
    "    df = _df.groupby(['tractce10'],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "    return df\n",
    "\n",
    "# aggregate the road crash information according to the CT it belongs\n",
    "# for cat in [1,2,3]:\n",
    "# 2010 Crash\n",
    "gdfCrash = gpd.read_file(BASE_DIR.parent.joinpath(folder_path.parent.parent,\"census_tract\",\"by_FTs\",\"SFCrash_CT_2010_fullnetwork.geojson\"))\n",
    "gdfCrash_agg = agg_crash_CT(gdfCrash)\n",
    "gdfCrash_agg[\"ACCIDENT_YEAR\"]=2010\n",
    "gdfCrash_agg.to_csv(BASE_DIR.parent.joinpath(folder_path.parent.parent,\"census_tract\",\"by_FTs\",\"SFCrash_CT_2010_fullnetwork_agg.csv\"))\n",
    "# convert this as geojson files\n",
    "SF_CT_2010 = SF_CT.merge(gdfCrash_agg,on='tractce10',how=\"left\")\n",
    "# save the file\n",
    "unwanted_cols = ['statefp10','mtfcc10','name10','intptlat10','awater10','namelsad10','funcstat10','aland10','geoid10','intptlon10','countyfp10']\n",
    "SF_CT_2010.loc[:,~SF_CT_2010.columns.isin(unwanted_cols)].to_file(BASE_DIR.parent.joinpath(folder_path.parent.parent,\"census_tract\",\"by_FTs\",\"SFCrash_CT_2010_fullnetwork_agg.geojson\"),driver=\"GeoJSON\")\n",
    "# save the file as csv\n",
    "unwanted_cols = ['statefp10','mtfcc10','name10','intptlat10','awater10','namelsad10','funcstat10','aland10','geoid10','intptlon10','countyfp10',\"geometry\"]\n",
    "SF_CT_2010.loc[:,~SF_CT_2010.columns.isin(unwanted_cols)].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFCrash_CT_2010_fullnetwork_agg_merged.csv\"))\n",
    "\n",
    "# for cat in [1,2,3]:\n",
    "# 2016 Crash\n",
    "gdfCrash = gpd.read_file(BASE_DIR.parent.joinpath(folder_path.parent.parent,\"census_tract\",\"by_FTs\",\"SFCrash_CT_2016_fullnetwork.geojson\"))\n",
    "gdfCrash_agg = agg_crash_CT(gdfCrash)\n",
    "gdfCrash_agg[\"ACCIDENT_YEAR\"]=2016\n",
    "gdfCrash_agg.to_csv(BASE_DIR.parent.joinpath(folder_path.parent.parent,\"census_tract\",\"by_FTs\",\"SFCrash_CT_2016_fullnetwork_agg.csv\"))\n",
    "# convert this as geojson files\n",
    "SF_CT_2016 = SF_CT.merge(gdfCrash_agg,on='tractce10',how=\"left\")\n",
    "# save the file\n",
    "unwanted_cols = ['statefp10','mtfcc10','name10','intptlat10','awater10','namelsad10','funcstat10','aland10','geoid10','intptlon10','countyfp10']\n",
    "SF_CT_2016.loc[:,~SF_CT_2016.columns.isin(unwanted_cols)].to_file(BASE_DIR.parent.joinpath(folder_path.parent.parent,\"census_tract\",\"by_FTs\",\"SFCrash_CT_2016_fullnetwork_agg.geojson\"),driver=\"GeoJSON\")\n",
    "# save the file as csv\n",
    "unwanted_cols = ['statefp10','mtfcc10','name10','intptlat10','awater10','namelsad10','funcstat10','aland10','geoid10','intptlon10','countyfp10',\"geometry\"]\n",
    "SF_CT_2016.loc[:,~SF_CT_2016.columns.isin(unwanted_cols)].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFCrash_CT_2016_fullnetwork_agg_merged.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# merge the network and the road crash"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Census Tract file\n",
    "dfSFCT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SF_CT_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "\n",
    "# for cat in [1,2,3]:\n",
    "gpdSFCrash = gpd.read_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFCrash_CT_2010_fullnetwork_agg.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash = pd.DataFrame(gpdSFCrash.drop(columns=\"geometry\"),copy=True)\n",
    "gpdSFRdNtwrk = gpd.read_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2010_fullnetwork_agg.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk = pd.DataFrame(gpdSFRdNtwrk.drop(columns=\"geometry\"),copy=True)\n",
    "gpd_CT_Crash = dfSFCT.merge(dfSFCrash,on=\"tractce10\",how=\"left\")\n",
    "gpd_CT_Crash[\"ACCIDENT_YEAR\"]=2010\n",
    "#convert back to geopandas dataframe\n",
    "# gpd_CT_Crash = gpd.GeoDataFrame(gpd_CT_Crash, geometry='geometry_x',crs=\"EPSG:4326\")\n",
    "gpd_CT_Crash_Rdntwrk = gpd_CT_Crash.merge(dfSFRdNtwrk,on=\"tractce10\",how=\"left\")\n",
    "gpd_CT_Crash_Rdntwrk[\"ACCIDENT_YEAR\"]=2010\n",
    "gpd_CT_Crash_Rdntwrk.to_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SF_merge_CT_2010_fullnetwork_agg.geojson\"),driver=\"GeoJSON\")\n",
    "gpd_CT_Crash_Rdntwrk.loc[:,~gpd_CT_Crash_Rdntwrk.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SF_merge_CT_2010_fullnetwork_agg.csv\"))\n",
    "\n",
    "# for cat in [1,2,3]:\n",
    "gpdSFCrash = gpd.read_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFCrash_CT_2016_fullnetwork_agg.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFCrash = pd.DataFrame(gpdSFCrash.drop(columns=\"geometry\"),copy=True)\n",
    "gpdSFRdNtwrk = gpd.read_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SFRdNtwrk_CT_2016_fullnetwork_agg.geojson\"), crs = \"EPSG:3857\")\n",
    "dfSFRdNtwrk = pd.DataFrame(gpdSFRdNtwrk.drop(columns=\"geometry\"),copy=True)\n",
    "gpd_CT_Crash = dfSFCT.merge(dfSFCrash,on=\"tractce10\",how=\"left\")\n",
    "gpd_CT_Crash[\"ACCIDENT_YEAR\"]=2016\n",
    "#convert back to geopandas dataframe\n",
    "# gpd_CT_Crash = gpd.GeoDataFrame(gpd_CT_Crash, geometry='geometry_x',crs=\"EPSG:4326\")\n",
    "gpd_CT_Crash_Rdntwrk = gpd_CT_Crash.merge(dfSFRdNtwrk,on=\"tractce10\",how=\"left\")\n",
    "gpd_CT_Crash_Rdntwrk[\"ACCIDENT_YEAR\"]=2016\n",
    "gpd_CT_Crash_Rdntwrk.to_file(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SF_merge_CT_2016_fullnetwork_agg.geojson\"),driver=\"GeoJSON\")\n",
    "gpd_CT_Crash_Rdntwrk.loc[:,~gpd_CT_Crash_Rdntwrk.columns.isin([\"geometry\"])].to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SF_merge_CT_2016_fullnetwork_agg.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df2010 = pd.read_csv((BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SF_merge_CT_2010_fullnetwork_agg.csv\")))\n",
    "df2016 = pd.read_csv((BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"by_FTs\",\"SF_merge_CT_2016_fullnetwork_agg.csv\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "dfmerged = pd.concat([df2010,df2016], ignore_index=True)\n",
    "dfmerged.fillna(0,inplace=True)\n",
    "# Calculate TNC Volume\n",
    "cols = [\"V13_1\",\"OOS\"]\n",
    "dfmerged[\"TNC_VOLUME\"]= dfmerged[cols].sum(axis=1)\n",
    "dfmerged[\"TNC_VOLUME_yr\"]= dfmerged[\"TNC_VOLUME\"]*365\n",
    "\n",
    "dfmerged[\"TNC_Tot_Vol\"]= dfmerged[cols].sum(axis=1)\n",
    "dfmerged[\"TNC_Tot_Vol_yr\"]= dfmerged[\"TNC_Tot_Vol\"]*365\n",
    "\n",
    "dfmerged[\"Tot_Vol_yr\"] = dfmerged[\"Tot_Vol\"] * 365\n",
    "dfmerged[\"PUDO_yr\"] = dfmerged[\"PUDO\"]*365\n",
    "\n",
    "dfmerged[\"NON_TNC_VOLUME\"] = dfmerged[\"Tot_Vol\"] - dfmerged[\"TNC_VOLUME\"]\n",
    "dfmerged[\"NON_TNC_VOLUME_yr\"] = dfmerged[\"Tot_Vol_yr\"] - dfmerged[\"TNC_VOLUME_yr\"]\n",
    "\n",
    "# Calculate VDTs\n",
    "dfmerged[\"Veh_Dist_Travel_TNC\"] = dfmerged[\"Length_meters\"]*dfmerged[\"TNC_VOLUME\"]*0.000621371\n",
    "dfmerged[\"Veh_Dist_Travel_NON_TNC\"] = dfmerged[\"Length_meters\"]*dfmerged[\"NON_TNC_VOLUME\"]*0.000621371\n",
    "\n",
    "dfmerged[\"Veh_Dist_Travel_TNC_yr\"] = dfmerged[\"Length_meters\"]*dfmerged[\"TNC_VOLUME\"]*0.000621371*365\n",
    "dfmerged[\"Veh_Dist_Travel_NON_TNC_yr\"] = dfmerged[\"Length_meters\"]*dfmerged[\"NON_TNC_VOLUME\"]*0.000621371*365\n",
    "\n",
    "# Calculate - PCT of the TNC Vol, Pick-up & Drop-off and VC Ratio on the segment\n",
    "dfmerged[\"PCT_TNC_VOL\"] = 0\n",
    "dfmerged[\"PCT_TNC_PUDO\"] = 0\n",
    "dfmerged[\"PCT_TNC_VOL\"] = dfmerged[\"TNC_VOLUME\"].divide(dfmerged[\"Tot_Vol\"])\n",
    "dfmerged[\"PCT_PUDO_Tot_Vol\"] = dfmerged[\"PUDO\"].divide(dfmerged[\"Tot_Vol\"])\n",
    "dfmerged[\"PCT_PUDO_TNC_Vol\"] = dfmerged[\"PUDO\"].divide(dfmerged[\"TNC_VOLUME\"])\n",
    "\n",
    "dfmerged[\"PCT_TNC_VOL_yr\"] = 0\n",
    "dfmerged[\"PCT_TNC_PUDO_yr\"] = 0\n",
    "dfmerged[\"PCT_TNC_VOL_yr\"] = dfmerged[\"TNC_VOLUME_yr\"].divide(dfmerged[\"Tot_Vol_yr\"])\n",
    "dfmerged[\"PCT_PUDO_Tot_Vol_yr\"] = dfmerged[\"PUDO_yr\"].divide(dfmerged[\"Tot_Vol_yr\"])\n",
    "dfmerged[\"PCT_PUDO_TNC_Vol_yr\"] = dfmerged[\"PUDO\"].divide(dfmerged[\"TNC_VOLUME\"])\n",
    "\n",
    "# PUDO\n",
    "dfmerged[\"PUDO_millions\"] = dfmerged[\"PUDO\"].divide(1000000)\n",
    "dfmerged[\"log_PUDO_millions\"] = np.log(dfmerged[\"PUDO_millions\"]+1)\n",
    "\n",
    "dfmerged[\"PUDO_millions_yr\"] = dfmerged[\"PUDO\"].divide(1000000)*365\n",
    "dfmerged[\"log_PUDO_millions_yr\"] = np.log((dfmerged[\"PUDO_millions\"]*365)+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Tot_Vol: log, square of and log of the square of\n",
    "dfmerged[\"log_Tot_Vol\"] = np.log(dfmerged[\"Tot_Vol\"]+1)\n",
    "dfmerged[\"Tot_Vol_SQR\"] = dfmerged[\"Tot_Vol\"]*dfmerged[\"Tot_Vol\"]\n",
    "dfmerged[\"log_Tot_Vol_SQR\"] = np.log(dfmerged[\"Tot_Vol_SQR\"]+1)\n",
    "\n",
    "# TNC_Vol: log, square of and log of the square of\n",
    "dfmerged[\"log_TNC_VOLUME\"] = np.log(dfmerged[\"TNC_VOLUME\"]+1)\n",
    "dfmerged[\"TNC_VOLUME_SQR\"] = dfmerged[\"TNC_VOLUME\"]*dfmerged[\"TNC_VOLUME\"]\n",
    "dfmerged[\"log_TNC_VOLUME_SQR\"] = np.log(dfmerged[\"TNC_VOLUME_SQR\"]+1)\n",
    "\n",
    "# Non_TNC_Vol: log, square of and log of the square of\n",
    "dfmerged[\"log_NON_TNC_VOLUME\"] = np.log(dfmerged[\"NON_TNC_VOLUME\"]+1)\n",
    "dfmerged[\"NON_TNC_VOLUME_SQR\"] = dfmerged[\"NON_TNC_VOLUME\"]*dfmerged[\"NON_TNC_VOLUME\"]\n",
    "dfmerged[\"log_NON_TNC_VOLUME_SQR\"] = np.log(dfmerged[\"TNC_VOLUME_SQR\"]+1)\n",
    "\n",
    "# PCT_NON_TNC_VOL: log, square of and log of the square of\n",
    "dfmerged[\"PCT_NON_TNC_VOL\"] = dfmerged[\"NON_TNC_VOLUME\"].divide(dfmerged[\"Tot_Vol\"])\n",
    "\n",
    "# Veh_Dist_Travel: log, square of and log of the square of\n",
    "dfmerged[\"log_Veh_Dist_Travel\"] = np.log(dfmerged[\"Veh_Dist_Travel\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_SQR\"] = dfmerged[\"Veh_Dist_Travel\"]*dfmerged[\"Veh_Dist_Travel\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_SQR\"] = np.log(dfmerged[\"Veh_Dist_Travel_SQR\"]+1)\n",
    "# also convert VDT in millions\n",
    "dfmerged[\"Veh_Dist_Travel_millions\"] = dfmerged[\"Veh_Dist_Travel\"].divide(1000000)\n",
    "dfmerged[\"log_Veh_Dist_Travel_millions\"] = np.log(dfmerged[\"Veh_Dist_Travel_millions\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_millions_SQR\"] = dfmerged[\"Veh_Dist_Travel_millions\"]*dfmerged[\"Veh_Dist_Travel_millions\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_millions_SQR\"] = np.log(dfmerged[\"Veh_Dist_Travel_millions_SQR\"]+1)\n",
    "\n",
    "# Veh_Dist_Travel_TNC: log, square of and log of the square of\n",
    "dfmerged[\"log_Veh_Dist_Travel_TNC\"] = np.log(dfmerged[\"Veh_Dist_Travel_TNC\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_TNC_SQR\"] = dfmerged[\"Veh_Dist_Travel_TNC\"]*dfmerged[\"Veh_Dist_Travel_TNC\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_TNC_SQR\"] = np.log(dfmerged[\"Veh_Dist_Travel_TNC_SQR\"]+1)\n",
    "# also convert VDT in millions\n",
    "dfmerged[\"Veh_Dist_Travel_TNC_millions\"] = dfmerged[\"Veh_Dist_Travel_TNC\"].divide(1000000)\n",
    "dfmerged[\"log_Veh_Dist_Travel_TNC_millions\"] = np.log(dfmerged[\"Veh_Dist_Travel_TNC_millions\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_TNC_millions_SQR\"] = dfmerged[\"Veh_Dist_Travel_TNC_millions\"]*dfmerged[\"Veh_Dist_Travel_TNC_millions\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_TNC_millions_SQR\"] = np.log(dfmerged[\"Veh_Dist_Travel_TNC_millions_SQR\"]+1)\n",
    "\n",
    "# Veh_Dist_Travel_NON_TNC: log, square of and log of the square of\n",
    "dfmerged[\"log_Veh_Dist_Travel_NON_TNC\"] = np.log(dfmerged[\"Veh_Dist_Travel_NON_TNC\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_NON_TNC_SQR\"] = dfmerged[\"Veh_Dist_Travel_NON_TNC\"]*dfmerged[\"Veh_Dist_Travel_NON_TNC\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_NON_TNC_SQR\"] = np.log(dfmerged[\"Veh_Dist_Travel_NON_TNC_SQR\"]+1)\n",
    "# also convert VDT in millions\n",
    "dfmerged[\"Veh_Dist_Travel_NON_TNC_millions\"] = dfmerged[\"Veh_Dist_Travel_NON_TNC\"].divide(1000000)\n",
    "dfmerged[\"log_Veh_Dist_Travel_NON_TNC_millions\"] = np.log(dfmerged[\"Veh_Dist_Travel_NON_TNC_millions\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_NON_TNC_millions_SQR\"] = dfmerged[\"Veh_Dist_Travel_NON_TNC_millions\"]*dfmerged[\"Veh_Dist_Travel_NON_TNC_millions\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_NON_TNC_millions_SQR\"] = np.log(dfmerged[\"Veh_Dist_Travel_NON_TNC_millions_SQR\"]+1)\n",
    "\n",
    "# CONGESTED_SPEED\n",
    "dfmerged[\"log_CONGESTED_SPEED\"] = np.log(dfmerged[\"CONGESTED_SPEED\"]+1)\n",
    "dfmerged[\"CONGESTED_SPEED_SQR\"] = dfmerged[\"CONGESTED_SPEED\"]*dfmerged[\"CONGESTED_SPEED\"]\n",
    "dfmerged[\"log_CONGESTED_SPEED_SQR\"] = np.log(dfmerged[\"CONGESTED_SPEED_SQR\"]+1)\n",
    "\n",
    "# CONGESTED_SPEED\n",
    "dfmerged[\"log_VC_ratio\"] = np.log(dfmerged[\"VC_ratio\"]+1)\n",
    "dfmerged[\"VC_ratio_SQR\"] = dfmerged[\"VC_ratio\"]*dfmerged[\"VC_ratio\"]\n",
    "dfmerged[\"log_VC_ratio_SQR\"] = np.log(dfmerged[\"VC_ratio_SQR\"]+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Tot_Vol: log, square of and log of the square of\n",
    "dfmerged[\"log_Tot_Vol_yr\"] = np.log(dfmerged[\"Tot_Vol_yr\"]+1)\n",
    "dfmerged[\"Tot_Vol_SQR_yr\"] = dfmerged[\"Tot_Vol_yr\"]*dfmerged[\"Tot_Vol_yr\"]\n",
    "dfmerged[\"log_Tot_Vol_SQR_yr\"] = np.log(dfmerged[\"Tot_Vol_SQR_yr\"]+1)\n",
    "\n",
    "# TNC_Vol: log, square of and log of the square of\n",
    "dfmerged[\"log_TNC_VOLUME_yr\"] = np.log(dfmerged[\"TNC_VOLUME_yr\"]+1)\n",
    "dfmerged[\"TNC_VOLUME_SQR_yr\"] = dfmerged[\"TNC_VOLUME_yr\"]*dfmerged[\"TNC_VOLUME_yr\"]\n",
    "dfmerged[\"log_TNC_VOLUME_SQR_yr\"] = np.log(dfmerged[\"TNC_VOLUME_SQR_yr\"]+1)\n",
    "\n",
    "# Non_TNC_Vol: log, square of and log of the square of\n",
    "dfmerged[\"log_NON_TNC_VOLUME_yr\"] = np.log(dfmerged[\"NON_TNC_VOLUME_yr\"]+1)\n",
    "dfmerged[\"NON_TNC_VOLUME_SQR_yr\"] = dfmerged[\"NON_TNC_VOLUME_yr\"]*dfmerged[\"NON_TNC_VOLUME_yr\"]\n",
    "dfmerged[\"log_NON_TNC_VOLUME_SQR_yr\"] = np.log(dfmerged[\"TNC_VOLUME_SQR_yr\"]+1)\n",
    "\n",
    "# PCT_NON_TNC_VOL: log, square of and log of the square of\n",
    "dfmerged[\"PCT_NON_TNC_VOL_yr\"] = dfmerged[\"NON_TNC_VOLUME_yr\"].divide(dfmerged[\"Tot_Vol_yr\"])\n",
    "\n",
    "# Veh_Dist_Travel: log, square of and log of the square of\n",
    "dfmerged[\"Veh_Dist_Travel_yr\"] = (dfmerged[\"Veh_Dist_Travel\"]*365)\n",
    "dfmerged[\"log_Veh_Dist_Travel_yr\"] = np.log((dfmerged[\"Veh_Dist_Travel_yr\"])+1)\n",
    "dfmerged[\"Veh_Dist_Travel_SQR_yr\"] = dfmerged[\"Veh_Dist_Travel_yr\"]*dfmerged[\"Veh_Dist_Travel_yr\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_SQR_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_SQR_yr\"]+1)\n",
    "# also convert VDT in millions\n",
    "dfmerged[\"Veh_Dist_Travel_millions_yr\"] = dfmerged[\"Veh_Dist_Travel\"].divide(1000000)*365\n",
    "dfmerged[\"log_Veh_Dist_Travel_millions_yr\"] = np.log((dfmerged[\"Veh_Dist_Travel_millions_yr\"])+1)\n",
    "dfmerged[\"Veh_Dist_Travel_millions_SQR_yr\"] = dfmerged[\"Veh_Dist_Travel_millions_yr\"]*dfmerged[\"Veh_Dist_Travel_millions_yr\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_millions_SQR_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_millions_SQR_yr\"]+1)\n",
    "\n",
    "# Veh_Dist_Travel_TNC: log, square of and log of the square of\n",
    "dfmerged[\"log_Veh_Dist_Travel_TNC_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_TNC_yr\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_TNC_SQR_yr\"] = dfmerged[\"Veh_Dist_Travel_TNC_yr\"]*dfmerged[\"Veh_Dist_Travel_TNC_yr\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_TNC_SQR_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_TNC_SQR_yr\"]+1)\n",
    "# also convert VDT in millions\n",
    "dfmerged[\"Veh_Dist_Travel_TNC_millions_yr\"] = dfmerged[\"Veh_Dist_Travel_TNC_yr\"].divide(1000000)*365\n",
    "dfmerged[\"log_Veh_Dist_Travel_TNC_millions_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_TNC_millions_yr\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_TNC_millions_SQR_yr\"] = dfmerged[\"Veh_Dist_Travel_TNC_millions_yr\"]*dfmerged[\"Veh_Dist_Travel_TNC_millions_yr\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_TNC_millions_SQR_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_TNC_millions_SQR_yr\"]+1)\n",
    "\n",
    "# Veh_Dist_Travel_NON_TNC: log, square of and log of the square of\n",
    "dfmerged[\"log_Veh_Dist_Travel_NON_TNC_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_NON_TNC_yr\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_NON_TNC_SQR_yr\"] = dfmerged[\"Veh_Dist_Travel_NON_TNC_yr\"]*dfmerged[\"Veh_Dist_Travel_NON_TNC_yr\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_NON_TNC_SQR_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_NON_TNC_SQR_yr\"]+1)\n",
    "# also convert VDT in millions\n",
    "dfmerged[\"Veh_Dist_Travel_NON_TNC_millions_yr\"] = dfmerged[\"Veh_Dist_Travel_NON_TNC_yr\"].divide(1000000)\n",
    "dfmerged[\"log_Veh_Dist_Travel_NON_TNC_millions_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_NON_TNC_millions_yr\"]+1)\n",
    "dfmerged[\"Veh_Dist_Travel_NON_TNC_millions_SQR_yr\"] = dfmerged[\"Veh_Dist_Travel_NON_TNC_millions_yr\"]*dfmerged[\"Veh_Dist_Travel_NON_TNC_millions_yr\"]\n",
    "dfmerged[\"log_Veh_Dist_Travel_NON_TNC_millions_SQR_yr\"] = np.log(dfmerged[\"Veh_Dist_Travel_NON_TNC_millions_SQR_yr\"]+1)\n",
    "\n",
    "# CONGESTED_SPEED\n",
    "dfmerged[\"log_CONGESTED_SPEED_yr\"] = np.log(dfmerged[\"CONGESTED_SPEED\"]+1)\n",
    "dfmerged[\"CONGESTED_SPEED_SQR_yr\"] = dfmerged[\"CONGESTED_SPEED\"]*dfmerged[\"CONGESTED_SPEED\"]\n",
    "dfmerged[\"log_CONGESTED_SPEED_SQR_yr\"] = np.log(dfmerged[\"CONGESTED_SPEED_SQR\"]+1)\n",
    "\n",
    "# CONGESTED_SPEED\n",
    "dfmerged[\"log_VC_ratio_yr\"] = np.log(dfmerged[\"VC_ratio\"]+1)\n",
    "dfmerged[\"VC_ratio_SQR_yr\"] = dfmerged[\"VC_ratio\"]*dfmerged[\"VC_ratio\"]\n",
    "dfmerged[\"log_VC_ratio_SQR_yr\"] = np.log(dfmerged[\"VC_ratio_SQR_yr\"]+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "dfmerged.to_csv((BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"sample\",\"SF_merge_CT_fullnetwork_5Feb2022.csv\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the difference\n",
    "\n",
    "dfmerged.sort_values([\"tractce10\",\"ACCIDENT_YEAR\"], inplace=True)\n",
    "cols = ['NUMBER_KILLED', 'NUMBER_INJURED', 'COUNT_SEVERE_INJ', 'COUNT_VISIBLE_INJ', 'COUNT_COMPLAINT_PAIN', 'COUNT_PED_KILLED', 'COUNT_PED_INJURED', 'COUNT_BICYCLIST_KILLED', 'COUNT_BICYCLIST_INJURED', 'COUNT_MC_KILLED', 'COUNT_MC_INJURED', 'Total_Crash', 'COUNT_Fatal', 'COUNT_Severe_Injury', 'COUNT_Visible_Injury', 'COUNT_Other_Injury', 'COUNT_PDO','Tot_CAP', 'OOS', 'PUDO', 'Tot_Vol', 'TNC_Tot_Vol','Veh_Dist_Travel', 'CONGESTED_SPEED', 'TNC_VOLUME', 'Veh_Dist_Travel_TNC', 'NON_TNC_VOLUME', 'Veh_Dist_Travel_NON_TNC', 'PCT_TNC_VOL', 'PCT_TNC_PUDO', 'VC_ratio', 'log_Tot_Vol', 'Tot_Vol_SQR', 'log_Tot_Vol_SQR', 'log_TNC_VOLUME', 'TNC_VOLUME_SQR', 'log_TNC_VOLUME_SQR', 'log_NON_TNC_VOLUME', 'NON_TNC_VOLUME_SQR', 'log_NON_TNC_VOLUME_SQR', 'PCT_NON_TNC_VOL', 'log_Veh_Dist_Travel', 'Veh_Dist_Travel_SQR', 'log_Veh_Dist_Travel_SQR', 'log_Veh_Dist_Travel_TNC', 'Veh_Dist_Travel_TNC_SQR', 'log_Veh_Dist_Travel_TNC_SQR', 'log_Veh_Dist_Travel_NON_TNC', 'Veh_Dist_Travel_NON_TNC_SQR', 'log_Veh_Dist_Travel_NON_TNC_SQR', 'log_CONGESTED_SPEED', 'CONGESTED_SPEED_SQR', 'log_CONGESTED_SPEED_SQR', 'log_VC_ratio', 'VC_ratio_SQR', 'log_VC_ratio_SQR']\n",
    "dfmerged.groupby([\"tractce10\"],as_index=True)[cols].diff(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}