{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1092df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyproj import CRS\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import codecs\n",
    "import osm2geojson\n",
    "from shapely import wkt\n",
    "\n",
    "import gzip\n",
    "from xml.etree.ElementTree import Element, SubElement, Comment, tostring\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# set the working directory\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "# auto reloading\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c918ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goyal\\.virtualenvs\\overpass_turbo-zwxzihl_\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3169: DtypeWarning: Columns (14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# read the SWITRS raw records. \n",
    "dfSWITRS = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"CollisionRecords.txt\"))\n",
    "dfSWITRS.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Raw_SWITRS.csv\"))\n",
    "# The file downloaded from the SWITRS website has records from 2010 to 2016.So keep only those year records\n",
    "dfSWITRS=dfSWITRS[dfSWITRS[\"ACCIDENT_YEAR\"].isin([2010,2016])]\n",
    "\"\"\"\n",
    "In total 13,926 (unique CASE_IDs) records are present together in the year 2010 & 2016.\n",
    "#########################\n",
    "Overall statistics\n",
    "YR 2010 = 6517 records and \n",
    "YR 2016 = 7409 (13.70% increase from YR 2010)\n",
    "#########################\n",
    "Collision Severity Codes: \n",
    "01 - Fatal\n",
    "02 - Injury (Severe)\n",
    "03 - Injury (Other Visible)\n",
    "04 - Injury (Complaint of Pain)\n",
    "00 - Property Damage Only (PDO)\n",
    "###################################\n",
    "Distribution of collission severity: \n",
    "############# YR 2010 #############\n",
    "00 = 2839\n",
    "01 = 29\n",
    "02 = 195\n",
    "03 = 1137\n",
    "04 = 2317\n",
    "###################################\n",
    "############# YR 2016 #############\n",
    "00 = 3351 (+19%)\n",
    "01 = 35 (+20%)\n",
    "02 = 273 (+40%)\n",
    "03 = 1079 (+5%)\n",
    "04 = 2671 (-13%)\n",
    "###################################\n",
    "Location information already present for\n",
    "YR 2010 = 373 (5%)\n",
    "YR 2016 = 1886 (25%)\n",
    "\n",
    "\"\"\"\n",
    "dfSWITRS2010=dfSWITRS[dfSWITRS[\"ACCIDENT_YEAR\"].isin([2010])]\n",
    "dfSWITRS2016=dfSWITRS[dfSWITRS[\"ACCIDENT_YEAR\"].isin([2016])]\n",
    "\n",
    "dfSWITRS2010.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Raw_SWITRS_2010.csv\"))\n",
    "dfSWITRS2016.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Raw_SWITR_2016.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789e187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goyal\\.virtualenvs\\overpass_turbo-zwxzihl_\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3169: DtypeWarning: Columns (7,14,17,26,29,30,33,42) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOnce the merge is committed between SWITRS records and TIMS records, following are the statistics \\n#############################################################\\nTotal Crashes (captured)\\nYR 2010 = 6517\\nYR 2016 = 7409\\n#############################################################\\nTotal number of latlong locations available\\nYR 2010 = 3860 (2657)\\nYR 2016 = 5057 (2352)\\n######## COLLISION SEVERITY based on LATLONG ################ \\nFORMULA: TOTAL = with LATLONG + without LATLONG\\nYR 2010\\n00 - 2839 = 212 + 2627\\n01 - 29 = 27 + 2 \\n02 - 195 = 193 + 02\\n03 - 1137 = 1129 + 08\\n04 - 2317 = 2301 + 18\\n\\nYR 2016\\n00 - 3351 = 1137 + 2214\\n01 - 35 = 34 + 1\\n02 - 273 = 269 + 4 \\n03 - 1079 = 1054 + 25\\n04 - 2671 = 2563 + 108\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TIMS crash records summary\n",
    "\"\"\"\n",
    "There are two level of information available via TIMS website. \n",
    "One is for SF-City and other is SF-StateRoutes. \n",
    "They do not contain PDO crash information as it is available in the SWITRS file\n",
    "\"\"\"\n",
    "dfTIMSCity = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"TIMS_Crashes_city.csv\"))\n",
    "dfTIMSStateRoutes = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"TIMS_Crahes_StateRoutes.csv\"))\n",
    "# TIMS database\n",
    "dfTIMSCity = dfTIMSCity[dfTIMSCity[\"ACCIDENT_YEAR\"].isin([2010,2016])]\n",
    "dfTIMSStateRoutes = dfTIMSStateRoutes[dfTIMSStateRoutes[\"ACCIDENT_YEAR\"].isin([2010,2016])]\n",
    "# YR 2010\n",
    "dfTIMSCity2010 = dfTIMSCity[dfTIMSCity[\"ACCIDENT_YEAR\"].isin([2010])]\n",
    "dfTIMSStateRoutes2010 = dfTIMSStateRoutes[dfTIMSStateRoutes[\"ACCIDENT_YEAR\"].isin([2010])]\n",
    "# YR 2016\n",
    "dfTIMSCity2016 = dfTIMSCity[dfTIMSCity[\"ACCIDENT_YEAR\"].isin([2016])]\n",
    "dfTIMSStateRoutes2016 = dfTIMSStateRoutes[dfTIMSStateRoutes[\"ACCIDENT_YEAR\"].isin([2016])]\n",
    "\n",
    "\"\"\"\n",
    "########### RAW Stats ##################\n",
    "SF-City \n",
    "YR 2010 = 3678\n",
    "YR 2016 = 4058\n",
    "SF-StateRoutes \n",
    "YR 2010 = 767\n",
    "YR 2016 = 688\n",
    "#######################################\n",
    "Crash records in SF-StateRoutes for both YR 2010 & 2016 are already mentioned in the SF-City record, so use of the SF-StateRoutes information could be avoided.\n",
    "Distribution of collission severity: \n",
    "############# YR 2010 #############\n",
    "01 = 29\n",
    "02 = 195\n",
    "03 = 1137\n",
    "04 = 2317\n",
    "###################################\n",
    "############# YR 2016 #############\n",
    "01 = 35 (+20%)\n",
    "02 = 273 (+40%)\n",
    "03 = 1079 (+5%)\n",
    "04 = 2671 (-13%)\n",
    "###################################\n",
    "Above information just indicates that except PDO crashes, the crash information 100% matches with SWITRS records.\n",
    "###################################\n",
    "Location information present for\n",
    "YR 2010 = 3647 (99%)\n",
    "YR 2016 = 3896 (96%)\n",
    "###################################\n",
    "Location information not present for\n",
    "##### YR 2010 = 31\n",
    "01 = 2\n",
    "02 = 2\n",
    "03 = 8\n",
    "04 = 19\n",
    "##### YR 2016 = 138\n",
    "01 = 1\n",
    "02 = 4\n",
    "03 = 25\n",
    "04 = 108\n",
    "###################################\n",
    "\n",
    "\"\"\"\n",
    "dfTIMSCity2010.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Raw_TIMS_2010.csv\"))\n",
    "dfTIMSCity2016.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Raw_TIMS_2016.csv\"))\n",
    "\n",
    "# df2010 = dfSWITRS2010.merge(dfTIMSCity2010.drop_duplicates(), on=['CASE_ID','CASE_ID'], how='outer', indicator=True)\n",
    "# df2016 = dfTIMSCity2016.merge(dfSWITRS2016.drop_duplicates(), on=['CASE_ID','CASE_ID'], how='left', indicator=True)\n",
    "\n",
    "frames = [dfTIMSCity,dfSWITRS]\n",
    "result_2 = pd.concat(frames)\n",
    "\n",
    "result_2.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Raw_Switrs_TIMS_merged_all.csv\"))\n",
    "\n",
    "result_2['POINT_X']= np.where(result_2['POINT_X'].isnull(), result_2['LONGITUDE'], result_2['POINT_X'])\n",
    "result_2['POINT_Y']= np.where(result_2['POINT_Y'].isnull(), result_2['LATITUDE'], result_2['POINT_Y'])\n",
    "\n",
    "result_2['POINT_X']= np.where(result_2['POINT_X']>0, -abs(result_2['POINT_X']), result_2['POINT_X'])\n",
    "result_2.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Raw_Switrs_TIMS_merged_all_v2.csv\"))\n",
    "result_3 = result_2.drop_duplicates(subset=['CASE_ID'])\n",
    "result_3.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Raw_Switrs_TIMS_merged_all_v3.csv\"))\n",
    "\n",
    "# YR 2010\n",
    "dfresult2010 = result_3[result_3[\"ACCIDENT_YEAR\"].isin([2010])]\n",
    "# YR 2016\n",
    "dfresult2016 = result_3[result_3[\"ACCIDENT_YEAR\"].isin([2016])]\n",
    "\n",
    "dfresult2010.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Switrs_TIMS_merged_2010.csv\"))\n",
    "dfresult2016.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Switrs_TIMS_merged_2016.csv\"))\n",
    "\n",
    "\"\"\"\n",
    "Once the merge is committed between SWITRS records and TIMS records, following are the statistics \n",
    "#############################################################\n",
    "Total Crashes (captured)\n",
    "YR 2010 = 6517\n",
    "YR 2016 = 7409\n",
    "#############################################################\n",
    "Total number of latlong locations available\n",
    "YR 2010 = 3860 (2657)\n",
    "YR 2016 = 5057 (2352)\n",
    "######## COLLISION SEVERITY based on LATLONG ################ \n",
    "FORMULA: TOTAL = with LATLONG + without LATLONG\n",
    "YR 2010\n",
    "00 - 2839 = 212 + 2627\n",
    "01 - 29 = 27 + 2 \n",
    "02 - 195 = 193 + 02\n",
    "03 - 1137 = 1129 + 08\n",
    "04 - 2317 = 2301 + 18\n",
    "\n",
    "YR 2016\n",
    "00 - 3351 = 1137 + 2214\n",
    "01 - 35 = 34 + 1\n",
    "02 - 273 = 269 + 4 \n",
    "03 - 1079 = 1054 + 25\n",
    "04 - 2671 = 2563 + 108\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ee8c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goyal\\.virtualenvs\\overpass_turbo-zwxzihl_\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3169: DtypeWarning: Columns (7,14,17,26,29,30,33,42) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "dfTIMSCity = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"TIMS_Crashes_city.csv\"))\n",
    "dfTIMSStateRoutes = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"TIMS_Crahes_StateRoutes.csv\"))\n",
    "# TIMS database\n",
    "dfTIMSCity = dfTIMSCity[dfTIMSCity[\"ACCIDENT_YEAR\"].isin([2010,2016])]\n",
    "dfTIMSStateRoutes = dfTIMSStateRoutes[dfTIMSStateRoutes[\"ACCIDENT_YEAR\"].isin([2010,2016])]\n",
    "# YR 2010\n",
    "dfTIMSCity2010 = dfTIMSCity[dfTIMSCity[\"ACCIDENT_YEAR\"].isin([2010])]\n",
    "dfTIMSStateRoutes2010 = dfTIMSStateRoutes[dfTIMSStateRoutes[\"ACCIDENT_YEAR\"].isin([2010])]\n",
    "# YR 2016\n",
    "dfTIMSCity2016 = dfTIMSCity[dfTIMSCity[\"ACCIDENT_YEAR\"].isin([2016])]\n",
    "dfTIMSStateRoutes2016 = dfTIMSStateRoutes[dfTIMSStateRoutes[\"ACCIDENT_YEAR\"].isin([2016])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9bc3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5057\n",
       "True     2352\n",
       "Name: POINT_X, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresult2016['POINT_X'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d71173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### REVERSE GEOCODE ########\n",
    "BASE_DIR = Path.cwd()\n",
    "dfIntersect_Rds = pd.read_csv(BASE_DIR.parent.joinpath(\"Exported_files\",\"Intersecting_Rds.csv\"))\n",
    "dfIntersect_Rds['geometry'] = dfIntersect_Rds['geometry'].apply(wkt.loads)\n",
    "dfIntersect_Rds = gpd.GeoDataFrame(dfIntersect_Rds, crs='epsg:4326')\n",
    "col = [\"Unnamed: 0\", \"index\",\"layer\",\"path\"]\n",
    "dfIntersect_Rds = dfIntersect_Rds.drop(col,axis=1).copy()\n",
    "\n",
    "def cleanStreetName(streetName):\n",
    "    newStreetName = streetName.strip()\n",
    "    corrections = {\"TWELFTH\":\"12TH\", \n",
    "                   \"ELEVENTH\":\"11TH\",\n",
    "                   \"TENTH\":\"10TH\",\n",
    "                   \"NINTH\":\"9TH\",\n",
    "                   \"EIGHTH\":\"8TH\",\n",
    "                   \"SEVENTH\":\"7TH\",\n",
    "                   \"SIXTH\":\"6TH\",\n",
    "                   \"FIFTH\":\"5TH\",\n",
    "                   \"FOURTH\":\"4TH\",\n",
    "                   \"THIRD\":\"3RD\",\n",
    "                   \"SECOND\":\"2ND\",\n",
    "                   \"FIRST\":\"1ST\",\n",
    "                   \"O'FARRELL\":\"O FARRELL\",\n",
    "                   \"3RDREET\":\"3RD\",\n",
    "                   \"EMBARCADERO/KING\":\"THE EMBARCADERO\",\n",
    "                   \"VAN NESSNUE\":\"VAN NESS\",\n",
    "                   \"3RD #3\":\"3RD\",\n",
    "                   \"BAYSHORE #3\":\"BAYSHORE\",\n",
    "                   \"09TH\":\"9TH\",\n",
    "                   \"08TH\":\"8TH\",\n",
    "                   \"07TH\":\"7TH\",\n",
    "                   \"06TH\":\"6TH\",\n",
    "                   \"05TH\":\"5TH\",\n",
    "                   \"04TH\":\"4TH\",\n",
    "                   \"03RD\":\"3RD\",\n",
    "                   \"02ND\":\"2ND\",\n",
    "                   \"01ST\":\"1ST\",\n",
    "                  }\n",
    "\n",
    "    itemsToRemove = [\" STREETS\",\n",
    "                     \" STS.\",\n",
    "                     \" STS\",\n",
    "                     \" ST.\",\n",
    "                     \" ST\",\n",
    "                     \" Street\",\n",
    "                     \" RD.\",\n",
    "                     \" RD\",\n",
    "                     \" Road\",\n",
    "                     \" AVE.\",\n",
    "                     \" AVES\",\n",
    "                     \" AVE\",\n",
    "                     \" AV\",\n",
    "                     \" Avenue\",\n",
    "                     \" BLVD.\",\n",
    "                     \" BLVD\",\n",
    "                     \" BL\",                     \n",
    "                     \" Boulevard\",\n",
    "                     \" MASTER:\",\n",
    "                     \" DR.\",\n",
    "                     \" Drive\",\n",
    "                     \" WY\",\n",
    "                     \" Way\",\n",
    "                     \" CT\",\n",
    "                     \" TERR\",\n",
    "                     \" Terrace\",\n",
    "                     \" HWY\",\n",
    "                     \" EXPY\"]\n",
    "          \n",
    "    for wrongName, rightName in corrections.items():\n",
    "        if wrongName in streetName:\n",
    "            newStreetName = streetName.replace(wrongName, rightName)\n",
    "        if streetName == 'EMBARCADERO':\n",
    "            newStreetName = \"THE EMBARCADERO\"\n",
    "        if streetName.endswith(\" DR\"):\n",
    "            newStreetName = streetName[:-3]\n",
    "        if streetName.endswith(\" AV\"):\n",
    "            newStreetName = streetName[:-3]\n",
    "        if \" TO \" in streetName:\n",
    "            cutOff = streetName.find(\" TO \")\n",
    "            newStreetName = streetName[:cutOff]            \n",
    "    \n",
    "    for item in itemsToRemove:\n",
    "        if item in newStreetName:\n",
    "            newStreetName = newStreetName.replace(item, \"\")\n",
    "    \n",
    "    return newStreetName.strip()\n",
    "\n",
    "dfIntersect_Rds=dfIntersect_Rds.fillna(\"\").copy()\n",
    "\n",
    "dfIntersect_Rds[\"Link_ID_0_2\"] = dfIntersect_Rds.apply(lambda x: cleanStreetName(x[\"Link_ID_0\"]), axis=1)\n",
    "dfIntersect_Rds[\"Link_ID_1_2\"] = dfIntersect_Rds.apply(lambda x: cleanStreetName(x[\"Link_ID_1\"]), axis=1)\n",
    "dfIntersect_Rds[\"Link_ID_2_2\"] = dfIntersect_Rds.apply(lambda x: cleanStreetName(x[\"Link_ID_2\"]), axis=1)\n",
    "dfIntersect_Rds[\"Link_ID_3_2\"] = dfIntersect_Rds.apply(lambda x: cleanStreetName(x[\"Link_ID_3\"]), axis=1)\n",
    "dfIntersect_Rds[\"Link_ID_4_2\"] = dfIntersect_Rds.apply(lambda x: cleanStreetName(x[\"Link_ID_4\"]), axis=1)\n",
    "dfIntersect_Rds[\"Link_ID_5_2\"] = dfIntersect_Rds.apply(lambda x: cleanStreetName(x[\"Link_ID_5\"]), axis=1)\n",
    "dfIntersect_Rds[\"Link_ID_6_2\"] = dfIntersect_Rds.apply(lambda x: cleanStreetName(x[\"Link_ID_6\"]), axis=1)\n",
    "dfIntersect_Rds[\"Link_ID_7_2\"] = dfIntersect_Rds.apply(lambda x: cleanStreetName(x[\"Link_ID_7\"]), axis=1)\n",
    "\n",
    "dfIntersect_Rds['Link_Names'] = dfIntersect_Rds[['Link_ID_0_2', 'Link_ID_1_2','Link_ID_2_2', 'Link_ID_3_2','Link_ID_4_2', 'Link_ID_5_2','Link_ID_6_2', 'Link_ID_7_2']].apply(lambda x: ','.join(set(x.unique())), axis=1)\n",
    "\n",
    "# remove first character from the values if it starts with \",\"\n",
    "dfIntersect_Rds['Link_Names'] = dfIntersect_Rds['Link_Names'].apply(lambda x : x[1:] if x.startswith(\",\") else x)\n",
    "\n",
    "# lowercase\n",
    "dfIntersect_Rds = dfIntersect_Rds.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "dfIntersect_Rds[\"LatLong\"] = dfIntersect_Rds[\"lat\"].astype(str) + \" \" + dfIntersect_Rds[\"lon\"].astype(str)\n",
    "dfIntersect_Rds.to_csv(BASE_DIR.parent.joinpath(\"Exported_Files\",\"Intersecting_Rds_2.csv\"))\n",
    "\n",
    "# get unique street intersections and their latitude and longtitude\n",
    "dfunique = dfIntersect_Rds[['Link_Names', 'LatLong']]\n",
    "# get unique street intersections and their latitude and longtitude\n",
    "dfunique.drop_duplicates(['Link_Names'])\n",
    "mydict = dict(zip(dfunique.Link_Names, dfunique.LatLong.astype(str)))\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def getDerivedLatLngPosition(lat,long, dist, bearing):\n",
    "    latitude = float(lat)\n",
    "    longitude = float(long)\n",
    "    \n",
    "    DegreesToRadians = math.pi/180.0\n",
    "    RadiansToDegrees = 180.0/math.pi \n",
    "    EarthRadius = 6378137.0 # Radius of Earth in meters\n",
    "\n",
    "    latA =  latitude * DegreesToRadians\n",
    "    longA = longitude * DegreesToRadians\n",
    "    angularDistance = dist/EarthRadius\n",
    "    trueCourse = bearing * DegreesToRadians\n",
    "    \n",
    "    lat =  math.asin(math.sin(latA) * math.cos(angularDistance) + math.cos(latA) * math.sin(angularDistance) * math.cos(trueCourse))\n",
    "    \n",
    "    dlon = math.atan2(math.sin(trueCourse) * math.sin(angularDistance) * math.cos(latA), math.cos(angularDistance) - math.sin(latA) * math.sin(lat))\n",
    "    lon = ((longA + dlon + math.pi) % (math.pi*2)) - math.pi\n",
    "    \n",
    "    return Point(round(lon * RadiansToDegrees,5),round(lat * RadiansToDegrees,5))\n",
    "\n",
    "# Switcher is dictionary data type here\n",
    "def get_direction(argument):\n",
    "    switcher = {\n",
    "        \"N\": 0,\n",
    "        \"W\": -90,\n",
    "        \"E\": +90,\n",
    "        \"S\": -180\n",
    "    }  \n",
    "    # get() method of dictionary data type returns value of passed argument if it is present in dictionary;\n",
    "    # otherwise second argument will be assigned as default value of passed argument\n",
    "    return switcher.get(argument, 0)\n",
    "\n",
    "def latlongsearch(row):\n",
    "    primary_rd = row[\"PRIMARY_RD_2\"]\n",
    "    secondary_rd = row[\"SECONDARY_RD_2\"] \n",
    "    _dict = mydict.copy()\n",
    "    for key, value in _dict.items():\n",
    "        if (primary_rd in key) and (secondary_rd in key):\n",
    "            lat = value.split(\" \")[0]\n",
    "            long = value.split(\" \")[1]\n",
    "            dist = 0 if (row[\"DISTANCE\"]<= 0) else (row[\"DISTANCE\"]/3.2808) # convert distance into meters\n",
    "#             print(row[\"DIRECTION\"])\n",
    "            bearing = get_direction(row[\"DIRECTION\"].upper())\n",
    "            return getDerivedLatLngPosition(lat,long, dist, bearing)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069ca0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-330666bc9c2e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfresult2010[\"PRIMARY_RD_2\"] = dfresult2010.apply(lambda x: cleanStreetName(x[\"PRIMARY_RD\"]), axis=1)\n",
      "<ipython-input-7-330666bc9c2e>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfresult2010[\"SECONDARY_RD_2\"] = dfresult2010.apply(lambda x: cleanStreetName(x[\"SECONDARY_RD\"]), axis=1)\n",
      "<ipython-input-7-330666bc9c2e>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfresult2010['Rd_Names'] = dfresult2010[['PRIMARY_RD_2', 'SECONDARY_RD_2']].apply(lambda x: ','.join(set(x.unique())), axis=1)\n",
      "<ipython-input-7-330666bc9c2e>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfresult2010['Rd_Names'] = dfresult2010['Rd_Names'].apply(lambda x : x[1:] if x.startswith(\",\") else x)\n"
     ]
    }
   ],
   "source": [
    "# YR 2010\n",
    "dfresult2010.columns = dfresult2010.columns.str.rstrip('_x')  # strip suffix at the right end only.\n",
    "dfresult2010[\"PRIMARY_RD_2\"] = dfresult2010.apply(lambda x: cleanStreetName(x[\"PRIMARY_RD\"]), axis=1)\n",
    "dfresult2010[\"SECONDARY_RD_2\"] = dfresult2010.apply(lambda x: cleanStreetName(x[\"SECONDARY_RD\"]), axis=1)\n",
    "# dfSWITRS['Rd_Names'] = dfSWITRS[['PRIMARY_RD_2', 'SECONDARY_RD_2']].apply(lambda x: ' '.join(x), axis=1)\n",
    "dfresult2010['Rd_Names'] = dfresult2010[['PRIMARY_RD_2', 'SECONDARY_RD_2']].apply(lambda x: ','.join(set(x.unique())), axis=1)\n",
    "# remove first character from the values if it starts with \",\"\n",
    "dfresult2010['Rd_Names'] = dfresult2010['Rd_Names'].apply(lambda x : x[1:] if x.startswith(\",\") else x)\n",
    "# lowercase\n",
    "dfresult2010 = dfresult2010.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "dfresult2010[\"DIRECTION\"]=dfresult2010[\"DIRECTION\"].fillna('')\n",
    "dfresult2010[\"ref_latlong\"] = dfresult2010.apply(lambda row: latlongsearch(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b3db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-d44de6d9133e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfresult2016[\"PRIMARY_RD_2\"] = dfresult2016.apply(lambda x: cleanStreetName(x[\"PRIMARY_RD\"]), axis=1)\n",
      "<ipython-input-8-d44de6d9133e>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfresult2016[\"SECONDARY_RD_2\"] = dfresult2016.apply(lambda x: cleanStreetName(x[\"SECONDARY_RD\"]), axis=1)\n",
      "<ipython-input-8-d44de6d9133e>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfresult2016['Rd_Names'] = dfresult2016[['PRIMARY_RD_2', 'SECONDARY_RD_2']].apply(lambda x: ','.join(set(x.unique())), axis=1)\n",
      "<ipython-input-8-d44de6d9133e>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfresult2016['Rd_Names'] = dfresult2016['Rd_Names'].apply(lambda x : x[1:] if x.startswith(\",\") else x)\n"
     ]
    }
   ],
   "source": [
    "# YR 2016\n",
    "dfresult2016.columns = dfresult2016.columns.str.rstrip('_x')  # strip suffix at the right end only.\n",
    "dfresult2016[\"PRIMARY_RD_2\"] = dfresult2016.apply(lambda x: cleanStreetName(x[\"PRIMARY_RD\"]), axis=1)\n",
    "dfresult2016[\"SECONDARY_RD_2\"] = dfresult2016.apply(lambda x: cleanStreetName(x[\"SECONDARY_RD\"]), axis=1)\n",
    "# dfSWITRS['Rd_Names'] = dfSWITRS[['PRIMARY_RD_2', 'SECONDARY_RD_2']].apply(lambda x: ' '.join(x), axis=1)\n",
    "dfresult2016['Rd_Names'] = dfresult2016[['PRIMARY_RD_2', 'SECONDARY_RD_2']].apply(lambda x: ','.join(set(x.unique())), axis=1)\n",
    "# remove first character from the values if it starts with \",\"\n",
    "dfresult2016['Rd_Names'] = dfresult2016['Rd_Names'].apply(lambda x : x[1:] if x.startswith(\",\") else x)\n",
    "# lowercase\n",
    "dfresult2016 = dfresult2016.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "dfresult2016[\"DIRECTION\"]=dfresult2016[\"DIRECTION\"].fillna('')\n",
    "dfresult2016[\"ref_latlong\"] = dfresult2016.apply(lambda row: latlongsearch(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a9cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_latlong(row):\n",
    "    if pd.isnull(row[\"ref_latlong\"]):\n",
    "        if pd.notna(row[\"POINT_X\"]):\n",
    "            return Point(round(row[\"POINT_X\"],5),round(row[\"POINT_Y\"],5))\n",
    "    else:\n",
    "        return row[\"ref_latlong\"]\n",
    "    \n",
    "dfresult2010[\"ref_latlong\"] = dfresult2010.apply(lambda row: fill_null_latlong(row), axis=1)        \n",
    "dfresult2016[\"ref_latlong\"] = dfresult2016.apply(lambda row: fill_null_latlong(row), axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1f4621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5322\n",
       "True     1195\n",
       "Name: ref_latlong, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresult2010[\"ref_latlong\"].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85da7a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2301\n",
       "0    1671\n",
       "3    1129\n",
       "2     194\n",
       "1      27\n",
       "Name: COLLISION_SEVERITY, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresult2010[dfresult2010[\"ref_latlong\"].isnull()==False][\"COLLISION_SEVERITY\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e0835ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################\n",
    "# Distribution of collission severity: \n",
    "# ############# YR 2010 #############\n",
    "# 00 = 2839\n",
    "# 01 = 29\n",
    "# 02 = 195\n",
    "# 03 = 1137\n",
    "# 04 = 2317\n",
    "# ###################################\n",
    "# ############# YR 2016 #############\n",
    "# 00 = 3351 (+19%)\n",
    "# 01 = 35 (+20%)\n",
    "# 02 = 273 (+40%)\n",
    "# 03 = 1079 (+5%)\n",
    "# 04 = 2671 (-13%)\n",
    "# ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3329bdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAfter merging the SWITRS and TIMS Data and performing reverse geocode following are the stats\\n#############################################################\\nTotal Crashes (captured)\\nYR 2010 = 6517\\nYR 2016 = 7409\\n###########################################\\nTotal number of latlong locations available\\nYR 2010 = 5322 (1195)\\nYR 2016 = 6672 (737)\\n######## COLLISION SEVERITY based on LATLONG ################\\nYR 2010\\n00 - 4511 = 1671  (2840)\\n01 - 29 = 27 + 02 \\n02 - 195 = 194 + 01\\n03 - 1137 = 1129 + 08\\n04 - 2317 = 2301 + 16\\n\\nYR 2016\\n00 - 3351 = 2752 (599)\\n01 - 35 = 34 + 1\\n02 - 273 = 269 + 4\\n03 - 1079 = 1054 + 25\\n04 - 2671 = 2563 + 108\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "After merging the SWITRS and TIMS Data and performing reverse geocode following are the stats\n",
    "#############################################################\n",
    "Total Crashes (captured)\n",
    "YR 2010 = 6517\n",
    "YR 2016 = 7409\n",
    "###########################################\n",
    "Total number of latlong locations available\n",
    "YR 2010 = 5322 (1195)\n",
    "YR 2016 = 6672 (737)\n",
    "######## COLLISION SEVERITY based on LATLONG ################\n",
    "YR 2010\n",
    "00 - 4511 = 1671  (2840)\n",
    "01 - 29 = 27 + 02 \n",
    "02 - 195 = 194 + 01\n",
    "03 - 1137 = 1129 + 08\n",
    "04 - 2317 = 2301 + 16\n",
    "\n",
    "YR 2016\n",
    "00 - 3351 = 2752 (599)\n",
    "01 - 35 = 34 + 1\n",
    "02 - 273 = 269 + 4\n",
    "03 - 1079 = 1054 + 25\n",
    "04 - 2671 = 2563 + 108\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86539aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult2010.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_2010_SWITRS_TIMS_matched_output.csv\"))\n",
    "dfresult2016.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_2016_SWITRS_TIMS_matched_output.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9755ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult = pd.concat([dfresult2010, dfresult2016]).to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_all_SWITRS_TIMS_matched_output.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f4f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### trying to refine the geocoding #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ac379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIntersections = pd.read_csv(BASE_DIR.parent.joinpath(\"TransBase_Shapefiles\",\"street_intersections.csv\"))\n",
    "dfStreetNames = pd.read_csv(BASE_DIR.parent.joinpath(\"TransBase_Shapefiles\",\"Street_Names.csv\"))\n",
    "# dfIntersections[\"x_street_comb\"].str.split(pat=\"/\",expand=True).rename(columns = lambda x: f\"StreetName_{x+1}\")\n",
    "# dfIntersections[\"x_street_comb\"].str.split(pat=\"/\",expand=True).rename(columns = lambda x: f\"StreetName_{x+1}\")\n",
    "dfIntersections[[\"StreetName_1\",\"StreetName_2\",\"StreetName_3\",\"StreetName_4\",\"StreetName_5\"]] = dfIntersections[\"x_street_comb\"].str.split(pat=\"/\",expand=True)\n",
    "\n",
    "dfIntersections = dfIntersections[dfIntersections[\"intrsctn_type\"]==\"INTERSECTIONS\"]\n",
    "dfIntersections.to_csv(BASE_DIR.parent.joinpath(\"TransBase_Shapefiles\",\"street_intersections_split.csv\"))\n",
    "\n",
    "\n",
    "dfresult2010 = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_2010_SWITRS_TIMS_matched_output.csv\"))\n",
    "dfresult2016 = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_2016_SWITRS_TIMS_matched_output.csv\"))\n",
    "\n",
    "dfresult = pd.concat([dfresult2010, dfresult2016]).to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_all_SWITRS_TIMS_matched_output.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e345c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique street names in TransBase Shapefile\n",
    "unique_streetNames = pd.unique(dfIntersections[[\"StreetName_1\",\"StreetName_2\",\"StreetName_3\",\"StreetName_4\",\"StreetName_5\"]].values.ravel('K'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedace1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUniqueStreets = pd.DataFrame(unique_streetNames)\n",
    "dfUniqueStreets.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Unique_Street_Names.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc27830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanStreetName(streetName):\n",
    "    newStreetName = streetName.strip()\n",
    "    corrections = {\"TWELFTH\":\"12TH\", \n",
    "                   \"ELEVENTH\":\"11TH\",\n",
    "                   \"TENTH\":\"10TH\",\n",
    "                   \"NINTH\":\"9TH\",\n",
    "                   \"EIGHTH\":\"8TH\",\n",
    "                   \"SEVENTH\":\"7TH\",\n",
    "                   \"SIXTH\":\"6TH\",\n",
    "                   \"FIFTH\":\"5TH\",\n",
    "                   \"FOURTH\":\"4TH\",\n",
    "                   \"THIRD\":\"3RD\",\n",
    "                   \"SECOND\":\"2ND\",\n",
    "                   \"FIRST\":\"1ST\",\n",
    "                   \"O'FARRELL\":\"O FARRELL\",\n",
    "                   \"3RDREET\":\"3RD\",\n",
    "                   \"EMBARCADERO/KING\":\"THE EMBARCADERO\",\n",
    "                   \"VAN NESSNUE\":\"VAN NESS\",\n",
    "                   \"3RD #3\":\"3RD\",\n",
    "                   \"BAYSHORE #3\":\"BAYSHORE\",\n",
    "                   \"09TH\":\"9TH\",\n",
    "                   \"08TH\":\"8TH\",\n",
    "                   \"07TH\":\"7TH\",\n",
    "                   \"06TH\":\"6TH\",\n",
    "                   \"05TH\":\"5TH\",\n",
    "                   \"04TH\":\"4TH\",\n",
    "                   \"03RD\":\"3RD\",\n",
    "                   \"02ND\":\"2ND\",\n",
    "                   \"01ST\":\"1ST\",\n",
    "                  }\n",
    "\n",
    "    itemsToRemove = [\" STREETS\",\n",
    "                     \" STS.\",\n",
    "                     \" STS\",\n",
    "                     \" ST.\",\n",
    "                     \" ST\",\n",
    "                     \" Street\",\n",
    "                     \" STREET\",\n",
    "                     \" RD.\",\n",
    "                     \" RD\",\n",
    "                     \" Road\",\n",
    "                     \" AVE.\",\n",
    "                     \" AVES\",\n",
    "                     \" AVE\",\n",
    "                     \" AV\",\n",
    "                     \" Avenue\",\n",
    "                     \" BLVD.\",\n",
    "                     \" BLVD\",\n",
    "                     \" BL\",                     \n",
    "                     \" Boulevard\",\n",
    "                     \" MASTER:\",\n",
    "                     \" DR.\",\n",
    "                     \" DR\",\n",
    "                     \" Drive\",\n",
    "                     \" WY\",\n",
    "                     \" Way\",\n",
    "                     \" CT\",\n",
    "                     \" TERR\",\n",
    "                     \" Terrace\",\n",
    "                     \" HWY\",\n",
    "                     \" EXPY\",\n",
    "                    ]\n",
    "          \n",
    "    for wrongName, rightName in corrections.items():\n",
    "        if wrongName in streetName:\n",
    "            newStreetName = streetName.replace(wrongName, rightName)\n",
    "        if streetName == 'EMBARCADERO':\n",
    "            newStreetName = \"THE EMBARCADERO\"\n",
    "        if streetName.endswith(\" DR\"):\n",
    "            newStreetName = streetName[:-3]\n",
    "        if streetName.endswith(\" AV\"):\n",
    "            newStreetName = streetName[:-3]\n",
    "        if \" TO \" in streetName:\n",
    "            cutOff = streetName.find(\" TO \")\n",
    "            newStreetName = streetName[:cutOff]            \n",
    "    \n",
    "    for item in itemsToRemove:\n",
    "        if item in newStreetName:\n",
    "            newStreetName = newStreetName.replace(item, \"\")\n",
    "    \n",
    "    return newStreetName.strip()\n",
    "\n",
    "# dfIntersections[\"Link_ID_1\"] = dfIntersections.apply(lambda x: cleanStreetName(x[\"StreetName_1\"]), axis=1)\n",
    "# dfIntersections[\"Link_ID_2\"] = dfIntersections.apply(lambda x: cleanStreetName(x[\"StreetName_2\"]), axis=1)\n",
    "# dfIntersections[\"Link_ID_3\"] = dfIntersections.apply(lambda x: cleanStreetName(x[\"StreetName_3\"]), axis=1)\n",
    "# dfIntersections[\"Link_ID_4\"] = dfIntersections.apply(lambda x: cleanStreetName(x[\"StreetName_4\"]), axis=1)\n",
    "# dfIntersections[\"Link_ID_5\"] = dfIntersections.apply(lambda x: cleanStreetName(x[\"StreetName_5\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b2ed631",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult=pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_all_SWITRS_TIMS_matched_output.csv\"))\n",
    "\n",
    "orig_list = [\" s/b\",\" n/b\",\" e/b\",\" w/b\",\n",
    "             \"us 101\",\"us-101\",\"rt 101\",\n",
    "            \" AV\", \" AVENUE\",\n",
    "            \" STREET\", \n",
    "             \" BL\", \n",
    "             \" WY\",\n",
    "            \"BAYSHORE \", \"BAYSHORE\", \"BAYSHORE AVE\", \"BAYSHORE BLVD\",\n",
    "             \"bernal hts \", \" tunnel\", \n",
    "             \"buena vista west\", \"buena vista west\",\"buena vista east\",\"buena vista av\",\n",
    "             \"chain of lakes dr e\", \n",
    "             \"martin luther king dr\", \"martin luther king jr\",\n",
    "             \"columbia st\",\n",
    "             \"conservatory dr east\",\n",
    "             \" BY PASS\",\n",
    "             \"e/b i-80\", \"eb i-80\",\n",
    "             \"nb us-101\",\n",
    "             \"embarcadero north\", \"embarcadero south\", \"embarcadero st\",\n",
    "             \"i- 80\", \" (sfobb)\", \"(n/b)\", \"u/c\", \" o/c\"\n",
    "             \" (1500 block)\",\" 1600 block\", \" (blk 20)\",\n",
    "             \"rt 1\", \"rt 80\",\"rt 280\", \"sfobb\",\n",
    "             \"s/b i-280\", \" to 18th street\",\"s/b us-101\",\n",
    "             \" 4100\", \" (rt35)\",\"southbound us-101 to san bruno ave.\",\"sr 1 (s/b)\", \"sr-1\",\" to us-101 n/b\",\n",
    "             \" 200 block\", \"onramp\", \" to i-80 Eastbound\", \" (golden gate bridge)\", \"u.s.\",\"united states highway\",\n",
    "             \" (ggb)\",\"(ggb)\", \" (n/b)\", \" (s/b)\", \" (e/b)\", \" (w/b)\",\" from cesar chavez st\",\" from merchant rd.\", \" to 9th st.\",\n",
    "             \" to i-280 s/b\", \" to i-80 e/b\", \" from 10th street\",\"(ggb west sidewalk)\",\" at i-280 split\", \n",
    "             \" from bayshore blvd\", \" from cesar chavez st on-ramp\", \"from cesar chavez st.\", \" to 9th street\", \"  to alemany blvd.\", \" to bayshore blvd.\",\n",
    "             \" to cesar chavez\",\" from cesar chavez st,\",\" from cesar chavez st\", \" to duboce avenue\",\" to i-280\",\n",
    "             \" to mission st.\", \" to mission st\",\" to mission street\", \" to octavia blvd.\",\" to silver avenue\", \" to us-101\", \" to vermont st\", \" to vermont street\",\n",
    "             \" golden gate bridge\", \" overpass\", \" under-crossing\", \" under crossing\", \" undercrossing\", \"faith street pedestrian\",\" ped\",\n",
    "             \" (doyle dr.)\",\" (ggb east side walk)\", \" (ggb west sidewalk)\", \"  from 10th st.\", \"  from 10th st\", \" from oyster point blvd\", \"  from s. van ness ave.\",\" from s. van ness\", \" oc\",\n",
    "             \" to alemany blvd.\",\"  to bayshore boulevard\", \". east\",\"  to paul avenue\", \" transition rd\", \" to westbound\", \" from van ness ave\", \" sb\", \" silver ave. off ramp\",\n",
    "             \"  from 10th street\",\" from merchant road\", \"1101\", \" 131\",\n",
    "            ]\n",
    "repl_list = [\" Southbound\",\" Northbound\",\" Eastbound\", \" Westbound\",\n",
    "             \"HWY 101\",\"HWY 101\",\"HWY 101\",\n",
    "            \" AVE\",\" AVE\",\n",
    "            \" ST\", \" BLVD\", \" WAY\",\n",
    "            \"BAY SHORE BLVD\", \"BAY SHORE BLVD\",\"BAY SHORE BLVD\",\"BAY SHORE BLVD\",\n",
    "            \"BERNAL HEIGHTS \", \"\",\n",
    "             \"BUENA VISTA WEST AVE\", \"BUENA VISTA EAST AVE\",\"BUENA VISTA WEST AVE\",\n",
    "             \"chain of lakes dr\", \n",
    "             \"MARTIN LUTHER KING JR DR\", \"MARTIN LUTHER KING JR DR\",\n",
    "             \"COLUMBIA SQUARE ST\",\n",
    "             \"CONSERVATORY EAST DR\",\n",
    "             \" BYPASS\",\n",
    "              \"I-80 EASTBOUND\", \"I-80 EASTBOUND\",\n",
    "             \"HWY 101 Northbound\",\n",
    "             \"THE EMBARCADERO\", \"THE EMBARCADERO\", \"THE EMBARCADERO\",\n",
    "             \"I-80\",\"\", \"Northbound\", \"\", \"\",\n",
    "             \"\",\"\",\"\",\n",
    "             \"HWY 1\", \"I-80\",\"I-280\", \"\",\n",
    "             \"I-280 Southbound\", \"\",\"HWY 101 Southbound\",\n",
    "             \"\", \"\",\"HWY 101 Southbound\", \"HWY 1 Southbound\", \"HWY 1\", \"\",\n",
    "             \"\", \"ON RAMP\",\"\",\"\",\"HWY\", \"HWY.\",\n",
    "             \"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\n",
    "             \"\",\"\", \"\", \"\", \"\", \n",
    "             \"\" , \"\", \"\",\"\", \"\", \"\", \"\",\n",
    "             \"\", \"\", \"\",\"\", \"\",\n",
    "             \"\", \"\", \"\", \"\", \"\",\"\",\"\",\"\",\n",
    "             \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "             \"\", \"\", \"\", \"\",\"\", \"\", \"\",\"\",\"\",\n",
    "             \"\", \"\", \"\", \"\",\"\", \"\",\"\",\"\",\"\",\n",
    "             \"\", \"\", \"\",   \n",
    "            ]\n",
    "\n",
    "dfresult[[\"PRIMARY_RD_3\",\"SECONDARY_RD_3\"]] = dfresult[[\"PRIMARY_RD_2\",\"SECONDARY_RD_2\"]].replace(orig_list,repl_list,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06476b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresult[\"PRIMARY_RD_3\"] = dfresult[\"PRIMARY_RD_3\"].str.upper()\n",
    "dfresult[\"SECONDARY_RD_3\"] = dfresult[\"SECONDARY_RD_3\"].str.upper()\n",
    "dfresult[\"PRIMARY_RD_3\"] = dfresult.apply(lambda x: cleanStreetName(x[\"PRIMARY_RD_3\"]), axis=1)\n",
    "dfresult[\"SECONDARY_RD_3\"] = dfresult.apply(lambda x: cleanStreetName(x[\"SECONDARY_RD_3\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbdf3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIntersections[\"Intersecting_Streets\"] = dfIntersections.apply(lambda x: cleanStreetName(x[\"x_street_comb\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9581c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIntersections[\"LatLong\"] = dfIntersections[\"latitude\"].astype(str) + \" \" + dfIntersections[\"longitude\"].astype(str)\n",
    "# get unique street intersections and their latitude and longtitude\n",
    "dfIntersections[\"Intersecting_Streets\"] = dfIntersections[\"Intersecting_Streets\"].str.upper()\n",
    "dfIntersections.drop_duplicates(['Intersecting_Streets'])\n",
    "mydict = dict(zip(dfIntersections.Intersecting_Streets, dfIntersections.LatLong.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b194e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def getDerivedLatLngPosition(lat,long, dist, bearing):\n",
    "    latitude = float(lat)\n",
    "    longitude = float(long)\n",
    "    \n",
    "    DegreesToRadians = math.pi/180.0\n",
    "    RadiansToDegrees = 180.0/math.pi \n",
    "    EarthRadius = 6378137.0 # Radius of Earth in meters\n",
    "\n",
    "    latA =  latitude * DegreesToRadians\n",
    "    longA = longitude * DegreesToRadians\n",
    "    angularDistance = dist/EarthRadius\n",
    "    trueCourse = bearing * DegreesToRadians\n",
    "    \n",
    "    lat =  math.asin(math.sin(latA) * math.cos(angularDistance) + math.cos(latA) * math.sin(angularDistance) * math.cos(trueCourse))\n",
    "    \n",
    "    dlon = math.atan2(math.sin(trueCourse) * math.sin(angularDistance) * math.cos(latA), math.cos(angularDistance) - math.sin(latA) * math.sin(lat))\n",
    "    lon = ((longA + dlon + math.pi) % (math.pi*2)) - math.pi\n",
    "    \n",
    "    return Point(round(lon * RadiansToDegrees,5),round(lat * RadiansToDegrees,5))\n",
    "\n",
    "# Switcher is dictionary data type here\n",
    "def get_direction(argument):\n",
    "    switcher = {\n",
    "        \"N\": 0,\n",
    "        \"W\": -90,\n",
    "        \"E\": +90,\n",
    "        \"S\": -180\n",
    "    }  \n",
    "    # get() method of dictionary data type returns value of passed argument if it is present in dictionary;\n",
    "    # otherwise second argument will be assigned as default value of passed argument\n",
    "    return switcher.get(argument, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33be8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlongsearch(row):\n",
    "    primary_rd = row[\"PRIMARY_RD_3\"]\n",
    "    secondary_rd = row[\"SECONDARY_RD_3\"] \n",
    "    _dict = mydict.copy()\n",
    "    for key, value in _dict.items():\n",
    "        if (primary_rd in key) and (secondary_rd in key):\n",
    "            lat = value.split(\" \")[0]\n",
    "            long = value.split(\" \")[1]\n",
    "            dist = 0 if (row[\"DISTANCE\"]<= 0) else (row[\"DISTANCE\"]/3.2808)\n",
    "#             print(row[\"DIRECTION\"])\n",
    "            bearing = get_direction(row[\"DIRECTION\"].upper())\n",
    "            return getDerivedLatLngPosition(lat,long, dist, bearing)\n",
    "    return None         \n",
    "dfresult[\"DIRECTION\"]=dfresult[\"DIRECTION\"].fillna('')\n",
    "dfresult[\"ref_latlong2\"] = dfresult.apply(lambda row: latlongsearch(row), axis=1)\n",
    "dfresult.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_SWITRS_TIMS_matched_output_v3.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026f847d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>CASE_ID</th>\n",
       "      <th>ACCIDENT_YEAR</th>\n",
       "      <th>PROC_DATE</th>\n",
       "      <th>JURIS</th>\n",
       "      <th>COLLISION_DATE</th>\n",
       "      <th>COLLISION_TIME</th>\n",
       "      <th>OFFICER_ID</th>\n",
       "      <th>REPORTING_DISTRICT</th>\n",
       "      <th>...</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CITY</th>\n",
       "      <th>POINT_X</th>\n",
       "      <th>POINT_Y</th>\n",
       "      <th>PRIMARY_RD_2</th>\n",
       "      <th>SECONDARY_RD_2</th>\n",
       "      <th>Rd_Names</th>\n",
       "      <th>ref_latlong</th>\n",
       "      <th>PRIMARY_RD_3</th>\n",
       "      <th>SECONDARY_RD_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4392133</td>\n",
       "      <td>2010</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>9335</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>415</td>\n",
       "      <td>19084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>-122.405503</td>\n",
       "      <td>37.775845</td>\n",
       "      <td>rt 80</td>\n",
       "      <td>7th</td>\n",
       "      <td>7th,rt 80</td>\n",
       "      <td>POINT (-122.4055 37.77584)</td>\n",
       "      <td>I-80</td>\n",
       "      <td>7th st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4392442</td>\n",
       "      <td>2010</td>\n",
       "      <td>2012-01-25</td>\n",
       "      <td>9335</td>\n",
       "      <td>2010-02-25</td>\n",
       "      <td>1302</td>\n",
       "      <td>17672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>-122.428798</td>\n",
       "      <td>37.732164</td>\n",
       "      <td>rt 280</td>\n",
       "      <td>mission</td>\n",
       "      <td>rt 280,mission</td>\n",
       "      <td>POINT (-122.4288 37.73216)</td>\n",
       "      <td>I-280</td>\n",
       "      <td>mission st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4523135</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-10-26</td>\n",
       "      <td>3801</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>841</td>\n",
       "      <td>1794</td>\n",
       "      <td>south</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>-122.410433</td>\n",
       "      <td>37.778795</td>\n",
       "      <td>7th</td>\n",
       "      <td>minna</td>\n",
       "      <td>7th,minna</td>\n",
       "      <td>POINT (-122.41043 37.77879)</td>\n",
       "      <td>7th st</td>\n",
       "      <td>minna st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4523139</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-10-26</td>\n",
       "      <td>3801</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>3</td>\n",
       "      <td>307</td>\n",
       "      <td>bayvi</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>-122.401383</td>\n",
       "      <td>37.762285</td>\n",
       "      <td>de haro</td>\n",
       "      <td>18th</td>\n",
       "      <td>de haro,18th</td>\n",
       "      <td>POINT (-122.40133 37.76228)</td>\n",
       "      <td>de haro st</td>\n",
       "      <td>18th st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4523186</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-10-26</td>\n",
       "      <td>3801</td>\n",
       "      <td>2010-01-09</td>\n",
       "      <td>545</td>\n",
       "      <td>000779</td>\n",
       "      <td>north</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>-122.417583</td>\n",
       "      <td>37.783305</td>\n",
       "      <td>larkin</td>\n",
       "      <td>eddy</td>\n",
       "      <td>larkin,eddy</td>\n",
       "      <td>POINT (-122.41755 37.78332)</td>\n",
       "      <td>larkin st</td>\n",
       "      <td>eddy st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13921</th>\n",
       "      <td>7404</td>\n",
       "      <td>44824</td>\n",
       "      <td>90366351</td>\n",
       "      <td>2016</td>\n",
       "      <td>20170111</td>\n",
       "      <td>9335</td>\n",
       "      <td>20161114</td>\n",
       "      <td>1800</td>\n",
       "      <td>020509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-122.428880</td>\n",
       "      <td>37.732190</td>\n",
       "      <td>i-280 southbound</td>\n",
       "      <td>missionreet o/c</td>\n",
       "      <td>i-280 southbound,missionreet o/c</td>\n",
       "      <td>POINT (-122.42888 37.73219)</td>\n",
       "      <td>i-280</td>\n",
       "      <td>mission st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13922</th>\n",
       "      <td>7405</td>\n",
       "      <td>44826</td>\n",
       "      <td>90371177</td>\n",
       "      <td>2016</td>\n",
       "      <td>20170213</td>\n",
       "      <td>9335</td>\n",
       "      <td>20161216</td>\n",
       "      <td>1735</td>\n",
       "      <td>021424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-122.406490</td>\n",
       "      <td>37.743290</td>\n",
       "      <td>us-101 s/b</td>\n",
       "      <td>cesar chavez</td>\n",
       "      <td>cesar chavez,us-101 s/b</td>\n",
       "      <td>POINT (-122.40649 37.74329)</td>\n",
       "      <td>HWY 101</td>\n",
       "      <td>cesar chavez st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13923</th>\n",
       "      <td>7406</td>\n",
       "      <td>44827</td>\n",
       "      <td>90373242</td>\n",
       "      <td>2016</td>\n",
       "      <td>20170119</td>\n",
       "      <td>9335</td>\n",
       "      <td>20161208</td>\n",
       "      <td>1550</td>\n",
       "      <td>021293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-122.405750</td>\n",
       "      <td>37.767930</td>\n",
       "      <td>us-101 s/b</td>\n",
       "      <td>alameda</td>\n",
       "      <td>us-101 s/b,alameda</td>\n",
       "      <td>POINT (-122.40575 37.76793)</td>\n",
       "      <td>HWY 101</td>\n",
       "      <td>alameda st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13924</th>\n",
       "      <td>7407</td>\n",
       "      <td>44828</td>\n",
       "      <td>90378461</td>\n",
       "      <td>2016</td>\n",
       "      <td>20170126</td>\n",
       "      <td>9335</td>\n",
       "      <td>20161203</td>\n",
       "      <td>1750</td>\n",
       "      <td>021455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-122.403710</td>\n",
       "      <td>37.749020</td>\n",
       "      <td>us-101 n/b</td>\n",
       "      <td>cesar chavez</td>\n",
       "      <td>cesar chavez,us-101 n/b</td>\n",
       "      <td>POINT (-122.40371 37.74902)</td>\n",
       "      <td>HWY 101</td>\n",
       "      <td>cesar chavez st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13925</th>\n",
       "      <td>7408</td>\n",
       "      <td>44831</td>\n",
       "      <td>90465697</td>\n",
       "      <td>2016</td>\n",
       "      <td>20170526</td>\n",
       "      <td>9335</td>\n",
       "      <td>20161012</td>\n",
       "      <td>928</td>\n",
       "      <td>021293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-122.406700</td>\n",
       "      <td>37.735360</td>\n",
       "      <td>i-280 n/b</td>\n",
       "      <td>bayshore</td>\n",
       "      <td>bayshore,i-280 n/b</td>\n",
       "      <td>POINT (-122.4067 37.73536)</td>\n",
       "      <td>i-280</td>\n",
       "      <td>bay shore bl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13926 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1   CASE_ID  ACCIDENT_YEAR   PROC_DATE  JURIS  \\\n",
       "0               0             0   4392133           2010  2012-01-23   9335   \n",
       "1               1             1   4392442           2010  2012-01-25   9335   \n",
       "2               2             2   4523135           2010  2010-10-26   3801   \n",
       "3               3             3   4523139           2010  2010-10-26   3801   \n",
       "4               4             4   4523186           2010  2010-10-26   3801   \n",
       "...           ...           ...       ...            ...         ...    ...   \n",
       "13921        7404         44824  90366351           2016    20170111   9335   \n",
       "13922        7405         44826  90371177           2016    20170213   9335   \n",
       "13923        7406         44827  90373242           2016    20170119   9335   \n",
       "13924        7407         44828  90378461           2016    20170126   9335   \n",
       "13925        7408         44831  90465697           2016    20170526   9335   \n",
       "\n",
       "      COLLISION_DATE  COLLISION_TIME OFFICER_ID REPORTING_DISTRICT  ...  \\\n",
       "0         2010-01-31             415      19084                NaN  ...   \n",
       "1         2010-02-25            1302      17672                NaN  ...   \n",
       "2         2010-01-07             841       1794              south  ...   \n",
       "3         2010-01-05               3        307              bayvi  ...   \n",
       "4         2010-01-09             545     000779              north  ...   \n",
       "...              ...             ...        ...                ...  ...   \n",
       "13921       20161114            1800     020509                NaN  ...   \n",
       "13922       20161216            1735     021424                NaN  ...   \n",
       "13923       20161208            1550     021293                NaN  ...   \n",
       "13924       20161203            1750     021455                NaN  ...   \n",
       "13925       20161012             928     021293                NaN  ...   \n",
       "\n",
       "              COUNTY           CITY     POINT_X    POINT_Y      PRIMARY_RD_2  \\\n",
       "0      san francisco  san francisco -122.405503  37.775845             rt 80   \n",
       "1      san francisco  san francisco -122.428798  37.732164            rt 280   \n",
       "2      san francisco  san francisco -122.410433  37.778795               7th   \n",
       "3      san francisco  san francisco -122.401383  37.762285           de haro   \n",
       "4      san francisco  san francisco -122.417583  37.783305            larkin   \n",
       "...              ...            ...         ...        ...               ...   \n",
       "13921            NaN            NaN -122.428880  37.732190  i-280 southbound   \n",
       "13922            NaN            NaN -122.406490  37.743290        us-101 s/b   \n",
       "13923            NaN            NaN -122.405750  37.767930        us-101 s/b   \n",
       "13924            NaN            NaN -122.403710  37.749020        us-101 n/b   \n",
       "13925            NaN            NaN -122.406700  37.735360         i-280 n/b   \n",
       "\n",
       "        SECONDARY_RD_2                          Rd_Names  \\\n",
       "0                  7th                         7th,rt 80   \n",
       "1              mission                    rt 280,mission   \n",
       "2                minna                         7th,minna   \n",
       "3                 18th                      de haro,18th   \n",
       "4                 eddy                       larkin,eddy   \n",
       "...                ...                               ...   \n",
       "13921  missionreet o/c  i-280 southbound,missionreet o/c   \n",
       "13922     cesar chavez           cesar chavez,us-101 s/b   \n",
       "13923          alameda                us-101 s/b,alameda   \n",
       "13924     cesar chavez           cesar chavez,us-101 n/b   \n",
       "13925         bayshore                bayshore,i-280 n/b   \n",
       "\n",
       "                       ref_latlong  PRIMARY_RD_3    SECONDARY_RD_3  \n",
       "0       POINT (-122.4055 37.77584)          I-80            7th st  \n",
       "1       POINT (-122.4288 37.73216)         I-280        mission st  \n",
       "2      POINT (-122.41043 37.77879)        7th st          minna st  \n",
       "3      POINT (-122.40133 37.76228)    de haro st           18th st  \n",
       "4      POINT (-122.41755 37.78332)     larkin st           eddy st  \n",
       "...                            ...           ...               ...  \n",
       "13921  POINT (-122.42888 37.73219)        i-280         mission st  \n",
       "13922  POINT (-122.40649 37.74329)       HWY 101  cesar chavez st   \n",
       "13923  POINT (-122.40575 37.76793)       HWY 101        alameda st  \n",
       "13924  POINT (-122.40371 37.74902)       HWY 101  cesar chavez st   \n",
       "13925   POINT (-122.4067 37.73536)         i-280      bay shore bl  \n",
       "\n",
       "[13926 rows x 88 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a01486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c68f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIntersections = pd.read_csv(BASE_DIR.parent.joinpath(\"TransBase_Shapefiles\",\"street_intersections.csv\"))\n",
    "\n",
    "dfresult=pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_all_SWITRS_TIMS_matched_output.csv\"))\n",
    "\n",
    "dfrectified = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Road_Crashes_SWITRS_TIMS_matched_output_v3.3.csv\"))\n",
    "PRIMARY_RD_dict = dict(zip(dfrectified[\"CASE_ID\"],dfrectified[\"PRIMARY_RD_3\"]))\n",
    "SECONDARY_RD_dict = dict(zip(dfrectified[\"CASE_ID\"],dfrectified[\"SECONDARY_RD_3\"]))\n",
    "dfresult[\"PRIMARY_RD_3\"] = dfresult[\"CASE_ID\"].map(PRIMARY_RD_dict)\n",
    "dfresult[\"SECONDARY_RD_3\"] = dfresult[\"CASE_ID\"].map(SECONDARY_RD_dict)\n",
    "dfresult = dfresult.dropna(axis=0, subset=[\"PRIMARY_RD_3\",\"SECONDARY_RD_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a835279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanStreetName(streetName):\n",
    "    newStreetName = streetName.strip()\n",
    "    corrections = {\"TWELFTH\":\"12TH\", \n",
    "                   \"ELEVENTH\":\"11TH\",\n",
    "                   \"TENTH\":\"10TH\",\n",
    "                   \"NINTH\":\"9TH\",\n",
    "                   \"EIGHTH\":\"8TH\",\n",
    "                   \"SEVENTH\":\"7TH\",\n",
    "                   \"SIXTH\":\"6TH\",\n",
    "                   \"FIFTH\":\"5TH\",\n",
    "                   \"FOURTH\":\"4TH\",\n",
    "                   \"THIRD\":\"3RD\",\n",
    "                   \"SECOND\":\"2ND\",\n",
    "                   \"FIRST\":\"1ST\",\n",
    "                   \"3RDREET\":\"3RD\",\n",
    "                   \"EMBARCADERO/KING\":\"THE EMBARCADERO\",\n",
    "                   \"VAN NESSNUE\":\"VAN NESS\",\n",
    "                   \"3RD #3\":\"3RD\",\n",
    "                   \"09TH\":\"9TH\",\n",
    "                   \"08TH\":\"8TH\",\n",
    "                   \"07TH\":\"7TH\",\n",
    "                   \"06TH\":\"6TH\",\n",
    "                   \"05TH\":\"5TH\",\n",
    "                   \"04TH\":\"4TH\",\n",
    "                   \"03RD\":\"3RD\",\n",
    "                   \"02ND\":\"2ND\",\n",
    "                   \"01ST\":\"1ST\",\n",
    "                  }\n",
    "\n",
    "    itemsToRemove = [\" STREETS\",\n",
    "                     \" STS.\",\n",
    "                     \" STS\",\n",
    "                     \" ST.\",\n",
    "                     \" ST\",\n",
    "                     \" Street\",\n",
    "                     \" STREET\",\n",
    "                     \" RD.\",\n",
    "                     \" RD\",\n",
    "                     \" Road\",\n",
    "                     \" AVE.\",\n",
    "                     \" AVES\",\n",
    "                     \" AVE\",\n",
    "                     \" AV\",\n",
    "                     \" Avenue\",\n",
    "                     \" BLVD.\",\n",
    "                     \" BLVD\",\n",
    "                     \" BL\",                     \n",
    "                     \" Boulevard\",\n",
    "                     \" MASTER:\",\n",
    "                     \" DR.\",\n",
    "                     \" DR\",\n",
    "                     \" Drive\",\n",
    "                     \" WY\",\n",
    "                     \" Way\",\n",
    "                     \" CT\",\n",
    "                     \" TERR\",\n",
    "                     \" Terrace\",\n",
    "                     \" EXPY\",\n",
    "                    ]\n",
    "          \n",
    "    for wrongName, rightName in corrections.items():\n",
    "        if wrongName in streetName:\n",
    "            newStreetName = streetName.replace(wrongName, rightName)\n",
    "        if streetName == 'EMBARCADERO':\n",
    "            newStreetName = \"THE EMBARCADERO\"\n",
    "        if streetName.endswith(\" DR\"):\n",
    "            newStreetName = streetName[:-3]\n",
    "        if streetName.endswith(\" AV\"):\n",
    "            newStreetName = streetName[:-3]\n",
    "        if \" TO \" in streetName:\n",
    "            cutOff = streetName.find(\" TO \")\n",
    "            newStreetName = streetName[:cutOff]            \n",
    "    \n",
    "    for item in itemsToRemove:\n",
    "        if item in newStreetName:\n",
    "            newStreetName = newStreetName.replace(item, \"\")\n",
    "    \n",
    "    return newStreetName.strip()\n",
    "\n",
    "dfresult[\"PRIMARY_RD_3\"] = dfresult[\"PRIMARY_RD_3\"].str.upper()\n",
    "dfresult[\"SECONDARY_RD_3\"] = dfresult[\"SECONDARY_RD_3\"].str.upper()\n",
    "dfresult[\"PRIMARY_RD_3\"] = dfresult.apply(lambda x: cleanStreetName(x[\"PRIMARY_RD_3\"]), axis=1)\n",
    "dfresult[\"SECONDARY_RD_3\"] = dfresult.apply(lambda x: cleanStreetName(x[\"SECONDARY_RD_3\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66bd32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIntersections[\"LatLong\"] = dfIntersections[\"latitude\"].astype(str) + \" \" + dfIntersections[\"longitude\"].astype(str)\n",
    "# get unique street intersections and their latitude and longtitude\n",
    "dfIntersections[\"Intersecting_Streets\"] = dfIntersections[\"x_street_comb\"].str.upper()\n",
    "dfIntersections.drop_duplicates(['Intersecting_Streets'])\n",
    "mydict = dict(zip(dfIntersections.Intersecting_Streets, dfIntersections.LatLong.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4feb9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def getDerivedLatLngPosition(lat,long, dist, bearing):\n",
    "    latitude = float(lat)\n",
    "    longitude = float(long)\n",
    "    \n",
    "    DegreesToRadians = math.pi/180.0\n",
    "    RadiansToDegrees = 180.0/math.pi \n",
    "    EarthRadius = 6378137.0 # Radius of Earth in meters\n",
    "\n",
    "    latA =  latitude * DegreesToRadians\n",
    "    longA = longitude * DegreesToRadians\n",
    "    angularDistance = dist/EarthRadius\n",
    "    trueCourse = bearing * DegreesToRadians\n",
    "    \n",
    "    lat =  math.asin(math.sin(latA) * math.cos(angularDistance) + math.cos(latA) * math.sin(angularDistance) * math.cos(trueCourse))\n",
    "    \n",
    "    dlon = math.atan2(math.sin(trueCourse) * math.sin(angularDistance) * math.cos(latA), math.cos(angularDistance) - math.sin(latA) * math.sin(lat))\n",
    "    lon = ((longA + dlon + math.pi) % (math.pi*2)) - math.pi\n",
    "    \n",
    "    return Point(round(lon * RadiansToDegrees,5),round(lat * RadiansToDegrees,5))\n",
    "\n",
    "# Switcher is dictionary data type here\n",
    "def get_direction(argument):\n",
    "    switcher = {\n",
    "        \"N\": 0,\n",
    "        \"W\": -90,\n",
    "        \"E\": +90,\n",
    "        \"S\": -180\n",
    "    }  \n",
    "    # get() method of dictionary data type returns value of passed argument if it is present in dictionary;\n",
    "    # otherwise second argument will be assigned as default value of passed argument\n",
    "    return switcher.get(argument, 0)\n",
    "\n",
    "\n",
    "def latlongsearch(row):\n",
    "    primary_rd = row[\"PRIMARY_RD_3\"]\n",
    "    secondary_rd = row[\"SECONDARY_RD_3\"] \n",
    "    _dict = mydict.copy()\n",
    "    for key, value in _dict.items():\n",
    "        if (primary_rd in key) and (secondary_rd in key):\n",
    "            lat = value.split(\" \")[0]\n",
    "            long = value.split(\" \")[1]\n",
    "            dist = 0 if (row[\"DISTANCE\"]<= 0) else (round((row[\"DISTANCE\"]/3.2808)*0.1,2))\n",
    "#             print(row[\"DIRECTION\"])\n",
    "            bearing = get_direction(row[\"DIRECTION\"].upper())\n",
    "            return getDerivedLatLngPosition(lat,long, dist, bearing)\n",
    "    return None         \n",
    "dfresult[\"DIRECTION\"]=dfresult[\"DIRECTION\"].fillna('')\n",
    "dfresult[\"ref_latlong2\"] = dfresult.apply(lambda row: latlongsearch(row), axis=1)\n",
    "dfresult.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"22Oct2021\",\"Road_Crashes_SWITRS_TIMS_matched_output_v3.4.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b5dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_latlong(row):\n",
    "    if pd.isnull(row[\"ref_latlong2\"]):\n",
    "        if pd.notna(row[\"POINT_X\"]):\n",
    "            return Point(round(row[\"POINT_X\"],5),round(row[\"POINT_Y\"],5))\n",
    "    else:\n",
    "        return row[\"ref_latlong2\"]\n",
    "dfresult[\"ref_latlong2\"] = dfresult.apply(lambda row: fill_null_latlong(row), axis=1)\n",
    "cols = [\"Unnamed: 0\",\"Unnamed: 0.1\",\"PRIMARY_RD_2\",\"SECONDARY_RD_2\",\"Rd_Names\",\"ref_latlong\"]\n",
    "dfresult = dfresult.drop(cols,axis=1).copy()\n",
    "dfresult.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"22Oct2021\",\"Road_Crashes_SWITRS_TIMS_matched_output_v3.4.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e597e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-e714b0e29bb8>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2010[\"ref_latlong2\"] = df2010[\"CASE_ID\"].map(lstcaseID2010).fillna(df2010[\"ref_latlong2\"])\n",
      "<ipython-input-29-e714b0e29bb8>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2016[\"ref_latlong2\"] = df2016[\"CASE_ID\"].map(lstcaseID2016).fillna(df2016[\"ref_latlong2\"])\n"
     ]
    }
   ],
   "source": [
    "df2010 = dfresult[dfresult[\"ACCIDENT_YEAR\"]==2010]\n",
    "df2016 = dfresult[dfresult[\"ACCIDENT_YEAR\"]==2016]\n",
    "\n",
    "# find the null cells\n",
    "dfcaseID2010 = df2010[df2010[\"ref_latlong2\"].isnull()][\"CASE_ID\"].to_list()\n",
    "dfcaseID2016 = df2016[df2016[\"ref_latlong2\"].isnull()][\"CASE_ID\"].to_list()\n",
    "\n",
    "rd_crash_2010 = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Edited\",\"2010_Road_Crashes_SWITRS_TIMS_matched_output_v3.4.csv\"))\n",
    "rd_crash_2016 = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Edited\",\"2016_Road_Crashes_SWITRS_TIMS_matched_output_v3.4.csv\"))\n",
    "\n",
    "lstcaseID2010 = dict(zip(rd_crash_2010[rd_crash_2010[\"CASE_ID\"].isin(dfcaseID2010)][\"CASE_ID\"],rd_crash_2010[rd_crash_2010[\"CASE_ID\"].isin(dfcaseID2010)][\"ref_latlong2\"]))\n",
    "lstcaseID2016 = dict(zip(rd_crash_2016[rd_crash_2016[\"CASE_ID\"].isin(dfcaseID2016)][\"CASE_ID\"],rd_crash_2016[rd_crash_2016[\"CASE_ID\"].isin(dfcaseID2016)][\"ref_latlong2\"]))\n",
    "\n",
    "df2010[\"ref_latlong2\"] = df2010[\"CASE_ID\"].map(lstcaseID2010).fillna(df2010[\"ref_latlong2\"])\n",
    "df2016[\"ref_latlong2\"] = df2016[\"CASE_ID\"].map(lstcaseID2016).fillna(df2016[\"ref_latlong2\"])\n",
    "    \n",
    "# df2010 = dfresult[dfresult[\"ACCIDENT_YEAR\"]==2010].copy()\n",
    "df2010.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"22Oct2021\",\"2010_Road_Crashes_SWITRS_TIMS_matched_output_v3.4.csv\"))\n",
    "# df2016 = dfresult[dfresult[\"ACCIDENT_YEAR\"]==2016].copy()\n",
    "df2016.to_csv(BASE_DIR.parent.joinpath(\"Data\",\"22Oct2021\",\"2016_Road_Crashes_SWITRS_TIMS_matched_output_v3.4.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11db3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2010 = dfresult[dfresult[\"ACCIDENT_YEAR\"]==2010]\n",
    "df2016 = dfresult[dfresult[\"ACCIDENT_YEAR\"]==2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907090a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2010[df2010[\"ref_latlong2\"].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08bf978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_crash_2010 = pd.read_csv(BASE_DIR.parent.joinpath(\"Data\",\"02Oct2021\",\"Edited\",\"2010_Road_Crashes_SWITRS_TIMS_matched_output_v3.4.csv\"))\n",
    "lstcaseID2010 = dict(zip(rd_crash_2010[rd_crash_2010[\"CASE_ID\"].isin(dfcaseID2010)][\"CASE_ID\"],rd_crash_2010[rd_crash_2010[\"CASE_ID\"].isin(dfcaseID2010)][\"ref_latlong2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fc2e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4892462\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(lstcaseID2010)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75f70b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_ID</th>\n",
       "      <th>ACCIDENT_YEAR</th>\n",
       "      <th>PROC_DATE</th>\n",
       "      <th>JURIS</th>\n",
       "      <th>COLLISION_DATE</th>\n",
       "      <th>COLLISION_TIME</th>\n",
       "      <th>OFFICER_ID</th>\n",
       "      <th>REPORTING_DISTRICT</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>CHP_SHIFT</th>\n",
       "      <th>...</th>\n",
       "      <th>SECONDARY_RAMP</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CITY</th>\n",
       "      <th>POINT_X</th>\n",
       "      <th>POINT_Y</th>\n",
       "      <th>PRIMARY_RD_3</th>\n",
       "      <th>SECONDARY_RD_3</th>\n",
       "      <th>ref_latlong2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>4892462</td>\n",
       "      <td>2010</td>\n",
       "      <td>2011-10-12</td>\n",
       "      <td>3801</td>\n",
       "      <td>2010-09-20</td>\n",
       "      <td>2153</td>\n",
       "      <td>2323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10TH</td>\n",
       "      <td>SILVER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CASE_ID  ACCIDENT_YEAR   PROC_DATE  JURIS COLLISION_DATE  \\\n",
       "2516  4892462           2010  2011-10-12   3801     2010-09-20   \n",
       "\n",
       "      COLLISION_TIME OFFICER_ID REPORTING_DISTRICT  DAY_OF_WEEK  CHP_SHIFT  \\\n",
       "2516            2153       2323                NaN            1          5   \n",
       "\n",
       "      ...  SECONDARY_RAMP  LATITUDE  LONGITUDE         COUNTY           CITY  \\\n",
       "2516  ...              -        NaN        NaN  san francisco  san francisco   \n",
       "\n",
       "      POINT_X  POINT_Y PRIMARY_RD_3 SECONDARY_RD_3 ref_latlong2  \n",
       "2516      NaN      NaN         10TH         SILVER         None  \n",
       "\n",
       "[1 rows x 83 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2010[df2010[\"CASE_ID\"]==4892462]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6455125c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CASE_ID</th>\n",
       "      <th>ACCIDENT_YEAR</th>\n",
       "      <th>PROC_DATE</th>\n",
       "      <th>JURIS</th>\n",
       "      <th>COLLISION_DATE</th>\n",
       "      <th>COLLISION_TIME</th>\n",
       "      <th>OFFICER_ID</th>\n",
       "      <th>REPORTING_DISTRICT</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>...</th>\n",
       "      <th>PRIMARY_RAMP</th>\n",
       "      <th>SECONDARY_RAMP</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CITY</th>\n",
       "      <th>PRIMARY_RD_3</th>\n",
       "      <th>SECONDARY_RD_3</th>\n",
       "      <th>ref_latlong2</th>\n",
       "      <th>XY</th>\n",
       "      <th>Unnamed: 83</th>\n",
       "      <th>Unnamed: 84</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2516</td>\n",
       "      <td>4892462</td>\n",
       "      <td>2010</td>\n",
       "      <td>10/12/2011</td>\n",
       "      <td>3801</td>\n",
       "      <td>9/20/2010</td>\n",
       "      <td>2153</td>\n",
       "      <td>2323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>10TH</td>\n",
       "      <td>SILVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  CASE_ID  ACCIDENT_YEAR   PROC_DATE  JURIS COLLISION_DATE  \\\n",
       "24        2516  4892462           2010  10/12/2011   3801      9/20/2010   \n",
       "\n",
       "    COLLISION_TIME OFFICER_ID REPORTING_DISTRICT  DAY_OF_WEEK  ...  \\\n",
       "24            2153       2323                NaN            1  ...   \n",
       "\n",
       "    PRIMARY_RAMP  SECONDARY_RAMP         COUNTY           CITY  PRIMARY_RD_3  \\\n",
       "24            -               -   san francisco  san francisco          10TH   \n",
       "\n",
       "   SECONDARY_RD_3  ref_latlong2   XY Unnamed: 83 Unnamed: 84  \n",
       "24         SILVER           NaN  NaN         NaN         NaN  \n",
       "\n",
       "[1 rows x 85 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_crash_2010.loc[rd_crash_2010[\"CASE_ID\"]==4892462]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a2835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
