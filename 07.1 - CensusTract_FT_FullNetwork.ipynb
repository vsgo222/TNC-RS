{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyproj import CRS\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "# set the working directory\n",
    "BASE_DIR = Path.cwd()\n",
    "# define the exported folder path\n",
    "# Check if folder exists\n",
    "folder_path = pathlib.Path(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"agg_network\",\"Feb242022\",\"CensusTract_FT_FullNetwork\"))\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "# print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\UoK\\OneDrive - University of Kentucky\\YR_2\\Overpass_Turbo\\notebook\n"
     ]
    }
   ],
   "source": [
    "print(BASE_DIR)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fetch SF Champ network both for year 2010, year 2016 and join the frames\n",
    "\"\"\"\n",
    "# Keep only FT types representing real road-network\n",
    "def get_realnetwork(_df):\n",
    "    \"\"\"\n",
    "    Keep only FT types representing real road-network\n",
    "    1: Fwy-Fwy Connector; 2: Freeway; 3: Expressway; 4: Collector; 5: Ramp; 6: Centroid Connector;\n",
    "    7: Major Arterial; 8: ; 9: Alley (only for DTA); 10: ; 11: Local; 12: Minor Arterial; 13: Bike only;\n",
    "    14: ; 15: Super Arterial\n",
    "    :param _df:\n",
    "    :return df:\n",
    "    \"\"\"\n",
    "    df = _df.copy()\n",
    "    if isinstance(df,gpd.GeoDataFrame):\n",
    "        if \"FT\" in df.columns:\n",
    "            df = df.loc[df[\"FT\"].isin([1,2,3,4,5,7,9,10,11,12,13,15])]\n",
    "    return df\n",
    "# create a new field to store Time-of-Day information\n",
    "def add_TOD_information(_df,TOD):\n",
    "    \"\"\"\n",
    "    before concatenating dataframes, insert the TOD information as column value\n",
    "    :param _df:\n",
    "    :param TOD:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[\"peak\"] = TOD\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "def clip_roadnetwork(_dfroadnetwork):\n",
    "    \"\"\"\n",
    "    Overlay and clip the line features to get only features inside SF County Area\n",
    "    :param _dfroadnetwork:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dfPolygon = gpd.read_file(BASE_DIR.parent.joinpath(\"Data\",\"SF_County\",\"SFBay_Boundary.shp\"))\n",
    "    dfPolygon=dfPolygon.to_crs(\"EPSG:4326\")\n",
    "    dfroadnetwork = _dfroadnetwork.to_crs(4326).copy()\n",
    "    df = gpd.clip(dfroadnetwork,dfPolygon)\n",
    "    return df\n",
    "\n",
    "def getTotalCapacity(_df):\n",
    "    \"\"\"\n",
    "    The \"CAP\" field is for per/hour; convert this to TOD period\n",
    "    :param _df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df = _df.copy()\n",
    "    df[\"Tot_CAP\"] = df[\"CAP\"]\n",
    "    df.loc[(df[\"peak\"]==\"EV\"),\"Tot_CAP\"]*=8.5\n",
    "    df.loc[(df[\"peak\"]==\"MD\"),\"Tot_CAP\"]*=6.5\n",
    "    df.loc[(df[\"peak\"]==\"AM\") | (df[\"peak\"]==\"PM\") |(df[\"peak\"]==\"EA\") ,\"Tot_CAP\"]*=3\n",
    "    return df\n",
    "\n",
    "def check_required_rdntwrk_colmns(_df):# road network\n",
    "    df = _df.copy()\n",
    "    d = {}\n",
    "    reqd_colmns = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                   'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "                   'OOS', 'PUDO','Tot_Vol',\"Tot_TNC_Vol\", \"Tot_Non_TNC_Vol\",\n",
    "                   'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',\n",
    "                   ]\n",
    "    for col in reqd_colmns:\n",
    "        if col not in df.columns:\n",
    "            d[col]=0\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "def merge_TOD_dfs(_dfAM,_dfPM,_dfEA,_dfEV,_dfMD):\n",
    "    # clean the dataframe\n",
    "    dfAM = get_realnetwork(_dfAM.copy())\n",
    "    dfPM = get_realnetwork(_dfPM.copy())\n",
    "    dfEA = get_realnetwork(_dfEA.copy())\n",
    "    dfEV = get_realnetwork(_dfEV.copy())\n",
    "    dfMD = get_realnetwork(_dfMD.copy())\n",
    "    # clip road network\n",
    "    dfAM = clip_roadnetwork(dfAM.copy())\n",
    "    dfPM = clip_roadnetwork(dfPM.copy())\n",
    "    dfEA = clip_roadnetwork(dfEA.copy())\n",
    "    dfEV = clip_roadnetwork(dfEV.copy())\n",
    "    dfMD = clip_roadnetwork(dfMD.copy())\n",
    "    # add TOD information\n",
    "    dfAM = add_TOD_information(dfAM.copy(),\"AM\")\n",
    "    dfPM = add_TOD_information(dfPM.copy(),\"PM\")\n",
    "    dfEA = add_TOD_information(dfEA.copy(),\"EA\")\n",
    "    dfEV = add_TOD_information(dfEV.copy(),\"EV\")\n",
    "    dfMD = add_TOD_information(dfMD.copy(),\"MD\")\n",
    "    # concat dataframes\n",
    "    df = pd.concat([dfAM,dfPM,dfEA,dfEV,dfMD])\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    # get total capacity on the link for the day\n",
    "    df = getTotalCapacity(df.copy())\n",
    "    df = check_required_rdntwrk_colmns(df.copy())\n",
    "    # get Tot_Vol: by adding the columns which together form Tot_Vol\n",
    "    add_for_Tot_Vol = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1',\n",
    "                       'V13_1','V14_1','V15_1','V16_1','V17_1','V18_1','V19_1',\n",
    "                       'BUSVOL_AM','BUSVOL_PM','BUSVOL_EA','BUSVOL_MD','BUSVOL_EV','OOS']\n",
    "    df[\"Tot_Vol\"] = df[add_for_Tot_Vol].sum(axis=1)\n",
    "    # get Tot_TNC_Vol: ['V13_1','OOS'] # V13_1 is TNC_Volumes plying on the road segment\n",
    "    # add_TNC_col = ['V13_1','OOS'] # V13_1 is TNC_Volumes plying on the road segment\n",
    "    # df[\"Tot_TNC_Vol\"] = df[add_TNC_col].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "def agg_roadnetwrk(_dfmerged):\n",
    "    # aggregate the dataframe using A_B\n",
    "    wt_avg = lambda x: np.ma.average(x, weights = _dfmerged.loc[x.index, \"Tot_Vol\"],axis=0)\n",
    "\n",
    "    # aggregate function\n",
    "    def agg_func(_dfagg):\n",
    "        lst_col = [\"SPEED\",\"TIME\",\"TIME_1\",\"CSPD_1\"]\n",
    "        # average the columns\n",
    "        avg_col = [ 'DISTANCE',\n",
    "                    \"FT\",\"AT\",\n",
    "                    'TIMESEED',\n",
    "                    'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "                    'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3',\n",
    "                    'TOLLEA_DA', 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA',\n",
    "                    'TOLLEV_SR2', 'TOLLEV_SR3',\"USE\"]\n",
    "        d = {}\n",
    "        for col in _dfagg.select_dtypes(np.number).columns:\n",
    "            if col in lst_col:\n",
    "                d[col] = wt_avg\n",
    "            elif col in avg_col:\n",
    "                d[col]=\"mean\"\n",
    "            else:\n",
    "                d[col] = \"sum\"\n",
    "        for col in _dfagg.select_dtypes(object).columns:\n",
    "            d[col] = \"first\"\n",
    "        d[\"geometry\"] = \"first\"\n",
    "        return d\n",
    "\n",
    "    _dfmerged[\"A_B\"] = _dfmerged[\"A\"].astype(str)  + \"_\" + _dfmerged[\"B\"].astype(str)\n",
    "    _dfmerged[\"A\"] = _dfmerged[\"A\"].astype(str)\n",
    "    _dfmerged[\"B\"] = _dfmerged[\"B\"].astype(str)\n",
    "\n",
    "    dfmerged_agg = _dfmerged.groupby(['A_B'],as_index=False).aggregate(agg_func(_dfmerged.copy())).copy()\n",
    "\n",
    "    return dfmerged_agg\n",
    "\n",
    "# Network for YR 2010\n",
    "dfsfrd2010 = merge_TOD_dfs(gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_AM.shp\")),\n",
    "                              gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_AM.shp\")),\n",
    "                              gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_EA.shp\")),\n",
    "                              gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_EV.shp\")),\n",
    "                              gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_MD.shp\"))\n",
    "                              )\n",
    "\n",
    "dfsfrd2010 = agg_roadnetwrk(dfsfrd2010)\n",
    "# above merge converts the geo-dataframe to pandas dataframe. So re-convert it into geodataframe\n",
    "dfsfrdntwrk2010_agg = gpd.GeoDataFrame(dfsfrd2010, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfsfrdntwrk2010_agg = dfsfrdntwrk2010_agg.to_crs(\"EPSG:4326\")\n",
    "dfsfrdntwrk2010_agg = dfsfrdntwrk2010_agg.to_crs(\"EPSG:3857\")\n",
    "dfsfrdntwrk2010_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_PCS.geojson\"), driver='GeoJSON')\n",
    "\n",
    "# Network for YR 2016\n",
    "dfsfrd2016 = merge_TOD_dfs(gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_AM.shp\")),\n",
    "                           gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_AM.shp\")),\n",
    "                           gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_EA.shp\")),\n",
    "                           gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_EV.shp\")),\n",
    "                           gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_MD.shp\"))\n",
    "                           )\n",
    "\n",
    "dfsfrd2016 = agg_roadnetwrk(dfsfrd2016)\n",
    "# above merge converts the geo-dataframe to pandas dataframe. So re-convert it into geodataframe\n",
    "dfsfrdntwrk2016_agg = gpd.GeoDataFrame(dfsfrd2016, geometry='geometry',crs=\"EPSG:4326\")\n",
    "dfsfrdntwrk2016_agg = dfsfrdntwrk2016_agg.to_crs(\"EPSG:4326\")\n",
    "dfsfrdntwrk2016_agg = dfsfrdntwrk2016_agg.to_crs(\"EPSG:3857\")\n",
    "dfsfrdntwrk2016_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_PCS.geojson\"), driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # read the files\n",
    "dfsfrdntwrk2010_agg = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_PCS.geojson\"))\n",
    "dfsfrdntwrk2016_agg = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_PCS.geojson\"))\n",
    "\n",
    "# intersect the road network with CensusTract file\n",
    "def intersect_CT_rdntwrk(_gpdCT, _gpdrdntwrkagg):\n",
    "    _gpdCT.to_crs(3857, inplace=True) # set its projection to EPSG:3857\n",
    "    # _gpdrdntwrkagg.to_crs(3857,inplace=True)\n",
    "    res_intersection = _gpdrdntwrkagg.overlay(_gpdCT, how='intersection')\n",
    "    return res_intersection\n",
    "\n",
    "def drop_intersect_CT_rdntwrk_columns(_df):\n",
    "    df = _df.copy()\n",
    "    cols = ['TOLL', 'USE', 'AT','LANE_AM', 'LANE_OP', 'LANE_PM',\n",
    "            'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "            'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3', 'TOLLEA_DA', 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA', 'TOLLEV_SR2', 'TOLLEV_SR3',\n",
    "            'VALUETOLL_', 'PASSTHRU',\n",
    "            'BUSTPS_AM', 'BUSTPS_OP', 'BUSTPS_PM', 'TSVA', 'BIKE_CLASS', 'PER_RISE', 'ONEWAY', 'DTA_EDIT_F', 'TOLLTIME', 'PHASE',\n",
    "            'AMBUSSAVE', 'MDBUSSAVE', 'PMBUSSAVE', 'EVBUSSAVE', 'EABUSSAVE',\n",
    "            'SPDC', 'CAPC', 'A', 'B', 'STREETNAME', 'TYPE', 'MTYPE', 'TSIN', 'PROJ', 'ACTION', 'AB', 'peak', 'statefp10', 'mtfcc10', 'name10', 'intptlat10', 'awater10', 'namelsad10', 'funcstat10', 'aland10', 'geoid10', 'intptlon10', 'countyfp10',\n",
    "            \"V1T_1\",'V2T_1', 'V3T_1',  'V4T_1', 'V5T_1','V6T_1', 'V7T_1','V8T_1', 'V9T_1',  'V10T_1', 'V11T_1', 'V12T_1', 'V13T_1', 'V14T_1','V15T_1', 'V16T_1', 'V17T_1','V18T_1', 'V19T_1', ]\n",
    "    df = df.drop([x for x in cols if x in df.columns], axis=1)\n",
    "    return df\n",
    "\n",
    "gdfsfct = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SF_CensusTract_PCS.geojson\"))\n",
    "\n",
    "gdfsfrd2010ct_int = intersect_CT_rdntwrk(gdfsfct,dfsfrdntwrk2010_agg)\n",
    "gdfsfrd2010ct_int = drop_intersect_CT_rdntwrk_columns(gdfsfrd2010ct_int)\n",
    "gdfsfrd2010ct_int.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_CT_PCS.geojson\"), driver='GeoJSON')\n",
    "\n",
    "gdfsfrd2016ct_int = intersect_CT_rdntwrk(gdfsfct,dfsfrdntwrk2016_agg)\n",
    "gdfsfrd2016ct_int =drop_intersect_CT_rdntwrk_columns(gdfsfrd2016ct_int)\n",
    "gdfsfrd2016ct_int.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_CT_PCS.geojson\"), driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Check in QGIS if the re-projection is successful .i.e.\n",
    "# 1. Both SFChamp_2010_agg.geojson and SFChamp_2016_agg.geojson into EPSG:3857, should ideally be named with _PCS suffix\n",
    "# 2. The SF_CensusTract is also reprojected to EPSG:3857, and ideally also named with _PCS suffix\n",
    "\n",
    "# After the above process do the following in QGIS\n",
    "# 3. Road Crash for each year i.e. 2010 and 2016, convert it to EPSG:3857, name it SFCrash_2010_PCS.geojson & SFCrash_2016_PCS.geojson\n",
    "# 4. Perform spatial intersection:\n",
    "# Intersect with SF_CensusTract and RoadNetwork and name the output as SFChamp_201x_agg_CT_PCS.geojson\n",
    "# Intersect road crashes with SF_Census Tract and name the output as SFCrash_201x_CT_PCS.geojson\n",
    "# for both, keep \"tractce\" column from SF_Census Tract in the output file\n",
    "# This ends QGIS manipulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nWe are grouping the crashes at two-levels\\n1. censustract/taz level\\n2. categorizing the crashes based upon facility type\\n\\nSo,\\nthe road crashes should be aggregated on\\n1. find the nearest link to which the crash could be attached (fld: A_B and D2NL<10)\\n2. create a unique identifier using  censustract_ID & FT of roadnetwork: tractce10_FT\\n3. aggregate all the road crashes attached to tractce10_FT\\n4. aggregate all road network attached to tractce10_FT\\n'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We are grouping the crashes at two-levels\n",
    "1. censustract/taz level\n",
    "2. categorizing the crashes based upon facility type\n",
    "\n",
    "So,\n",
    "the road crashes should be aggregated on\n",
    "1. find the nearest link to which the crash could be attached (fld: A_B and D2NL<10)\n",
    "2. create a unique identifier using  censustract_ID & FT of roadnetwork: tractce10_FT\n",
    "3. aggregate all the road crashes attached to tractce10_FT\n",
    "4. aggregate all road network attached to tractce10_FT\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# modify the dataframe to create a new column \"CATEGORY\" using \"FacilityType\"\n",
    "def label_df_by_road_category(_df,fld):\n",
    "    _df[\"category\"]=0\n",
    "    _df.loc[_df[fld].isin([1, 2, 3, 5,13]),'category']=1\n",
    "    _df.loc[_df[fld].isin([4,7,12,15]),'category']=2\n",
    "    _df.loc[_df[fld].isin([9,11 ]),'category']=3\n",
    "    return _df\n",
    "\n",
    "def add_unique_ID_using_CT_CAT(_df,CT_ID_fld,CAT_fld):\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[f\"{CAT_fld}_{CT_ID_fld}\"] = _df[CT_ID_fld].astype(str) + \"_\" + _df[CAT_fld].astype(int).astype(str)\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "# Fetch SF_Census Tract\n",
    "SF_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SF_CensusTract_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "SF_CT = SF_CT.to_crs(3857)\n",
    "\n",
    "# read the merged road network files and containing CensusTract IDs\n",
    "gdfsfrd2010ct_int= gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_CT_PCS.geojson\"))\n",
    "gdfsfrd2016ct_int= gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_CT_PCS.geojson\"))\n",
    "\n",
    "# categorize the road network by FacilityType\n",
    "gdfsfrd2010ct = label_df_by_road_category(gdfsfrd2010ct_int.copy(),\"FT\")\n",
    "gdfsfrd2016ct = label_df_by_road_category(gdfsfrd2016ct_int.copy(),\"FT\")\n",
    "\n",
    "gdfsfrd2010ct_cat = add_unique_ID_using_CT_CAT(gdfsfrd2010ct.copy(),\"tractce10\",\"category\")\n",
    "gdfsfrd2010ct_cat.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "gdfsfrd2010ct_cat.fillna(0, inplace=True)\n",
    "\n",
    "gdfsfrd2016ct_cat = add_unique_ID_using_CT_CAT(gdfsfrd2016ct.copy(),\"tractce10\",\"category\")\n",
    "gdfsfrd2016ct_cat.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "gdfsfrd2016ct_cat.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# read the (Nearest Neighbour) road crash files and containing CensusTract IDs\n",
    "gdfsfnncrash2010ct = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"NN_SFCrash_2010_CT_PCS.geojson\"))\n",
    "gdfsfnncrash2016ct = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"NN_SFCrash_2016_CT_PCS.geojson\"))\n",
    "\n",
    "# categorize the road crash by FacilityType\n",
    "gdfsfrdcrash2010ct_cat = label_df_by_road_category(gdfsfnncrash2010ct.copy(),\"FT\")\n",
    "gdfsfrdcrash2016ct_cat = label_df_by_road_category(gdfsfnncrash2016ct.copy(),\"FT\")\n",
    "\n",
    "# drop tractce10 and rename join_tractce10\n",
    "gdfsfrdcrash2010ct_cat = gdfsfrdcrash2010ct_cat.loc[:,gdfsfrdcrash2010ct_cat.columns!=\"tractce10\"].copy()\n",
    "gdfsfrdcrash2010ct_cat.rename(columns={\"join_tractce10\":\"tractce10\"},inplace=True)\n",
    "gdfsfrdcrash2016ct_cat = gdfsfrdcrash2016ct_cat.loc[:,gdfsfrdcrash2016ct_cat.columns!=\"tractce10\"].copy()\n",
    "gdfsfrdcrash2016ct_cat.rename(columns={\"join_tractce10\":\"tractce10\"},inplace=True)\n",
    "\n",
    "def convert_categorical_variables_to_dummy_variables(_df,_field_list):\n",
    "    \"\"\"\n",
    "    :param _df: this is the dataframe on which the action needs to be performed\n",
    "    :param _field_list: these is column name list which needs to be converted from categorical values to dummy values (0: No, 1: Yes)\n",
    "    :return: modified dataframe\n",
    "    \"\"\"\n",
    "    # four variables: PEDESTRIAN_ACCIDENT, BICYCLE_ACCIDENT, MOTORCYCLE_ACCIDENT, TRUCK_ACCIDENT\n",
    "    _df.loc[(_df[\"PEDESTRIAN_ACCIDENT\"]==\"Y\") | (_df[\"PEDESTRIAN_ACCIDENT\"]==\"y\"),\"Pedestrian_Collision_Count\" ] = 1\n",
    "    _df.loc[(_df[\"BICYCLE_ACCIDENT\"]==\"Y\") | (_df[\"BICYCLE_ACCIDENT\"]==\"y\"),\"Bicycle_Collision_Count\" ] = 1\n",
    "    _df.loc[(_df[\"MOTORCYCLE_ACCIDENT\"]==\"Y\") | (_df[\"MOTORCYCLE_ACCIDENT\"]==\"y\"),\"MC_Collision_Count\" ] = 1\n",
    "    _df.loc[(_df[\"TRUCK_ACCIDENT\"]==\"Y\") | (_df[\"TRUCK_ACCIDENT\"]==\"y\"),\"Truck_Collision_Count\" ] = 1\n",
    "\n",
    "    return _df\n",
    "\n",
    "def drop_crash_columns(_df):\n",
    "    \"\"\"\n",
    "    :param _df:\n",
    "    :return: drop columns which are unnecessary\n",
    "    \"\"\"\n",
    "    df = _df.copy()\n",
    "    cols = ['ACCIDENT_YEAR', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'REPORTING_DISTRICT', 'DAY_OF_WEEK', 'CHP_SHIFT', 'POPULATION', 'CNTY_CITY_LOC', 'SPECIAL_COND', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'CITY_DIVISION_LAPD', 'CHP_BEAT_CLASS', 'BEAT_NUMBER', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'WEATHER_1', 'WEATHER_2', 'STATE_HWY_IND', 'CALTRANS_COUNTY', 'CALTRANS_DISTRICT', 'STATE_ROUTE', 'ROUTE_SUFFIX', 'POSTMILE_PREFIX', 'POSTMILE', 'LOCATION_TYPE', 'RAMP_INTERSECTION', 'SIDE_OF_HWY', 'TOW_AWAY', 'COLLISION_SEVERITY',  'PARTY_COUNT', 'PRIMARY_COLL_FACTOR', 'PCF_CODE_OF_VIOL', 'PCF_VIOL_CATEGORY', 'PCF_VIOLATION', 'PCF_VIOL_SUBSECTION', 'HIT_AND_RUN', 'TYPE_OF_COLLISION', 'MVIW', 'PED_ACTION', 'ROAD_SURFACE', 'ROAD_COND_1', 'ROAD_COND_2', 'LIGHTING', 'CONTROL_DEVICE', 'CHP_ROAD_TYPE', 'PEDESTRIAN_ACCIDENT', 'BICYCLE_ACCIDENT', 'MOTORCYCLE_ACCIDENT', 'TRUCK_ACCIDENT', 'NOT_PRIVATE_PROPERTY', 'ALCOHOL_INVOLVED', 'STWD_VEHTYPE_AT_FAULT', 'CHP_VEHTYPE_AT_FAULT', 'PRIMARY_RAMP', 'SECONDARY_RAMP', 'LATITUDE', 'LONGITUDE', 'COUNTY', 'CITY', 'POINT_X', 'POINT_Y', 'PRIMARY_RD_3', 'SECONDARY_RD_3', 'tractce10', 'FT',\"CASE_ID\"]\n",
    "    df = df.drop([x for x in cols if x in df.columns], axis=1)\n",
    "    return df\n",
    "\n",
    "gdfsfrdcrash2010ct_cat = convert_categorical_variables_to_dummy_variables(gdfsfrdcrash2010ct_cat.copy(),['PEDESTRIAN_ACCIDENT', 'BICYCLE_ACCIDENT',\n",
    "                                                                                                         'MOTORCYCLE_ACCIDENT', 'TRUCK_ACCIDENT'])\n",
    "gdfsfrdcrash2016ct_cat = convert_categorical_variables_to_dummy_variables(gdfsfrdcrash2010ct_cat.copy(),['PEDESTRIAN_ACCIDENT', 'BICYCLE_ACCIDENT',\n",
    "                                                                                                         'MOTORCYCLE_ACCIDENT', 'TRUCK_ACCIDENT'])\n",
    "\n",
    "gdfsfrdcrash2010ct_cat = drop_crash_columns(add_unique_ID_using_CT_CAT(gdfsfrdcrash2010ct_cat,\"tractce10\",\"category\"))\n",
    "gdfsfrdcrash2016ct_cat = drop_crash_columns(add_unique_ID_using_CT_CAT(gdfsfrdcrash2016ct_cat,\"tractce10\",\"category\"))\n",
    "\n",
    "def min_D2NL(_df,dist):\n",
    "    df = _df.loc[_df[\"D2NL\"]<dist,:]\n",
    "    return df\n",
    "\n",
    "gdfsfrdcrash2010ct_cat = min_D2NL(gdfsfrdcrash2010ct_cat.copy(),10)\n",
    "gdfsfrdcrash2016ct_cat = min_D2NL(gdfsfrdcrash2016ct_cat.copy(),10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vsgo222\\AppData\\Local\\Temp\\ipykernel_6456\\1142291639.py:142: FutureWarning: Dropping invalid columns in SeriesGroupBy.agg is deprecated. In a future version, a TypeError will be raised. Before calling .agg, select only columns which should be valid for the function.\n",
      "  df = _df.groupby([uniqueID],as_index=False).aggregate(agg_func_rdntwrk(_df.copy())).copy()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Axis must be specified when shapes of a and weights differ.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 166>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    163\u001B[0m gdfsfrd2010ct_cat \u001B[38;5;241m=\u001B[39m add_VMT(gdfsfrd2010ct_cat\u001B[38;5;241m.\u001B[39mcopy(),\u001B[38;5;241m2010\u001B[39m)\n\u001B[0;32m    164\u001B[0m gdfsfrd2016ct_cat \u001B[38;5;241m=\u001B[39m add_VMT(gdfsfrd2016ct_cat\u001B[38;5;241m.\u001B[39mcopy(),\u001B[38;5;241m2016\u001B[39m)\n\u001B[1;32m--> 166\u001B[0m gdfsfrd2010ct_cat_agg \u001B[38;5;241m=\u001B[39m \u001B[43magg_network_by_uniqueIDs\u001B[49m\u001B[43m(\u001B[49m\u001B[43madd_length_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgdfsfrd2010ct_cat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2010\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcategory_tractce10\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m gdfsfrd2016ct_cat_agg \u001B[38;5;241m=\u001B[39m agg_network_by_uniqueIDs(add_length_columns(gdfsfrd2016ct_cat\u001B[38;5;241m.\u001B[39mcopy(),\u001B[38;5;241m2016\u001B[39m),\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory_tractce10\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36magg_network_by_uniqueIDs\u001B[1;34m(_df, uniqueID)\u001B[0m\n\u001B[0;32m    139\u001B[0m drop_col \u001B[38;5;241m=\u001B[39m [ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA_B\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtractce10\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m'\u001B[39m,]\n\u001B[0;32m    141\u001B[0m _df\u001B[38;5;241m.\u001B[39mdrop(drop_col,axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 142\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43m_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43muniqueID\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43mas_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maggregate\u001B[49m\u001B[43m(\u001B[49m\u001B[43magg_func_rdntwrk\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m get_req_fields(df)\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:869\u001B[0m, in \u001B[0;36mDataFrameGroupBy.aggregate\u001B[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    866\u001B[0m func \u001B[38;5;241m=\u001B[39m maybe_mangle_lambdas(func)\n\u001B[0;32m    868\u001B[0m op \u001B[38;5;241m=\u001B[39m GroupByApply(\u001B[38;5;28mself\u001B[39m, func, args, kwargs)\n\u001B[1;32m--> 869\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    870\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_dict_like(func) \u001B[38;5;129;01mand\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\pandas\\core\\apply.py:168\u001B[0m, in \u001B[0;36mApply.agg\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_dict_like(arg):\n\u001B[1;32m--> 168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magg_dict_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_list_like(arg):\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;66;03m# we require a list, but not a 'str'\u001B[39;00m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magg_list_like()\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\pandas\\core\\apply.py:475\u001B[0m, in \u001B[0;36mApply.agg_dict_like\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    472\u001B[0m     results \u001B[38;5;241m=\u001B[39m {key: colg\u001B[38;5;241m.\u001B[39magg(how) \u001B[38;5;28;01mfor\u001B[39;00m key, how \u001B[38;5;129;01min\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m    473\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;66;03m# key used for column selection and output\u001B[39;00m\n\u001B[1;32m--> 475\u001B[0m     results \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    476\u001B[0m         key: obj\u001B[38;5;241m.\u001B[39m_gotitem(key, ndim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39magg(how) \u001B[38;5;28;01mfor\u001B[39;00m key, how \u001B[38;5;129;01min\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    477\u001B[0m     }\n\u001B[0;32m    479\u001B[0m \u001B[38;5;66;03m# set the final keys\u001B[39;00m\n\u001B[0;32m    480\u001B[0m keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(arg\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\pandas\\core\\apply.py:476\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    472\u001B[0m     results \u001B[38;5;241m=\u001B[39m {key: colg\u001B[38;5;241m.\u001B[39magg(how) \u001B[38;5;28;01mfor\u001B[39;00m key, how \u001B[38;5;129;01min\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m    473\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;66;03m# key used for column selection and output\u001B[39;00m\n\u001B[0;32m    475\u001B[0m     results \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m--> 476\u001B[0m         key: \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gotitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key, how \u001B[38;5;129;01min\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    477\u001B[0m     }\n\u001B[0;32m    479\u001B[0m \u001B[38;5;66;03m# set the final keys\u001B[39;00m\n\u001B[0;32m    480\u001B[0m keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(arg\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:287\u001B[0m, in \u001B[0;36mSeriesGroupBy.aggregate\u001B[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_python_agg_general(func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 287\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_python_agg_general(func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;66;03m# TODO: KeyError is raised in _python_agg_general,\u001B[39;00m\n\u001B[0;32m    290\u001B[0m     \u001B[38;5;66;03m#  see test_groupby.test_basic\u001B[39;00m\n\u001B[0;32m    291\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_aggregate_named(func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1490\u001B[0m, in \u001B[0;36mGroupBy._python_agg_general\u001B[1;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1487\u001B[0m     output[key] \u001B[38;5;241m=\u001B[39m result\n\u001B[0;32m   1489\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m output:\n\u001B[1;32m-> 1490\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_python_apply_general\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selected_obj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1492\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_aggregated_output(output)\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1455\u001B[0m, in \u001B[0;36mGroupBy._python_apply_general\u001B[1;34m(self, f, data, not_indexed_same)\u001B[0m\n\u001B[0;32m   1429\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m   1430\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_python_apply_general\u001B[39m(\n\u001B[0;32m   1431\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1434\u001B[0m     not_indexed_same: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1435\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   1436\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1437\u001B[0m \u001B[38;5;124;03m    Apply function f in python space\u001B[39;00m\n\u001B[0;32m   1438\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1453\u001B[0m \u001B[38;5;124;03m        data after applying f\u001B[39;00m\n\u001B[0;32m   1454\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1455\u001B[0m     values, mutated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrouper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1457\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m not_indexed_same \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1458\u001B[0m         not_indexed_same \u001B[38;5;241m=\u001B[39m mutated \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmutated\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:761\u001B[0m, in \u001B[0;36mBaseGrouper.apply\u001B[1;34m(self, f, data, axis)\u001B[0m\n\u001B[0;32m    759\u001B[0m \u001B[38;5;66;03m# group might be modified\u001B[39;00m\n\u001B[0;32m    760\u001B[0m group_axes \u001B[38;5;241m=\u001B[39m group\u001B[38;5;241m.\u001B[39maxes\n\u001B[1;32m--> 761\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mutated \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_indexed_like(res, group_axes, axis):\n\u001B[0;32m    763\u001B[0m     mutated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1467\u001B[0m, in \u001B[0;36mGroupBy._python_agg_general.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m   1464\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m   1465\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_python_agg_general\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1466\u001B[0m     func \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mis_builtin_func(func)\n\u001B[1;32m-> 1467\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: func(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1469\u001B[0m     \u001B[38;5;66;03m# iterate through \"columns\" ex exclusions to populate output dict\u001B[39;00m\n\u001B[0;32m   1470\u001B[0m     output: \u001B[38;5;28mdict\u001B[39m[base\u001B[38;5;241m.\u001B[39mOutputKey, ArrayLike] \u001B[38;5;241m=\u001B[39m {}\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36magg_network_by_uniqueIDs.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    117\u001B[0m _df[uniqueID] \u001B[38;5;241m=\u001B[39m _df[uniqueID]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mstr\u001B[39m)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;66;03m# aggregate the dataframe using A_B\u001B[39;00m\n\u001B[1;32m--> 119\u001B[0m wt_avg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTot_VMT\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;66;03m# Aggregating rows based on one column with “, ”.join\u001B[39;00m\n\u001B[0;32m    121\u001B[0m concat_agg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m ar: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([item \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m ar \u001B[38;5;28;01mif\u001B[39;00m item])\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\numpy\\ma\\extras.py:605\u001B[0m, in \u001B[0;36maverage\u001B[1;34m(a, axis, weights, returned)\u001B[0m\n\u001B[0;32m    603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m a\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m!=\u001B[39m wgt\u001B[38;5;241m.\u001B[39mshape:\n\u001B[0;32m    604\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 605\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    606\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAxis must be specified when shapes of a and weights \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    607\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiffer.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m wgt\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    609\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    610\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1D weights expected when shapes of a and weights differ.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: Axis must be specified when shapes of a and weights differ."
     ]
    }
   ],
   "source": [
    "# aggregate road network by unqiueID: category_tractce10\n",
    "def reqd_colmns(_df):# road network\n",
    "    df = _df.copy()\n",
    "    d = {}\n",
    "    reqd_colmns = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1', 'V13_1', 'V14_1', 'V15_1', 'V16_1', 'V17_1', 'V18_1', 'V19_1',\n",
    "                   'OOS', 'PUDO',\n",
    "                   'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',\n",
    "                   'Tot_CAP', 'CAP', 'SPEED','CSPD_1',\n",
    "                   'Tot_Vol',\n",
    "                   # 'Tot_Non_TNC_Vol', 'Tot_TNC_Vol',\n",
    "                   ]\n",
    "    for col in reqd_colmns:\n",
    "        if col not in df.columns:\n",
    "            d[col]=0\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "def get_req_fields(_df):\n",
    "    fields = [\"Tot_Vol\",\"Tot_TNC_Vol\", \"Tot_Non_TNC_Vol\", \"Tot_VMT\",\"Tot_TNC_VMT\",\"Tot_Non_TNC_VMT\", \"Congested_Speed\",'CSPD_1',\"SPEED\",\"PUDO\",\"OOS\"]\n",
    "    d = {}\n",
    "    for fld in fields:\n",
    "        if fld == \"Tot_Vol\":# TNC Tot Vol\n",
    "            # cols = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "            #         'V11_1', 'V12_1', 'V13_1', 'V14_1', 'V15_1', 'V16_1', 'V17_1', 'V18_1', 'V19_1',\n",
    "            #         \"OOS\",\n",
    "            #         'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',]\n",
    "            # _df[fld] = _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = _df[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_TNC_Vol\":# TNC Tot Vol\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_Vol\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = d[\"Tot_Vol\"] - _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_VMT\":\n",
    "            d[fld] = d[\"Tot_Vol\"]*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (d[\"Tot_Vol\"] - _df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Congested_Speed\":\n",
    "            d[\"Congested_Speed\"] = (((_df[\"Length_meters\"]*0.000621371).divide(_df[\"TIME_1\"]))*60)\n",
    "            d[\"Congested_Speed_yr\"] = d[\"Congested_Speed\"]\n",
    "        elif fld == \"CSPD_1\":\n",
    "            d[\"CSPD_1_yr\"] = _df[\"CSPD_1\"]\n",
    "        elif fld == \"SPEED\":\n",
    "            d[\"SPEED_yr\"] = _df[\"SPEED\"]\n",
    "        elif fld == \"PUDO\":\n",
    "            d[\"PUDO_yr\"] = (_df[\"PUDO\"]*365)\n",
    "            d[\"PUDO_thousands\"] = _df[\"PUDO\"].divide(1000)\n",
    "            d[\"PUDO_thousands_yr\"] = d[\"PUDO_thousands\"]*365\n",
    "            d[\"PUDO_mil\"] = _df[\"PUDO\"].divide(1000000)\n",
    "            d[\"PUDO_mil_yr\"] = d[\"PUDO_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_PUDO\"] = np.log(_df[\"PUDO\"]+1)\n",
    "            d[\"log_PUDO_yr\"] = np.log(d[\"PUDO_yr\"]+1)\n",
    "            d[\"log_PUDO_thousands\"] = np.log(d[\"PUDO_thousands\"]+1)\n",
    "            d[\"log_PUDO_thousands_yr\"] = np.log(d[\"PUDO_thousands_yr\"]+1)\n",
    "            d[\"log_PUDO_mil\"] = np.log(d[\"PUDO_mil\"]+1)\n",
    "            d[\"log_PUDO_mil_yr\"] = np.log(d[\"PUDO_mil_yr\"]+1)\n",
    "        elif fld == \"OOS\":\n",
    "            d[\"OOS_yr\"] = (_df[\"OOS\"]*365)\n",
    "            d[\"OOS_thousands\"] = _df[\"OOS\"].divide(1000)\n",
    "            d[\"OOS_thousands_yr\"] = d[\"OOS_thousands\"]*365\n",
    "            d[\"OOS_mil\"] = _df[\"OOS\"].divide(1000000)\n",
    "            d[\"OOS_mil_yr\"] = d[\"OOS_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_OOS\"] = np.log(_df[\"OOS\"]+1)\n",
    "            d[\"log_OOS_yr\"] = np.log(d[\"OOS_yr\"]+1)\n",
    "            d[\"log_OOS_thousands\"] = np.log(d[\"OOS_thousands\"]+1)\n",
    "            d[\"log_OOS_thousands_yr\"] = np.log(d[\"OOS_thousands_yr\"]+1)\n",
    "            d[\"log_OOS_mil\"] = np.log(d[\"OOS_mil\"]+1)\n",
    "            d[\"log_OOS_mil_yr\"] = np.log(d[\"OOS_mil_yr\"]+1)\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "def agg_network_by_uniqueIDs(_df,uniqueID):\n",
    "    _df = reqd_colmns(_df.copy())\n",
    "    _df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    _df.fillna(0, inplace=True)\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    _df[uniqueID] = _df[uniqueID].astype(str)\n",
    "    # aggregate the dataframe using A_B\n",
    "    wt_avg = lambda x: np.ma.average(x, weights = _df.loc[x.index, \"Tot_VMT\"])\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "    def agg_func_rdntwrk(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            if col in wt_col:\n",
    "                d[col] = wt_avg\n",
    "            else:\n",
    "                d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            elif col==\"FT\":\n",
    "                d[col] = concat_agg\n",
    "        d[\"geometry\"] = \"first\"\n",
    "        return d\n",
    "\n",
    "    wt_col = [\"SPEED\",\"TIME\",\"CSPD_1\", 'VDT_1', 'VHT_1','VC_1', ]\n",
    "    str_col = ['category_tractce10',\"FT\"]\n",
    "    drop_col = [ \"A_B\",'tractce10','category',]\n",
    "\n",
    "    _df.drop(drop_col,axis=1,inplace=True)\n",
    "    df = _df.groupby([uniqueID],as_index=False).aggregate(agg_func_rdntwrk(_df.copy())).copy()\n",
    "\n",
    "    return get_req_fields(df)\n",
    "\n",
    "def add_length_columns(_df,_yr):\n",
    "    #remember to rename DISTANCE variable (as this is no longer the actual distance (in miles), given that feature is split-up)\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[\"Length_meters\"] = _df.geometry.length\n",
    "        d[\"Length_miles\"] = d[\"Length_meters\"]* 0.000621371\n",
    "        d[\"Year\"] = _yr\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "def add_VMT(_df,_yr):\n",
    "    #remember to rename DISTANCE variable (as this is no longer the actual distance (in miles), given that feature is split-up)\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[\"Tot_VMT\"] = _df[\"Tot_Vol\"]*_df.geometry.length* 0.000621371\n",
    "        d[\"Year\"] = _yr\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "gdfsfrd2010ct_cat = add_VMT(gdfsfrd2010ct_cat.copy(),2010)\n",
    "gdfsfrd2016ct_cat = add_VMT(gdfsfrd2016ct_cat.copy(),2016)\n",
    "\n",
    "gdfsfrd2010ct_cat_agg = agg_network_by_uniqueIDs(add_length_columns(gdfsfrd2010ct_cat.copy(),2010),\"category_tractce10\")\n",
    "gdfsfrd2016ct_cat_agg = agg_network_by_uniqueIDs(add_length_columns(gdfsfrd2016ct_cat.copy(),2016),\"category_tractce10\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0           10.440271\n1            0.746311\n2            0.000653\n3            3.802296\n4            0.003373\n             ...     \n33390       32.039507\n33391      440.015633\n33392      110.541627\n33393       24.418926\n33394    37150.324391\nName: Tot_VMT, Length: 33395, dtype: float64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdfsfrd2010ct_cat[\"Tot_VMT\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read the cyclist facility feature class. Assign the cycle corridor length to the right Census Tract &  FT\n",
    "\n",
    "# Fetch SF_Census Tract\n",
    "gdfCycFacility2010 = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"NN_CycFacility_SFChamp2010.geojson\"), crs = \"EPSG:3857\")\n",
    "gdfCycFacility2016 = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"NN_CycFacility_SFChamp2016.geojson\"), crs = \"EPSG:3857\")\n",
    "\n",
    "#drop original CT ids\n",
    "gdfCycFacility2010.drop(columns=[\"tractce10\"],axis=1,inplace=True)\n",
    "gdfCycFacility2016.drop(columns=[\"tractce10\"],axis=1,inplace=True)\n",
    "\n",
    "# rename columns\n",
    "gdfCycFacility2010.rename(columns={\"join_tractce10\":\"tractce10\", \"join_FT\":\"FT\"},inplace=True)\n",
    "gdfCycFacility2016.rename(columns={\"join_tractce10\":\"tractce10\", \"join_FT\":\"FT\"},inplace=True)\n",
    "\n",
    "# categorize the features\n",
    "gdfCycFacility2010 = label_df_by_road_category(gdfCycFacility2010.copy(),\"FT\")\n",
    "gdfCycFacility2016 = label_df_by_road_category(gdfCycFacility2016.copy(),\"FT\")\n",
    "\n",
    "gdfCycFacility2010_cat = add_unique_ID_using_CT_CAT(gdfCycFacility2010.copy(),\"tractce10\",\"category\")\n",
    "gdfCycFacility2016_cat = add_unique_ID_using_CT_CAT(gdfCycFacility2016.copy(),\"tractce10\",\"category\")\n",
    "\n",
    "def get_revised_cycle_length(_df):\n",
    "    #remember to rename DISTANCE variable (as this is no longer the actual distance (in miles), given that feature is split-up)\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[\"Length_meters\"] = _df.geometry.length\n",
    "        d[\"Length_miles\"] = d[\"Length_meters\"]* 0.000621371\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "gdfCycFacility2010_cat = min_D2NL(get_revised_cycle_length(gdfCycFacility2010_cat),10)\n",
    "gdfCycFacility2016_cat = min_D2NL(get_revised_cycle_length(gdfCycFacility2016_cat),10)\n",
    "\n",
    "def agg_cycinfra_length(_df,_uniqueID,_yr):\n",
    "    df = _df.loc[_df[\"install_year\"]<=_yr].copy()\n",
    "    df = df.groupby(_uniqueID).agg({\"Length_miles\":\"sum\"}).reset_index()\n",
    "\n",
    "    return dict(zip(df[_uniqueID],df[\"Length_miles\"]))\n",
    "\n",
    "cycinfra_length_2010 = agg_cycinfra_length(gdfCycFacility2010_cat,\"category_tractce10\",2010)\n",
    "cycinfra_length_2016 = agg_cycinfra_length(gdfCycFacility2016_cat,\"category_tractce10\",2016)\n",
    "\n",
    "gdfsfrd2010ct_cat_agg[\"cycleinfra_length\"] = gdfsfrd2010ct_cat_agg[\"category_tractce10\"].map(cycinfra_length_2010)\n",
    "gdfsfrd2016ct_cat_agg[\"cycleinfra_length\"] = gdfsfrd2016ct_cat_agg[\"category_tractce10\"].map(cycinfra_length_2016)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# aggregate crashes by uniqueID: category_tractce10\n",
    "def agg_crash_by_uniqueIDs(_df,uniqueID):\n",
    "    _df[uniqueID] = _df[uniqueID].astype(str)\n",
    "    str_col = [uniqueID]\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "    df = _df.groupby([uniqueID],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "    return df\n",
    "\n",
    "gdfsfrdcrash2010ct_cat_agg = agg_crash_by_uniqueIDs(gdfsfrdcrash2010ct_cat,\"category_tractce10\")\n",
    "gdfsfrdcrash2010ct_cat_agg[\"Accident_Year\"] = 2010\n",
    "gdfsfrdcrash2016ct_cat_agg = agg_crash_by_uniqueIDs(gdfsfrdcrash2016ct_cat,\"category_tractce10\")\n",
    "gdfsfrdcrash2016ct_cat_agg[\"Accident_Year\"] = 2016"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dataframe_merge(_dfroadnetwork, _dfroadcrash,left_fld,right_fld):\n",
    "    dfmerge = pd.merge(_dfroadnetwork,_dfroadcrash,left_on=left_fld,right_on=right_fld,how=\"left\")\n",
    "    return dfmerge\n",
    "\n",
    "# def update_fields(_df,_yr):\n",
    "#     df = _df.copy()\n",
    "#     if isinstance(df,pd.DataFrame):\n",
    "#         df[\"Accident_Year\"] = _yr\n",
    "#         df[\"Crash_Year\"] = _yr\n",
    "#         d = {**dict.fromkeys(df.select_dtypes(np.number).columns, 0),\n",
    "#              **dict.fromkeys(df.select_dtypes(exclude=np.number).columns,np.nan)}\n",
    "#         df.fillna(d, inplace=True)\n",
    "#     return pd.concat([df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "dfsf_rd_ntwrk_crash_2010_cat_agg = dataframe_merge(gdfsfrd2010ct_cat_agg,gdfsfrdcrash2010ct_cat_agg,\"category_tractce10\",\"category_tractce10\")\n",
    "d = {**dict.fromkeys(dfsf_rd_ntwrk_crash_2010_cat_agg.select_dtypes(np.number).columns, 0),\n",
    "     **dict.fromkeys(dfsf_rd_ntwrk_crash_2010_cat_agg.select_dtypes(exclude=np.number).columns,np.nan)}\n",
    "dfsf_rd_ntwrk_crash_2010_cat_agg =dfsf_rd_ntwrk_crash_2010_cat_agg.fillna(d)\n",
    "dfsf_rd_ntwrk_crash_2010_cat_agg[\"Accident_Year\"]=2010\n",
    "\n",
    "dfsf_rd_ntwrk_crash_2016_cat_agg = dataframe_merge(gdfsfrd2016ct_cat_agg,gdfsfrdcrash2016ct_cat_agg,\"category_tractce10\",\"category_tractce10\")\n",
    "d = {**dict.fromkeys(dfsf_rd_ntwrk_crash_2016_cat_agg.select_dtypes(np.number).columns, 0),\n",
    "     **dict.fromkeys(dfsf_rd_ntwrk_crash_2016_cat_agg.select_dtypes(exclude=np.number).columns,np.nan)}\n",
    "dfsf_rd_ntwrk_crash_2016_cat_agg =dfsf_rd_ntwrk_crash_2016_cat_agg.fillna(d)\n",
    "dfsf_rd_ntwrk_crash_2016_cat_agg[\"Accident_Year\"]=2016\n",
    "\n",
    "\n",
    "df2010 = dfsf_rd_ntwrk_crash_2010_cat_agg.loc[:,~dfsf_rd_ntwrk_crash_2010_cat_agg.columns.isin([\"geometry\"])].copy()\n",
    "df2016 = dfsf_rd_ntwrk_crash_2016_cat_agg.loc[:,~dfsf_rd_ntwrk_crash_2016_cat_agg.columns.isin([\"geometry\"])].copy()\n",
    "df2010.reset_index(drop=True, inplace=True)\n",
    "df2016.reset_index(drop=True, inplace=True)\n",
    "\n",
    "columns_to_retain = set(df2010.columns.to_list()).intersection(set(df2016.columns.to_list()))\n",
    "dfmerged =pd.concat([df2010[columns_to_retain], df2016[columns_to_retain]], ignore_index=True,verify_integrity=True,copy=True,axis=0)\n",
    "\n",
    "# few cosmetic changes to read the dataframe better\n",
    "def add_custom_fields(_df):\n",
    "    df = _df.copy()\n",
    "    df = df.sort_index(axis=1).sort_values(by=[\"category_tractce10\", \"Accident_Year\"])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.insert(0, \"category_tractce10\", df.pop(\"category_tractce10\"))\n",
    "    df[[\"tractce10\", \"category\"]] = df[\"category_tractce10\"].str.split(\"_\", 1, expand=True)\n",
    "    df.insert(1, \"tractce10\", df.pop(\"tractce10\"))\n",
    "    df.insert(3, \"category\", df.pop(\"category\"))\n",
    "    df = df.assign(cat_1=0, cat_2=0, cat_3=0, vision_zero=0)\n",
    "    df.loc[df[\"category\"] == 1, \"cat_1\"]=1\n",
    "    df.loc[df[\"category\"] == 2, \"cat_2\"]=1\n",
    "    df.loc[df[\"category\"] == 3, \"cat_3\"]=1\n",
    "    df.loc[df[\"Accident_Year\"] == 2016, \"Vision_Zero\"]=1\n",
    "    d= {}\n",
    "    d[\"SPD_ratio\"] = df[\"CSPD_1\"].divide(df[\"SPEED\"])\n",
    "    d[\"PUDO_pct_Tot_TNC_VMT\"] = df[\"PUDO\"].divide(df[\"Tot_TNC_VMT\"])\n",
    "    d[\"PUDO_pct_Tot_TNC_VMT_yr\"] = df[\"PUDO_yr\"].divide(df[\"Tot_TNC_VMT_yr\"])\n",
    "    d[\"COUNT_Fatal_and_Injury\"] = df[\"COUNT_Fatal\"] + df[\"COUNT_Visible_Injury\"] + df[\"COUNT_Severe_Injury\"]+ df[\"COUNT_Other_Injury\"]\n",
    "    d[\"TotCrash_permile\"] = df[\"Total_Crash\"].divide(df[\"Length_miles\"])\n",
    "    d[\"Tot_FatalInj_permile\"] = d[\"COUNT_Fatal_and_Injury\"].divide(df[\"Length_miles\"])\n",
    "    d[\"Tot_TNC_VMT_permile\"] = df[\"Tot_TNC_VMT\"].divide(df[\"Length_miles\"])\n",
    "    d[\"log_Tot_TNC_VMT_permile\"] = np.log(d[\"Tot_TNC_VMT_permile\"]+1)\n",
    "    d[\"Tot_Non_TNC_VMT_permile\"] = df[\"Tot_Non_TNC_VMT\"].divide(df[\"Length_miles\"])\n",
    "    d[\"log_Tot_Non_TNC_VMT_permile\"] = np.log(d[\"Tot_Non_TNC_VMT_permile\"]+1)\n",
    "    d[\"PUDO_permile\"] = df[\"PUDO\"].divide(df[\"Length_miles\"])\n",
    "    d[\"log_PUDO_permile\"] = np.log(d[\"PUDO_permile\"]+1)\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "dfmerged_mod = add_custom_fields(dfmerged)\n",
    "dfmerged_mod.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "dfmerged_mod.fillna(0,inplace=True)\n",
    "\n",
    "# dfmerged = dfmerged.sort_index(axis=1).sort_values(by=[\"category_tractce10\",\"Accident_Year\"])\n",
    "# dfmerged.reset_index(drop=True,inplace=True)\n",
    "# dfmerged.insert(0,\"category_tractce10\",dfmerged.pop(\"category_tractce10\"))\n",
    "# dfmerged[[\"tractce10\", \"category\"]] = dfmerged[\"category_tractce10\"].str.split(\"_\",1,expand=True)\n",
    "# dfmerged.insert(1,\"tractce10\",dfmerged.pop(\"tractce10\"))\n",
    "# dfmerged.insert(3,\"category\",dfmerged.pop(\"category\"))\n",
    "# dfmerged = dfmerged.assign(cat_1=0,cat_2=0,cat_3=0,vision_zero=0)\n",
    "# dfmerged.loc[dfmerged[\"category\"]==1,\"cat_1\"]=1\n",
    "# dfmerged.loc[dfmerged[\"category\"]==2,\"cat_1\"]=2\n",
    "# dfmerged.loc[dfmerged[\"category\"]==3,\"cat_1\"]=3\n",
    "# dfmerged.loc[dfmerged[\"Accident_Year\"]==2016,\"vision_zero\"]=1\n",
    "# dfmerged[\"SPD_ratio\"] = dfmerged[\"CSPD_1\"].divide(dfmerged[\"SPEED\"])\n",
    "# dfmerged[\"PUDO_pct_Tot_TNC_VMT\"] = dfmerged[\"PUDO\"].divide(dfmerged[\"Tot_TNC_VMT\"])\n",
    "dfmerged_mod.to_csv(BASE_DIR.parent.joinpath(folder_path,\"SF_merged_CAT_CT.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# Plot graphs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR.parent.joinpath(folder_path,\"SF_merged_CAT_CT.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "cols= [ 'CSPD_1',  'NUMBER_INJURED', 'NUMBER_KILLED', 'OOS', 'PUDO', 'SPEED', 'TIME', 'TIME_1', 'Tot_CAP', 'Tot_Non_TNC_VMT', 'Tot_Non_TNC_Vol','Tot_TNC_VMT', 'Tot_TNC_Vol', 'Tot_VMT', 'Tot_Vol',  'Total_Crash', 'SPD_ratio', 'PUDO_pct_Tot_TNC_VMT', 'COUNT_Fatal_and_Injury']\n",
    "\n",
    "df =df[cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpairplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcols\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\seaborn\\_decorators.py:46\u001B[0m, in \u001B[0;36m_deprecate_positional_args.<locals>.inner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     36\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPass the following variable\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m as \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124mkeyword arg\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFrom version 0.12, the only valid positional argument \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m\n\u001B[0;32m     44\u001B[0m     )\n\u001B[0;32m     45\u001B[0m kwargs\u001B[38;5;241m.\u001B[39mupdate({k: arg \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args)})\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\seaborn\\axisgrid.py:2126\u001B[0m, in \u001B[0;36mpairplot\u001B[1;34m(data, hue, hue_order, palette, vars, x_vars, y_vars, kind, diag_kind, markers, height, aspect, corner, dropna, plot_kws, diag_kws, grid_kws, size)\u001B[0m\n\u001B[0;32m   2124\u001B[0m diag_kws\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlegend\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   2125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m diag_kind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhist\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 2126\u001B[0m     grid\u001B[38;5;241m.\u001B[39mmap_diag(histplot, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdiag_kws)\n\u001B[0;32m   2127\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m diag_kind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkde\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   2128\u001B[0m     diag_kws\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfill\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\seaborn\\axisgrid.py:1478\u001B[0m, in \u001B[0;36mPairGrid.map_diag\u001B[1;34m(self, func, **kwargs)\u001B[0m\n\u001B[0;32m   1476\u001B[0m     plot_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhue_order\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hue_order)\n\u001B[0;32m   1477\u001B[0m     plot_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpalette\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_orig_palette)\n\u001B[1;32m-> 1478\u001B[0m     func(x\u001B[38;5;241m=\u001B[39mvector, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mplot_kwargs)\n\u001B[0;32m   1479\u001B[0m     ax\u001B[38;5;241m.\u001B[39mlegend_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1481\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_add_axis_labels()\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\seaborn\\distributions.py:1462\u001B[0m, in \u001B[0;36mhistplot\u001B[1;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001B[0m\n\u001B[0;32m   1451\u001B[0m estimate_kws \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\n\u001B[0;32m   1452\u001B[0m     stat\u001B[38;5;241m=\u001B[39mstat,\n\u001B[0;32m   1453\u001B[0m     bins\u001B[38;5;241m=\u001B[39mbins,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1457\u001B[0m     cumulative\u001B[38;5;241m=\u001B[39mcumulative,\n\u001B[0;32m   1458\u001B[0m )\n\u001B[0;32m   1460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39munivariate:\n\u001B[1;32m-> 1462\u001B[0m     p\u001B[38;5;241m.\u001B[39mplot_univariate_histogram(\n\u001B[0;32m   1463\u001B[0m         multiple\u001B[38;5;241m=\u001B[39mmultiple,\n\u001B[0;32m   1464\u001B[0m         element\u001B[38;5;241m=\u001B[39melement,\n\u001B[0;32m   1465\u001B[0m         fill\u001B[38;5;241m=\u001B[39mfill,\n\u001B[0;32m   1466\u001B[0m         shrink\u001B[38;5;241m=\u001B[39mshrink,\n\u001B[0;32m   1467\u001B[0m         common_norm\u001B[38;5;241m=\u001B[39mcommon_norm,\n\u001B[0;32m   1468\u001B[0m         common_bins\u001B[38;5;241m=\u001B[39mcommon_bins,\n\u001B[0;32m   1469\u001B[0m         kde\u001B[38;5;241m=\u001B[39mkde,\n\u001B[0;32m   1470\u001B[0m         kde_kws\u001B[38;5;241m=\u001B[39mkde_kws,\n\u001B[0;32m   1471\u001B[0m         color\u001B[38;5;241m=\u001B[39mcolor,\n\u001B[0;32m   1472\u001B[0m         legend\u001B[38;5;241m=\u001B[39mlegend,\n\u001B[0;32m   1473\u001B[0m         estimate_kws\u001B[38;5;241m=\u001B[39mestimate_kws,\n\u001B[0;32m   1474\u001B[0m         line_kws\u001B[38;5;241m=\u001B[39mline_kws,\n\u001B[0;32m   1475\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1476\u001B[0m     )\n\u001B[0;32m   1478\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1480\u001B[0m     p\u001B[38;5;241m.\u001B[39mplot_bivariate_histogram(\n\u001B[0;32m   1481\u001B[0m         common_bins\u001B[38;5;241m=\u001B[39mcommon_bins,\n\u001B[0;32m   1482\u001B[0m         common_norm\u001B[38;5;241m=\u001B[39mcommon_norm,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1492\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1493\u001B[0m     )\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\seaborn\\distributions.py:565\u001B[0m, in \u001B[0;36m_DistributionPlotter.plot_univariate_histogram\u001B[1;34m(self, multiple, element, fill, common_norm, common_bins, shrink, kde, kde_kws, color, legend, line_kws, estimate_kws, **plot_kws)\u001B[0m\n\u001B[0;32m    560\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m element \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbars\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    561\u001B[0m \n\u001B[0;32m    562\u001B[0m     \u001B[38;5;66;03m# Use matplotlib bar plotting\u001B[39;00m\n\u001B[0;32m    564\u001B[0m     plot_func \u001B[38;5;241m=\u001B[39m ax\u001B[38;5;241m.\u001B[39mbar \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_variable \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m ax\u001B[38;5;241m.\u001B[39mbarh\n\u001B[1;32m--> 565\u001B[0m     artists \u001B[38;5;241m=\u001B[39m plot_func(\n\u001B[0;32m    566\u001B[0m         hist[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124medges\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    567\u001B[0m         hist[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheights\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m-\u001B[39m bottom,\n\u001B[0;32m    568\u001B[0m         hist[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwidths\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    569\u001B[0m         bottom,\n\u001B[0;32m    570\u001B[0m         align\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124medge\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    571\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39martist_kws,\n\u001B[0;32m    572\u001B[0m     )\n\u001B[0;32m    573\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m bar \u001B[38;5;129;01min\u001B[39;00m artists:\n\u001B[0;32m    574\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_variable \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\matplotlib\\__init__.py:1412\u001B[0m, in \u001B[0;36m_preprocess_data.<locals>.inner\u001B[1;34m(ax, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1409\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m   1410\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(ax, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1411\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1412\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(ax, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mmap\u001B[39m(sanitize_sequence, args), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1414\u001B[0m     bound \u001B[38;5;241m=\u001B[39m new_sig\u001B[38;5;241m.\u001B[39mbind(ax, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1415\u001B[0m     auto_label \u001B[38;5;241m=\u001B[39m (bound\u001B[38;5;241m.\u001B[39marguments\u001B[38;5;241m.\u001B[39mget(label_namer)\n\u001B[0;32m   1416\u001B[0m                   \u001B[38;5;129;01mor\u001B[39;00m bound\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(label_namer))\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\matplotlib\\axes\\_axes.py:2409\u001B[0m, in \u001B[0;36mAxes.bar\u001B[1;34m(self, x, height, width, bottom, align, **kwargs)\u001B[0m\n\u001B[0;32m   2407\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m orientation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhorizontal\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m   2408\u001B[0m         r\u001B[38;5;241m.\u001B[39msticky_edges\u001B[38;5;241m.\u001B[39mx\u001B[38;5;241m.\u001B[39mappend(l)\n\u001B[1;32m-> 2409\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_patch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2410\u001B[0m     patches\u001B[38;5;241m.\u001B[39mappend(r)\n\u001B[0;32m   2412\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m xerr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m yerr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\matplotlib\\axes\\_base.py:2358\u001B[0m, in \u001B[0;36m_AxesBase.add_patch\u001B[1;34m(self, p)\u001B[0m\n\u001B[0;32m   2356\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mget_clip_path() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2357\u001B[0m     p\u001B[38;5;241m.\u001B[39mset_clip_path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch)\n\u001B[1;32m-> 2358\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_patch_limits\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2359\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_children\u001B[38;5;241m.\u001B[39mappend(p)\n\u001B[0;32m   2360\u001B[0m p\u001B[38;5;241m.\u001B[39m_remove_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_children\u001B[38;5;241m.\u001B[39mremove\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\matplotlib\\axes\\_base.py:2381\u001B[0m, in \u001B[0;36m_AxesBase._update_patch_limits\u001B[1;34m(self, patch)\u001B[0m\n\u001B[0;32m   2379\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m vertices\u001B[38;5;241m.\u001B[39msize:\n\u001B[0;32m   2380\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m-> 2381\u001B[0m patch_trf \u001B[38;5;241m=\u001B[39m \u001B[43mpatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2382\u001B[0m updatex, updatey \u001B[38;5;241m=\u001B[39m patch_trf\u001B[38;5;241m.\u001B[39mcontains_branch_seperately(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransData)\n\u001B[0;32m   2383\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (updatex \u001B[38;5;129;01mor\u001B[39;00m updatey):\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\matplotlib\\patches.py:278\u001B[0m, in \u001B[0;36mPatch.get_transform\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    277\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the `~.transforms.Transform` applied to the `Patch`.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_patch_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m artist\u001B[38;5;241m.\u001B[39mArtist\u001B[38;5;241m.\u001B[39mget_transform(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\matplotlib\\patches.py:752\u001B[0m, in \u001B[0;36mRectangle.get_patch_transform\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_patch_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;66;03m# Note: This cannot be called until after this has been added to\u001B[39;00m\n\u001B[0;32m    749\u001B[0m     \u001B[38;5;66;03m# an Axes, otherwise unit conversion will fail. This makes it very\u001B[39;00m\n\u001B[0;32m    750\u001B[0m     \u001B[38;5;66;03m# important to call the accessor method and not directly access the\u001B[39;00m\n\u001B[0;32m    751\u001B[0m     \u001B[38;5;66;03m# transformation member variable.\u001B[39;00m\n\u001B[1;32m--> 752\u001B[0m     bbox \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bbox\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    753\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (transforms\u001B[38;5;241m.\u001B[39mBboxTransformTo(bbox)\n\u001B[0;32m    754\u001B[0m             \u001B[38;5;241m+\u001B[39m transforms\u001B[38;5;241m.\u001B[39mAffine2D()\u001B[38;5;241m.\u001B[39mrotate_deg_around(\n\u001B[0;32m    755\u001B[0m                 bbox\u001B[38;5;241m.\u001B[39mx0, bbox\u001B[38;5;241m.\u001B[39my0, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mangle))\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\matplotlib\\patches.py:845\u001B[0m, in \u001B[0;36mRectangle.get_bbox\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    843\u001B[0m \u001B[38;5;124;03m\"\"\"Return the `.Bbox`.\"\"\"\u001B[39;00m\n\u001B[0;32m    844\u001B[0m x0, y0, x1, y1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_units()\n\u001B[1;32m--> 845\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBbox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_extents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my1\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\matplotlib\\transforms.py:839\u001B[0m, in \u001B[0;36mBbox.from_extents\u001B[1;34m(minpos, *args)\u001B[0m\n\u001B[0;32m    822\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_extents\u001B[39m(\u001B[38;5;241m*\u001B[39margs, minpos\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    825\u001B[0m \u001B[38;5;124;03m    Create a new Bbox from *left*, *bottom*, *right* and *top*.\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    837\u001B[0m \u001B[38;5;124;03m       scales where negative bounds result in floating point errors.\u001B[39;00m\n\u001B[0;32m    838\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 839\u001B[0m     bbox \u001B[38;5;241m=\u001B[39m \u001B[43mBbox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    840\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m minpos \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    841\u001B[0m         bbox\u001B[38;5;241m.\u001B[39m_minpos[:] \u001B[38;5;241m=\u001B[39m minpos\n",
      "File \u001B[1;32m~\\.virtualenvs\\python_virtual_env-DJ2NW_7b\\lib\\site-packages\\matplotlib\\transforms.py:785\u001B[0m, in \u001B[0;36mBbox.__init__\u001B[1;34m(self, points, **kwargs)\u001B[0m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ignore \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;66;03m# it is helpful in some contexts to know if the bbox is a\u001B[39;00m\n\u001B[0;32m    783\u001B[0m \u001B[38;5;66;03m# default or has been mutated; we store the orig points to\u001B[39;00m\n\u001B[0;32m    784\u001B[0m \u001B[38;5;66;03m# support the mutated methods\u001B[39;00m\n\u001B[1;32m--> 785\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_points_orig \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_points\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x000001FFD5D625E0> (for post_execute):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sns.pairplot(df[cols])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}