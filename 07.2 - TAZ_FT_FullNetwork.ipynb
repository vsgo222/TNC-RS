{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyproj import CRS\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "# set the working directory\n",
    "BASE_DIR = Path.cwd()\n",
    "# define the exported folder path\n",
    "# Check if folder exists\n",
    "folder_path = pathlib.Path(BASE_DIR.parent.joinpath(\"Exported_Files\",\"census_tract\",\"agg_network\",\"Feb242022\",\"CensusTract_FT_FullNetwork\"))\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "# print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Fetch SF Champ network both for year 2010, year 2016 and join the frames\n",
    "# \"\"\"\n",
    "# # Keep only FT types representing real road-network\n",
    "# def get_realnetwork(_df):\n",
    "#     \"\"\"\n",
    "#     Keep only FT types representing real road-network\n",
    "#     1: Fwy-Fwy Connector; 2: Freeway; 3: Expressway; 4: Collector; 5: Ramp; 6: Centroid Connector;\n",
    "#     7: Major Arterial; 8: ; 9: Alley (only for DTA); 10: ; 11: Local; 12: Minor Arterial; 13: Bike only;\n",
    "#     14: ; 15: Super Arterial\n",
    "#     :param _df:\n",
    "#     :return df:\n",
    "#     \"\"\"\n",
    "#     df = _df.copy()\n",
    "#     if isinstance(df,gpd.GeoDataFrame):\n",
    "#         if \"FT\" in df.columns:\n",
    "#             df = df.loc[df[\"FT\"].isin([1,2,3,4,5,7,9,10,11,12,13,15])]\n",
    "#     return df\n",
    "# # create a new field to store Time-of-Day information\n",
    "# def add_TOD_information(_df,TOD):\n",
    "#     \"\"\"\n",
    "#     before concatenating dataframes, insert the TOD information as column value\n",
    "#     :param _df:\n",
    "#     :param TOD:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     d = {}\n",
    "#     if isinstance(_df,gpd.GeoDataFrame):\n",
    "#         d[\"peak\"] = TOD\n",
    "#     return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "#\n",
    "# def clip_roadnetwork(_dfroadnetwork):\n",
    "#     \"\"\"\n",
    "#     Overlay and clip the line features to get only features inside SF County Area\n",
    "#     :param _dfroadnetwork:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     dfPolygon = gpd.read_file(BASE_DIR.parent.joinpath(\"Data\",\"SF_County\",\"SFBay_Boundary.shp\"))\n",
    "#     dfPolygon=dfPolygon.to_crs(\"EPSG:4326\")\n",
    "#     dfroadnetwork = _dfroadnetwork.to_crs(4326).copy()\n",
    "#     df = gpd.clip(dfroadnetwork,dfPolygon)\n",
    "#     return df\n",
    "#\n",
    "# def getTotalCapacity(_df):\n",
    "#     \"\"\"\n",
    "#     The \"CAP\" field is for per/hour; convert this to TOD period\n",
    "#     :param _df:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     df = _df.copy()\n",
    "#     df[\"Tot_CAP\"] = df[\"CAP\"]\n",
    "#     df.loc[(df[\"peak\"]==\"EV\"),\"Tot_CAP\"]*=8.5\n",
    "#     df.loc[(df[\"peak\"]==\"MD\"),\"Tot_CAP\"]*=6.5\n",
    "#     df.loc[(df[\"peak\"]==\"AM\") | (df[\"peak\"]==\"PM\") |(df[\"peak\"]==\"EA\") ,\"Tot_CAP\"]*=3\n",
    "#     return df\n",
    "#\n",
    "# def check_required_rdntwrk_colmns(_df):# road network\n",
    "#     df = _df.copy()\n",
    "#     d = {}\n",
    "#     reqd_colmns = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "#                    'V11_1', 'V12_1',\"V13_1\",'V14_1', 'V15_1',\"V16_1\",'V17_1', 'V18_1',\"V19_1\",\n",
    "#                    'OOS', 'PUDO','Tot_Vol',\"Tot_TNC_Vol\", \"Tot_Non_TNC_Vol\",\n",
    "#                    'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',\n",
    "#                    ]\n",
    "#     for col in reqd_colmns:\n",
    "#         if col not in df.columns:\n",
    "#             d[col]=0\n",
    "#     return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "#\n",
    "# def merge_TOD_dfs(_dfAM,_dfPM,_dfEA,_dfEV,_dfMD):\n",
    "#     # clean the dataframe\n",
    "#     dfAM = get_realnetwork(_dfAM.copy())\n",
    "#     dfPM = get_realnetwork(_dfPM.copy())\n",
    "#     dfEA = get_realnetwork(_dfEA.copy())\n",
    "#     dfEV = get_realnetwork(_dfEV.copy())\n",
    "#     dfMD = get_realnetwork(_dfMD.copy())\n",
    "#     # clip road network\n",
    "#     dfAM = clip_roadnetwork(dfAM.copy())\n",
    "#     dfPM = clip_roadnetwork(dfPM.copy())\n",
    "#     dfEA = clip_roadnetwork(dfEA.copy())\n",
    "#     dfEV = clip_roadnetwork(dfEV.copy())\n",
    "#     dfMD = clip_roadnetwork(dfMD.copy())\n",
    "#     # add TOD information\n",
    "#     dfAM = add_TOD_information(dfAM.copy(),\"AM\")\n",
    "#     dfPM = add_TOD_information(dfPM.copy(),\"PM\")\n",
    "#     dfEA = add_TOD_information(dfEA.copy(),\"EA\")\n",
    "#     dfEV = add_TOD_information(dfEV.copy(),\"EV\")\n",
    "#     dfMD = add_TOD_information(dfMD.copy(),\"MD\")\n",
    "#     # concat dataframes\n",
    "#     df = pd.concat([dfAM,dfPM,dfEA,dfEV,dfMD])\n",
    "#     df.reset_index(drop=True,inplace=True)\n",
    "#     # get total capacity on the link for the day\n",
    "#     df = getTotalCapacity(df.copy())\n",
    "#     df = check_required_rdntwrk_colmns(df.copy())\n",
    "#     # get Tot_Vol: by adding the columns which together form Tot_Vol\n",
    "#     add_for_Tot_Vol = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1',\n",
    "#                        'V13_1','V14_1','V15_1','V16_1','V17_1','V18_1','V19_1',\n",
    "#                        'BUSVOL_AM','BUSVOL_PM','BUSVOL_EA','BUSVOL_MD','BUSVOL_EV','OOS']\n",
    "#     df[\"Tot_Vol\"] = df[add_for_Tot_Vol].sum(axis=1)\n",
    "#     # get Tot_TNC_Vol: ['V13_1','OOS'] # V13_1 is TNC_Volumes plying on the road segment\n",
    "#     add_TNC_col = ['V13_1','OOS'] # V13_1 is TNC_Volumes plying on the road segment\n",
    "#     df[\"Tot_TNC_Vol\"] = df[add_TNC_col].sum(axis=1)\n",
    "#     return df\n",
    "#\n",
    "# def agg_roadnetwrk(_dfmerged):\n",
    "#     # aggregate the dataframe using A_B\n",
    "#     wt_avg = lambda x: np.ma.average(x, weights = _dfmerged.loc[x.index, \"Tot_Vol\"],axis=0)\n",
    "#\n",
    "#     # aggregate function\n",
    "#     def agg_func(_dfagg):\n",
    "#         lst_col = [\"SPEED\",\"TIME\",\"TIME_1\",\"CSPD_1\"]\n",
    "#         # average the columns\n",
    "#         avg_col = [ 'DISTANCE',\n",
    "#                     \"FT\",\"AT\",\n",
    "#                     'TIMESEED',\n",
    "#                     'LANE_AM', 'LANE_OP', 'LANE_PM', 'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "#                     'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3',\n",
    "#                     'TOLLEA_DA', 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA',\n",
    "#                     'TOLLEV_SR2', 'TOLLEV_SR3',\"USE\"]\n",
    "#         d = {}\n",
    "#         for col in _dfagg.select_dtypes(np.number).columns:\n",
    "#             if col in lst_col:\n",
    "#                 d[col] = wt_avg\n",
    "#             elif col in avg_col:\n",
    "#                 d[col]=\"mean\"\n",
    "#             else:\n",
    "#                 d[col] = \"sum\"\n",
    "#         for col in _dfagg.select_dtypes(object).columns:\n",
    "#             d[col] = \"first\"\n",
    "#         d[\"geometry\"] = \"first\"\n",
    "#         return d\n",
    "#\n",
    "#     _dfmerged[\"A_B\"] = _dfmerged[\"A\"].astype(str)  + \"_\" + _dfmerged[\"B\"].astype(str)\n",
    "#     _dfmerged[\"A\"] = _dfmerged[\"A\"].astype(str)\n",
    "#     _dfmerged[\"B\"] = _dfmerged[\"B\"].astype(str)\n",
    "#\n",
    "#     dfmerged_agg = _dfmerged.groupby(['A_B'],as_index=False).aggregate(agg_func(_dfmerged.copy())).copy()\n",
    "#\n",
    "#     return dfmerged_agg\n",
    "#\n",
    "# # Network for YR 2010\n",
    "# dfsfrd2010 = merge_TOD_dfs(gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_AM.shp\")),\n",
    "#                               gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_AM.shp\")),\n",
    "#                               gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_EA.shp\")),\n",
    "#                               gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_EV.shp\")),\n",
    "#                               gpd.read_file(BASE_DIR.parent.joinpath(\"2010\",\"2010_MD.shp\"))\n",
    "#                               )\n",
    "#\n",
    "# dfsfrd2010 = agg_roadnetwrk(dfsfrd2010)\n",
    "# # above merge converts the geo-dataframe to pandas dataframe. So re-convert it into geodataframe\n",
    "# dfsfrdntwrk2010_agg = gpd.GeoDataFrame(dfsfrd2010, geometry='geometry',crs=\"EPSG:4326\")\n",
    "# dfsfrdntwrk2010_agg = dfsfrdntwrk2010_agg.to_crs(\"EPSG:4326\")\n",
    "# dfsfrdntwrk2010_agg = dfsfrdntwrk2010_agg.to_crs(\"EPSG:3857\")\n",
    "# dfsfrdntwrk2010_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_PCS.geojson\"), driver='GeoJSON')\n",
    "#\n",
    "# # Network for YR 2016\n",
    "# dfsfrd2016 = merge_TOD_dfs(gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_AM.shp\")),\n",
    "#                            gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_AM.shp\")),\n",
    "#                            gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_EA.shp\")),\n",
    "#                            gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_EV.shp\")),\n",
    "#                            gpd.read_file(BASE_DIR.parent.joinpath(\"2016\",\"2016_MD.shp\"))\n",
    "#                            )\n",
    "#\n",
    "# dfsfrd2016 = agg_roadnetwrk(dfsfrd2016)\n",
    "# # above merge converts the geo-dataframe to pandas dataframe. So re-convert it into geodataframe\n",
    "# dfsfrdntwrk2016_agg = gpd.GeoDataFrame(dfsfrd2016, geometry='geometry',crs=\"EPSG:4326\")\n",
    "# dfsfrdntwrk2016_agg = dfsfrdntwrk2016_agg.to_crs(\"EPSG:4326\")\n",
    "# dfsfrdntwrk2016_agg = dfsfrdntwrk2016_agg.to_crs(\"EPSG:3857\")\n",
    "# dfsfrdntwrk2016_agg.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_PCS.geojson\"), driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\geopandas\\io\\file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "# # read the files\n",
    "dfsfrdntwrk2010_agg = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_PCS.geojson\"))\n",
    "dfsfrdntwrk2016_agg = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_PCS.geojson\"))\n",
    "\n",
    "# intersect the road network with CensusTract file\n",
    "def intersect_CT_rdntwrk(_gpdCT, _gpdrdntwrkagg):\n",
    "    _gpdCT.to_crs(3857, inplace=True) # set its projection to EPSG:3857\n",
    "    # _gpdrdntwrkagg.to_crs(3857,inplace=True)\n",
    "    res_intersection = _gpdrdntwrkagg.overlay(_gpdCT, how='intersection')\n",
    "    return res_intersection\n",
    "\n",
    "def drop_intersect_CT_rdntwrk_columns(_df):\n",
    "    df = _df.copy()\n",
    "    cols = ['TOLL', 'USE', 'AT','LANE_AM', 'LANE_OP', 'LANE_PM',\n",
    "            'BUSLANE_AM', 'BUSLANE_OP', 'BUSLANE_PM',\n",
    "            'TOLLAM_DA', 'TOLLAM_SR2', 'TOLLAM_SR3', 'TOLLPM_DA', 'TOLLPM_SR2', 'TOLLPM_SR3', 'TOLLEA_DA', 'TOLLEA_SR2', 'TOLLEA_SR3', 'TOLLMD_DA', 'TOLLMD_SR2', 'TOLLMD_SR3', 'TOLLEV_DA', 'TOLLEV_SR2', 'TOLLEV_SR3',\n",
    "            'VALUETOLL_', 'PASSTHRU',\n",
    "            'BUSTPS_AM', 'BUSTPS_OP', 'BUSTPS_PM', 'TSVA', 'BIKE_CLASS', 'PER_RISE', 'ONEWAY', 'DTA_EDIT_F', 'TOLLTIME', 'PHASE',\n",
    "            'AMBUSSAVE', 'MDBUSSAVE', 'PMBUSSAVE', 'EVBUSSAVE', 'EABUSSAVE',\n",
    "            'SPDC', 'CAPC', 'A', 'B', 'STREETNAME', 'TYPE', 'MTYPE', 'TSIN', 'PROJ', 'ACTION', 'AB', 'peak', 'statefp10', 'mtfcc10', 'name10', 'intptlat10', 'awater10', 'namelsad10', 'funcstat10', 'aland10', 'geoid10', 'intptlon10', 'countyfp10',\n",
    "            \"V1T_1\",'V2T_1', 'V3T_1',  'V4T_1', 'V5T_1','V6T_1', 'V7T_1','V8T_1', 'V9T_1',  'V10T_1', 'V11T_1', 'V12T_1', 'V13T_1', 'V14T_1','V15T_1', 'V16T_1', 'V17T_1','V18T_1', 'V19T_1', ]\n",
    "    df = df.drop([x for x in cols if x in df.columns], axis=1)\n",
    "    return df\n",
    "\n",
    "gdfsfct = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SF_CensusTract_PCS.geojson\"))\n",
    "\n",
    "gdfsfrd2010ct_int = intersect_CT_rdntwrk(gdfsfct,dfsfrdntwrk2010_agg)\n",
    "gdfsfrd2010ct_int = drop_intersect_CT_rdntwrk_columns(gdfsfrd2010ct_int)\n",
    "gdfsfrd2010ct_int.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_CT_PCS.geojson\"), driver='GeoJSON')\n",
    "\n",
    "gdfsfrd2016ct_int = intersect_CT_rdntwrk(gdfsfct,dfsfrdntwrk2016_agg)\n",
    "gdfsfrd2016ct_int =drop_intersect_CT_rdntwrk_columns(gdfsfrd2016ct_int)\n",
    "gdfsfrd2016ct_int.to_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_CT_PCS.geojson\"), driver='GeoJSON')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# Check in QGIS if the re-projection is successful .i.e.\n",
    "# 1. Both SFChamp_2010_agg.geojson and SFChamp_2016_agg.geojson into EPSG:3857, are named with _PCS suffix\n",
    "# 2. The SF_CensusTract to EPSG:3857, is also name with _PCS suffix\n",
    "\n",
    "# After the above process do the following in QGIS\n",
    "# 3. Road Crash for each year i.e. 2010 and 2016, convert it to EPSG:3857, name it SFCrash_2010_PCS.geojson & SFCrash_2016_PCS.geojson\n",
    "# 4. Perform spatial intersection:\n",
    "# Intersect with SF_CensusTract and RoadNetwork and name the output as SFChamp_201x_agg_CT_PCS.geojson\n",
    "# Intersect road crashes with SF_Census Tract and name the output as SFCrash_201x_CT_PCS.geojson\n",
    "# for both, keep \"tractce\" column from SF_Census Tract in the output file\n",
    "# This ends QGIS manipulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nWe are grouping the crashes at two-levels\\n1. censustract/taz level\\n2. depending upon facility type\\n\\nSo,\\nthe road crashes should be aggregated on\\n1. find the nearest link to which the crash could be attached (fld: A_B and D2NL<10)\\n2. create a unique identifier using  censustract_ID & FT of roadnetwork: tractce10_FT\\n3. aggregate all the road crashes attached to tractce10_FT\\n4. aggregate all road network attached to tractce10_FT\\n'"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We are grouping the crashes at two-levels\n",
    "1. censustract/taz level\n",
    "2. depending upon facility type\n",
    "\n",
    "So,\n",
    "the road crashes should be aggregated on\n",
    "1. find the nearest link to which the crash could be attached (fld: A_B and D2NL<10)\n",
    "2. create a unique identifier using  censustract_ID & FT of roadnetwork: tractce10_FT\n",
    "3. aggregate all the road crashes attached to tractce10_FT\n",
    "4. aggregate all road network attached to tractce10_FT\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# modify the dataframe to create a new column \"CATEGORY\" using \"FacilityType\"\n",
    "def label_df_by_road_category(_df,fld):\n",
    "    _df[\"category\"]=0\n",
    "    _df.loc[_df[fld].isin([1, 2, 3, 5,13]),'category']=1\n",
    "    _df.loc[_df[fld].isin([4,7,12,15]),'category']=2\n",
    "    _df.loc[_df[fld].isin([9,11 ]),'category']=3\n",
    "    return _df\n",
    "\n",
    "def add_unique_ID_using_CT_CAT(_df,CT_ID_fld,CAT_fld):\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[f\"{CAT_fld}_{CT_ID_fld}\"] = _df[CT_ID_fld].astype(str) + \"_\" + _df[CAT_fld].astype(int).astype(str)\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "# Fetch SF_Census Tract\n",
    "SF_CT = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SF_CensusTract_PCS.geojson\"), crs = \"EPSG:3857\")\n",
    "SF_CT = SF_CT.to_crs(3857)\n",
    "\n",
    "# read the merged road network files and containing CensusTract IDs\n",
    "gdfsfrd2010ct_int= gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2010_CT_PCS.geojson\"))\n",
    "gdfsfrd2016ct_int= gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"SFChamp_2016_CT_PCS.geojson\"))\n",
    "\n",
    "# categorize the road network by FacilityType\n",
    "gdfsfrd2010ct = label_df_by_road_category(gdfsfrd2010ct_int.copy(),\"FT\")\n",
    "gdfsfrd2016ct = label_df_by_road_category(gdfsfrd2016ct_int.copy(),\"FT\")\n",
    "\n",
    "gdfsfrd2010ct_cat = add_unique_ID_using_CT_CAT(gdfsfrd2010ct.copy(),\"tractce10\",\"category\")\n",
    "gdfsfrd2010ct_cat.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "gdfsfrd2010ct_cat.fillna(0, inplace=True)\n",
    "\n",
    "gdfsfrd2016ct_cat = add_unique_ID_using_CT_CAT(gdfsfrd2016ct.copy(),\"tractce10\",\"category\")\n",
    "gdfsfrd2016ct_cat.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "gdfsfrd2016ct_cat.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# read the (Nearest Neighbour) road crash files and containing CensusTract IDs\n",
    "gdfsfnncrash2010ct = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"NN_SFCrash_2010_CT_PCS.geojson\"))\n",
    "gdfsfnncrash2016ct = gpd.read_file(BASE_DIR.parent.joinpath(folder_path,\"NN_SFCrash_2016_CT_PCS.geojson\"))\n",
    "\n",
    "# categorize the road crash by FacilityType\n",
    "gdfsfrdcrash2010ct_cat = label_df_by_road_category(gdfsfnncrash2010ct.copy(),\"FT\")\n",
    "gdfsfrdcrash2016ct_cat = label_df_by_road_category(gdfsfnncrash2016ct.copy(),\"FT\")\n",
    "\n",
    "# drop tractce10 and rename join_tractce10\n",
    "gdfsfrdcrash2010ct_cat = gdfsfrdcrash2010ct_cat.loc[:,gdfsfrdcrash2010ct_cat.columns!=\"tractce10\"].copy()\n",
    "gdfsfrdcrash2010ct_cat.rename(columns={\"join_tractce10\":\"tractce10\"},inplace=True)\n",
    "gdfsfrdcrash2016ct_cat = gdfsfrdcrash2016ct_cat.loc[:,gdfsfrdcrash2016ct_cat.columns!=\"tractce10\"].copy()\n",
    "gdfsfrdcrash2016ct_cat.rename(columns={\"join_tractce10\":\"tractce10\"},inplace=True)\n",
    "\n",
    "def drop_crash_columns(_df):\n",
    "    df = _df.copy()\n",
    "    cols = ['ACCIDENT_YEAR', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'REPORTING_DISTRICT', 'DAY_OF_WEEK', 'CHP_SHIFT', 'POPULATION', 'CNTY_CITY_LOC', 'SPECIAL_COND', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'CITY_DIVISION_LAPD', 'CHP_BEAT_CLASS', 'BEAT_NUMBER', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'WEATHER_1', 'WEATHER_2', 'STATE_HWY_IND', 'CALTRANS_COUNTY', 'CALTRANS_DISTRICT', 'STATE_ROUTE', 'ROUTE_SUFFIX', 'POSTMILE_PREFIX', 'POSTMILE', 'LOCATION_TYPE', 'RAMP_INTERSECTION', 'SIDE_OF_HWY', 'TOW_AWAY', 'COLLISION_SEVERITY',  'PARTY_COUNT', 'PRIMARY_COLL_FACTOR', 'PCF_CODE_OF_VIOL', 'PCF_VIOL_CATEGORY', 'PCF_VIOLATION', 'PCF_VIOL_SUBSECTION', 'HIT_AND_RUN', 'TYPE_OF_COLLISION', 'MVIW', 'PED_ACTION', 'ROAD_SURFACE', 'ROAD_COND_1', 'ROAD_COND_2', 'LIGHTING', 'CONTROL_DEVICE', 'CHP_ROAD_TYPE', 'PEDESTRIAN_ACCIDENT', 'BICYCLE_ACCIDENT', 'MOTORCYCLE_ACCIDENT', 'TRUCK_ACCIDENT', 'NOT_PRIVATE_PROPERTY', 'ALCOHOL_INVOLVED', 'STWD_VEHTYPE_AT_FAULT', 'CHP_VEHTYPE_AT_FAULT', 'PRIMARY_RAMP', 'SECONDARY_RAMP', 'LATITUDE', 'LONGITUDE', 'COUNTY', 'CITY', 'POINT_X', 'POINT_Y', 'PRIMARY_RD_3', 'SECONDARY_RD_3', 'tractce10', 'FT',\"CASE_ID\"]\n",
    "    df = df.drop([x for x in cols if x in df.columns], axis=1)\n",
    "    return df\n",
    "\n",
    "gdfsfrdcrash2010ct_cat = drop_crash_columns(add_unique_ID_using_CT_CAT(gdfsfrdcrash2010ct_cat,\"tractce10\",\"category\"))\n",
    "gdfsfrdcrash2016ct_cat = drop_crash_columns(add_unique_ID_using_CT_CAT(gdfsfrdcrash2016ct_cat,\"tractce10\",\"category\"))\n",
    "\n",
    "def min_D2NL(_df,dist):\n",
    "    df = _df.loc[_df[\"D2NL\"]<dist,:]\n",
    "    return df\n",
    "\n",
    "gdfsfrdcrash2010ct_cat = min_D2NL(gdfsfrdcrash2010ct_cat.copy(),10)\n",
    "gdfsfrdcrash2016ct_cat = min_D2NL(gdfsfrdcrash2016ct_cat.copy(),10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\numpy\\ma\\extras.py:623: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n",
      "C:\\Users\\Goyal\\.virtualenvs\\Overpass_Turbo-ZwXzihL_\\lib\\site-packages\\numpy\\ma\\extras.py:623: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n"
     ]
    }
   ],
   "source": [
    "# aggregate road network by unqiueID: category_tractce10\n",
    "def reqd_colmns(_df):# road network\n",
    "    df = _df.copy()\n",
    "    d = {}\n",
    "    reqd_colmns = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1', 'V11_1', 'V12_1', 'V13_1', 'V14_1', 'V15_1', 'V16_1', 'V17_1', 'V18_1', 'V19_1',\n",
    "                   'OOS', 'PUDO',\n",
    "                   'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',\n",
    "                   'Tot_CAP', 'CAP', 'SPEED','CSPD_1',\n",
    "                   'Tot_Vol', 'Tot_TNC_Vol', 'Tot_Non_TNC_Vol', 'TNC_Tot_Vol'\n",
    "                   ]\n",
    "    for col in reqd_colmns:\n",
    "        if col not in df.columns:\n",
    "            d[col]=0\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "def get_req_fields(_df):\n",
    "    fields = [\"Tot_Vol\",\"Tot_TNC_Vol\", \"Tot_Non_TNC_Vol\", \"Tot_VMT\",\"Tot_TNC_VMT\",\"Tot_Non_TNC_VMT\", \"Congested_Speed\",'CSPD_1',\"SPEED\",\"PUDO\",\"OOS\"]\n",
    "    d = {}\n",
    "    for fld in fields:\n",
    "        if fld == \"Tot_Vol\":# TNC Tot Vol\n",
    "            cols = ['V1_1', 'V2_1', 'V3_1', 'V4_1', 'V5_1', 'V6_1', 'V7_1', 'V8_1', 'V9_1', 'V10_1',\n",
    "                    'V11_1', 'V12_1', 'V13_1', 'V14_1', 'V15_1', 'V16_1', 'V17_1', 'V18_1', 'V19_1',\n",
    "                    \"OOS\",\n",
    "                    'BUSVOL_AM', 'BUSVOL_AM', 'BUSVOL_MD', 'BUSVOL_PM', 'BUSVOL_EV', 'BUSVOL_EA',]\n",
    "            d[fld] = _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_TNC_Vol\":# TNC Tot Vol\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_Vol\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = d[\"Tot_Vol\"] - _df[cols].sum(axis=1)\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_VMT\":\n",
    "            d[fld] = d[\"Tot_Vol\"]*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (_df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Tot_Non_TNC_VMT\":\n",
    "            cols = ['V13_1',\"OOS\"]\n",
    "            d[fld] = (d[\"Tot_Vol\"] - _df[cols].sum(axis=1))*_df[\"Length_meters\"]*0.000621371\n",
    "            d[f\"{fld}_mil\"] = d[fld].divide(1000000)\n",
    "            d[f\"{fld}_yr\"] = d[fld]*365\n",
    "            d[f\"{fld}_mil_yr\"] =  d[f\"{fld}_yr\"].divide(1000000)\n",
    "            d[f\"log_{fld}\"] = np.log(d[fld]+1)\n",
    "            d[f\"log_{fld}_mil\"] = np.log(d[f\"{fld}_mil\"]+1)\n",
    "            d[f\"log_{fld}_mil_yr\"] = np.log(d[f\"{fld}_mil_yr\"]+1)\n",
    "        elif fld == \"Congested_Speed\":\n",
    "            d[\"Congested_Speed\"] = (((_df[\"Length_meters\"]*0.000621371).divide(_df[\"TIME_1\"]))*60)\n",
    "            d[\"Congested_Speed_yr\"] = d[\"Congested_Speed\"]\n",
    "        elif fld == \"CSPD_1\":\n",
    "            d[\"CSPD_1_yr\"] = _df[\"CSPD_1\"]\n",
    "        elif fld == \"SPEED\":\n",
    "            d[\"SPEED_yr\"] = _df[\"SPEED\"]\n",
    "        elif fld == \"PUDO\":\n",
    "            d[\"PUDO_yr\"] = (_df[\"PUDO\"]*365)\n",
    "            d[\"PUDO_thousands\"] = _df[\"PUDO\"].divide(1000)\n",
    "            d[\"PUDO_thousands_yr\"] = d[\"PUDO_thousands\"]*365\n",
    "            d[\"PUDO_mil\"] = _df[\"PUDO\"].divide(1000000)\n",
    "            d[\"PUDO_mil_yr\"] = d[\"PUDO_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_PUDO\"] = np.log(_df[\"PUDO\"]+1)\n",
    "            d[\"log_PUDO_yr\"] = np.log(d[\"PUDO_yr\"]+1)\n",
    "            d[\"log_PUDO_thousands\"] = np.log(d[\"PUDO_thousands\"]+1)\n",
    "            d[\"log_PUDO_thousands_yr\"] = np.log(d[\"PUDO_thousands_yr\"]+1)\n",
    "            d[\"log_PUDO_mil\"] = np.log(d[\"PUDO_mil\"]+1)\n",
    "            d[\"log_PUDO_mil_yr\"] = np.log(d[\"PUDO_mil_yr\"]+1)\n",
    "        elif fld == \"OOS\":\n",
    "            d[\"OOS_yr\"] = (_df[\"OOS\"]*365)\n",
    "            d[\"OOS_thousands\"] = _df[\"OOS\"].divide(1000)\n",
    "            d[\"OOS_thousands_yr\"] = d[\"OOS_thousands\"]*365\n",
    "            d[\"OOS_mil\"] = _df[\"OOS\"].divide(1000000)\n",
    "            d[\"OOS_mil_yr\"] = d[\"OOS_yr\"].divide(1000000)\n",
    "\n",
    "            d[\"log_OOS\"] = np.log(_df[\"OOS\"]+1)\n",
    "            d[\"log_OOS_yr\"] = np.log(d[\"OOS_yr\"]+1)\n",
    "            d[\"log_OOS_thousands\"] = np.log(d[\"OOS_thousands\"]+1)\n",
    "            d[\"log_OOS_thousands_yr\"] = np.log(d[\"OOS_thousands_yr\"]+1)\n",
    "            d[\"log_OOS_mil\"] = np.log(d[\"OOS_mil\"]+1)\n",
    "            d[\"log_OOS_mil_yr\"] = np.log(d[\"OOS_mil_yr\"]+1)\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "def agg_network_by_uniqueIDs(_df,uniqueID):\n",
    "    _df = reqd_colmns(_df.copy())\n",
    "    _df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    _df.fillna(0, inplace=True)\n",
    "    _df[\"FT\"] = _df[\"FT\"].astype(str)\n",
    "    _df[uniqueID] = _df[uniqueID].astype(str)\n",
    "    # aggregate the dataframe using A_B\n",
    "    wt_avg = lambda x: np.ma.average(x, weights = _df.loc[x.index, \"Tot_Vol\"])\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "    def agg_func_rdntwrk(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            if col in wt_col:\n",
    "                d[col] = wt_avg\n",
    "            else:\n",
    "                d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            elif col==\"FT\":\n",
    "                d[col] = concat_agg\n",
    "        d[\"geometry\"] = \"first\"\n",
    "        return d\n",
    "\n",
    "    wt_col = [\"SPEED\",\"TIME\",\"CSPD_1\", 'VDT_1', 'VHT_1','VC_1', ]\n",
    "    str_col = ['category_tractce10',\"FT\"]\n",
    "    drop_col = [ \"A_B\",'tractce10','category',]\n",
    "\n",
    "    _df.drop(drop_col,axis=1,inplace=True)\n",
    "    df = _df.groupby([uniqueID],as_index=False).aggregate(agg_func_rdntwrk(_df.copy())).copy()\n",
    "\n",
    "    return get_req_fields(df)\n",
    "\n",
    "def add_length_columns(_df,_yr):\n",
    "    #remember to rename DISTANCE variable (as this is no longer the actual distance (in miles), given that feature is split-up)\n",
    "    d = {}\n",
    "    if isinstance(_df,gpd.GeoDataFrame):\n",
    "        d[\"Length_meters\"] = _df.geometry.length\n",
    "        d[\"Length_miles\"] = d[\"Length_meters\"]* 0.000621371\n",
    "        d[\"Year\"] = _yr\n",
    "    return pd.concat([_df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "gdfsfrd2010ct_cat_agg = agg_network_by_uniqueIDs(add_length_columns(gdfsfrd2010ct_cat.copy(),2010),\"category_tractce10\")\n",
    "gdfsfrd2016ct_cat_agg = agg_network_by_uniqueIDs(add_length_columns(gdfsfrd2016ct_cat.copy(),2016),\"category_tractce10\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# aggregate crashes by uniqueID: category_tractce10\n",
    "def agg_crash_by_uniqueIDs(_df,uniqueID):\n",
    "    _df[uniqueID] = _df[uniqueID].astype(str)\n",
    "    str_col = [uniqueID]\n",
    "    # Aggregating rows based on one column with “, ”.join\n",
    "    concat_agg = lambda ar: ', '.join([item for item in ar if item])\n",
    "    def agg_func(df):\n",
    "        d = {}\n",
    "        for col in df.select_dtypes(np.number).columns:\n",
    "            d[col] = \"sum\"\n",
    "        for col in df.select_dtypes(object).columns:\n",
    "            if col in str_col:\n",
    "                d[col] = \"first\"\n",
    "            else:\n",
    "                d[col] = concat_agg\n",
    "        return d\n",
    "    df = _df.groupby([uniqueID],as_index=False).aggregate(agg_func(_df.copy())).copy()\n",
    "    return df\n",
    "\n",
    "gdfsfrdcrash2010ct_cat_agg = agg_crash_by_uniqueIDs(gdfsfrdcrash2010ct_cat,\"category_tractce10\")\n",
    "gdfsfrdcrash2010ct_cat_agg[\"Accident_Year\"] = 2010\n",
    "gdfsfrdcrash2016ct_cat_agg = agg_crash_by_uniqueIDs(gdfsfrdcrash2016ct_cat,\"category_tractce10\")\n",
    "gdfsfrdcrash2016ct_cat_agg[\"Accident_Year\"] = 2016"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goyal\\AppData\\Local\\Temp\\ipykernel_18568\\1867483328.py:32: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  dfmerged =pd.concat([df2010[columns_to_retain], df2016[columns_to_retain]], ignore_index=True,verify_integrity=True,copy=True,axis=0)\n",
      "C:\\Users\\Goyal\\AppData\\Local\\Temp\\ipykernel_18568\\1867483328.py:32: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  dfmerged =pd.concat([df2010[columns_to_retain], df2016[columns_to_retain]], ignore_index=True,verify_integrity=True,copy=True,axis=0)\n"
     ]
    }
   ],
   "source": [
    "def dataframe_merge(_dfroadnetwork, _dfroadcrash,left_fld,right_fld):\n",
    "    dfmerge = pd.merge(_dfroadnetwork,_dfroadcrash,left_on=left_fld,right_on=right_fld,how=\"left\")\n",
    "    return dfmerge\n",
    "\n",
    "# def update_fields(_df,_yr):\n",
    "#     df = _df.copy()\n",
    "#     if isinstance(df,pd.DataFrame):\n",
    "#         df[\"Accident_Year\"] = _yr\n",
    "#         df[\"Crash_Year\"] = _yr\n",
    "#         d = {**dict.fromkeys(df.select_dtypes(np.number).columns, 0),\n",
    "#              **dict.fromkeys(df.select_dtypes(exclude=np.number).columns,np.nan)}\n",
    "#         df.fillna(d, inplace=True)\n",
    "#     return pd.concat([df, pd.DataFrame(d, index=_df.index)],axis=1)\n",
    "\n",
    "dfsf_rd_ntwrk_crash_2010_cat_agg = dataframe_merge(gdfsfrd2010ct_cat_agg,gdfsfrdcrash2010ct_cat_agg,\"category_tractce10\",\"category_tractce10\")\n",
    "d = {**dict.fromkeys(dfsf_rd_ntwrk_crash_2010_cat_agg.select_dtypes(np.number).columns, 0),\n",
    "     **dict.fromkeys(dfsf_rd_ntwrk_crash_2010_cat_agg.select_dtypes(exclude=np.number).columns,np.nan)}\n",
    "dfsf_rd_ntwrk_crash_2010_cat_agg =dfsf_rd_ntwrk_crash_2010_cat_agg.fillna(d)\n",
    "dfsf_rd_ntwrk_crash_2010_cat_agg[\"Accident_Year\"]=2010\n",
    "dfsf_rd_ntwrk_crash_2016_cat_agg = dataframe_merge(gdfsfrd2016ct_cat_agg,gdfsfrdcrash2016ct_cat_agg,\"category_tractce10\",\"category_tractce10\")\n",
    "d = {**dict.fromkeys(dfsf_rd_ntwrk_crash_2016_cat_agg.select_dtypes(np.number).columns, 0),\n",
    "     **dict.fromkeys(dfsf_rd_ntwrk_crash_2016_cat_agg.select_dtypes(exclude=np.number).columns,np.nan)}\n",
    "dfsf_rd_ntwrk_crash_2016_cat_agg =dfsf_rd_ntwrk_crash_2016_cat_agg.fillna(d)\n",
    "dfsf_rd_ntwrk_crash_2016_cat_agg[\"Accident_Year\"]=2016\n",
    "\n",
    "\n",
    "df2010 = dfsf_rd_ntwrk_crash_2010_cat_agg.loc[:,~dfsf_rd_ntwrk_crash_2010_cat_agg.columns.isin([\"geometry\"])].copy()\n",
    "df2016 = dfsf_rd_ntwrk_crash_2016_cat_agg.loc[:,~dfsf_rd_ntwrk_crash_2016_cat_agg.columns.isin([\"geometry\"])].copy()\n",
    "df2010.reset_index(drop=True, inplace=True)\n",
    "df2016.reset_index(drop=True, inplace=True)\n",
    "columns_to_retain = set(df2010.columns.to_list()).intersection(set(df2016.columns.to_list()))\n",
    "dfmerged =pd.concat([df2010[columns_to_retain], df2016[columns_to_retain]], ignore_index=True,verify_integrity=True,copy=True,axis=0)\n",
    "# few cosmetic changes to read the dataframe better\n",
    "def add_custom_fields(_df):\n",
    "    df = _df.copy()\n",
    "    df = df.sort_index(axis=1).sort_values(by=[\"category_tractce10\", \"Accident_Year\"])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.insert(0, \"category_tractce10\", df.pop(\"category_tractce10\"))\n",
    "    df[[\"tractce10\", \"category\"]] = df[\"category_tractce10\"].str.split(\"_\", 1, expand=True)\n",
    "    df.insert(1, \"tractce10\", df.pop(\"tractce10\"))\n",
    "    df.insert(3, \"category\", df.pop(\"category\"))\n",
    "    df = df.assign(cat_1=0, cat_2=0, cat_3=0, vision_zero=0)\n",
    "    df.loc[df[\"category\"] == 1, \"cat_1\"]=1\n",
    "    df.loc[df[\"category\"] == 2, \"cat_2\"]=1\n",
    "    df.loc[df[\"category\"] == 3, \"cat_3\"]=1\n",
    "    df.loc[df[\"Accident_Year\"] == 2016, \"Vision_Zero\"]=1\n",
    "    d= {}\n",
    "    d[\"SPD_ratio\"] = df[\"CSPD_1\"].divide(df[\"SPEED\"])\n",
    "    d[\"PUDO_pct_Tot_TNC_VMT\"] = df[\"PUDO\"].divide(df[\"Tot_TNC_VMT\"])\n",
    "    d[\"PUDO_pct_Tot_TNC_VMT_yr\"] = df[\"PUDO_yr\"].divide(df[\"Tot_TNC_VMT_yr\"])\n",
    "    d[\"COUNT_Fatal_and_Injury\"] = df[\"COUNT_Fatal\"] + df[\"COUNT_Visible_Injury\"] + df[\"COUNT_Severe_Injury\"]+ df[\"COUNT_Other_Injury\"]\n",
    "    d[\"TotCrash_permile\"] = df[\"Total_Crash\"].divide(df[\"Length_miles\"])\n",
    "    d[\"Tot_FatalInj_permile\"] = d[\"COUNT_Fatal_and_Injury\"].divide(df[\"Length_miles\"])\n",
    "    d[\"Tot_TNC_VMT_permile\"] = df[\"Tot_TNC_VMT\"].divide(df[\"Length_miles\"])\n",
    "    d[\"log_Tot_TNC_VMT_permile\"] = np.log(d[\"Tot_TNC_VMT_permile\"]+1)\n",
    "    d[\"Tot_Non_TNC_VMT_permile\"] = df[\"Tot_Non_TNC_VMT\"].divide(df[\"Length_miles\"])\n",
    "    d[\"log_Tot_Non_TNC_VMT_permile\"] = np.log(d[\"Tot_Non_TNC_VMT_permile\"]+1)\n",
    "    d[\"PUDO_permile\"] = df[\"PUDO\"].divide(df[\"Length_miles\"])\n",
    "    d[\"log_PUDO_permile\"] = np.log(d[\"PUDO_permile\"]+1)\n",
    "    return pd.concat([df, pd.DataFrame(d, index=df.index)],axis=1)\n",
    "\n",
    "dfmerged_mod = add_custom_fields(dfmerged)\n",
    "dfmerged_mod.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "dfmerged_mod.fillna(0,inplace=True)\n",
    "\n",
    "# dfmerged = dfmerged.sort_index(axis=1).sort_values(by=[\"category_tractce10\",\"Accident_Year\"])\n",
    "# dfmerged.reset_index(drop=True,inplace=True)\n",
    "# dfmerged.insert(0,\"category_tractce10\",dfmerged.pop(\"category_tractce10\"))\n",
    "# dfmerged[[\"tractce10\", \"category\"]] = dfmerged[\"category_tractce10\"].str.split(\"_\",1,expand=True)\n",
    "# dfmerged.insert(1,\"tractce10\",dfmerged.pop(\"tractce10\"))\n",
    "# dfmerged.insert(3,\"category\",dfmerged.pop(\"category\"))\n",
    "# dfmerged = dfmerged.assign(cat_1=0,cat_2=0,cat_3=0,vision_zero=0)\n",
    "# dfmerged.loc[dfmerged[\"category\"]==1,\"cat_1\"]=1\n",
    "# dfmerged.loc[dfmerged[\"category\"]==2,\"cat_1\"]=2\n",
    "# dfmerged.loc[dfmerged[\"category\"]==3,\"cat_1\"]=3\n",
    "# dfmerged.loc[dfmerged[\"Accident_Year\"]==2016,\"vision_zero\"]=1\n",
    "# dfmerged[\"SPD_ratio\"] = dfmerged[\"CSPD_1\"].divide(dfmerged[\"SPEED\"])\n",
    "# dfmerged[\"PUDO_pct_Tot_TNC_VMT\"] = dfmerged[\"PUDO\"].divide(dfmerged[\"Tot_TNC_VMT\"])\n",
    "dfmerged_mod.to_csv(BASE_DIR.parent.joinpath(folder_path,\"SF_merged_CAT_CT.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "dfmerged.insert(0,\"category_tractce10\",dfmerged.pop(\"category_tractce10\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "    category_tractce10  Accident_Year  BUSVOL_AM  BUSVOL_EA  BUSVOL_EV  \\\n0             010100_2           2010   11514.75     2993.6    11424.0   \n1             010100_2           2016   11514.75     2993.6    11424.0   \n2             010100_3           2010    1680.00      723.6     4250.0   \n3             010100_3           2016    1680.00      723.6     4250.0   \n4             010200_1           2010       0.00        0.0        0.0   \n..                 ...            ...        ...        ...        ...   \n901           980900_1           2016     697.50        0.0        0.0   \n902           980900_2           2010    8822.50     5738.6    14492.5   \n903           980900_2           2016    8822.50     5738.6    14492.5   \n904           980900_3           2010    4215.00     3484.4     6332.5   \n905           980900_3           2016    4215.00     3484.4     6332.5   \n\n     BUSVOL_MD  BUSVOL_PM     CAP  COUNT_BICYCLIST_INJURED  \\\n0      17123.0    13249.2  373500                      9.0   \n1      17123.0    13249.2  385500                     13.0   \n2       4463.2     2220.0  159000                      1.0   \n3       4463.2     2220.0  159000                      0.0   \n4          0.0        0.0     990                      0.0   \n..         ...        ...     ...                      ...   \n901        0.0        0.0  113960                      0.0   \n902    17972.5     8790.0  651250                      6.0   \n903    17972.5     8790.0  651250                      2.0   \n904     8287.5     4105.0  538750                      6.0   \n905     8287.5     4105.0  538750                      7.0   \n\n     COUNT_BICYCLIST_KILLED  ...  log_Tot_TNC_VMT_mil_yr  log_Tot_TNC_Vol  \\\n0                       0.0  ...                0.000000         0.000000   \n1                       0.0  ...                5.564499        11.197776   \n2                       0.0  ...                0.000000         0.000000   \n3                       0.0  ...                3.551347         9.663567   \n4                       0.0  ...                0.000000         0.000000   \n..                      ...  ...                     ...              ...   \n901                     0.0  ...                3.980938        10.431795   \n902                     0.0  ...                0.000000         0.000000   \n903                     0.0  ...                5.627785        10.995623   \n904                     0.0  ...                0.000000         0.000000   \n905                     0.0  ...                5.456614         9.890819   \n\n     log_Tot_TNC_Vol_mil  log_Tot_TNC_Vol_mil_yr  log_Tot_VMT  \\\n0               0.000000                0.000000    15.143741   \n1               0.070428                3.319009    15.410032   \n2               0.000000                0.000000    13.270761   \n3               0.015610                1.908427    13.403585   \n4               0.000000                0.000000     0.000000   \n..                   ...                     ...          ...   \n901             0.033358                2.593826    14.164770   \n902             0.000000                0.000000    15.942779   \n903             0.057902                3.124928    16.230872   \n904             0.000000                0.000000    15.719732   \n905             0.019555                2.105077    15.955437   \n\n     log_Tot_VMT_mil  log_Tot_VMT_mil_yr  log_Tot_Vol  log_Tot_Vol_mil  \\\n0           1.563259            7.228853    12.913454         0.340559   \n1           1.779345            7.494975    13.131523         0.408527   \n2           0.457416            5.359858    11.496418         0.093820   \n3           0.508246            5.492098    11.629241         0.106461   \n4           0.000000            0.000000     0.000000         0.000000   \n..               ...                 ...          ...              ...   \n901         0.882948            6.251087    12.718832         0.288165   \n902         2.239849            8.027492    13.398592         0.506259   \n903         2.500929            8.315504    13.686685         0.630807   \n904         2.043060            7.804526    12.242554         0.188494   \n905         2.251167            8.040146    12.478258         0.233145   \n\n     log_Tot_Vol_mil_yr  \n0              5.004568  \n1              5.221323  \n2              3.608267  \n3              3.737716  \n4              0.000000  \n..                  ...  \n901            4.811385  \n902            5.487126  \n903            5.774182  \n904            4.340057  \n905            4.573022  \n\n[906 rows x 136 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category_tractce10</th>\n      <th>Accident_Year</th>\n      <th>BUSVOL_AM</th>\n      <th>BUSVOL_EA</th>\n      <th>BUSVOL_EV</th>\n      <th>BUSVOL_MD</th>\n      <th>BUSVOL_PM</th>\n      <th>CAP</th>\n      <th>COUNT_BICYCLIST_INJURED</th>\n      <th>COUNT_BICYCLIST_KILLED</th>\n      <th>...</th>\n      <th>log_Tot_TNC_VMT_mil_yr</th>\n      <th>log_Tot_TNC_Vol</th>\n      <th>log_Tot_TNC_Vol_mil</th>\n      <th>log_Tot_TNC_Vol_mil_yr</th>\n      <th>log_Tot_VMT</th>\n      <th>log_Tot_VMT_mil</th>\n      <th>log_Tot_VMT_mil_yr</th>\n      <th>log_Tot_Vol</th>\n      <th>log_Tot_Vol_mil</th>\n      <th>log_Tot_Vol_mil_yr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>010100_2</td>\n      <td>2010</td>\n      <td>11514.75</td>\n      <td>2993.6</td>\n      <td>11424.0</td>\n      <td>17123.0</td>\n      <td>13249.2</td>\n      <td>373500</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>15.143741</td>\n      <td>1.563259</td>\n      <td>7.228853</td>\n      <td>12.913454</td>\n      <td>0.340559</td>\n      <td>5.004568</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>010100_2</td>\n      <td>2016</td>\n      <td>11514.75</td>\n      <td>2993.6</td>\n      <td>11424.0</td>\n      <td>17123.0</td>\n      <td>13249.2</td>\n      <td>385500</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5.564499</td>\n      <td>11.197776</td>\n      <td>0.070428</td>\n      <td>3.319009</td>\n      <td>15.410032</td>\n      <td>1.779345</td>\n      <td>7.494975</td>\n      <td>13.131523</td>\n      <td>0.408527</td>\n      <td>5.221323</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>010100_3</td>\n      <td>2010</td>\n      <td>1680.00</td>\n      <td>723.6</td>\n      <td>4250.0</td>\n      <td>4463.2</td>\n      <td>2220.0</td>\n      <td>159000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>13.270761</td>\n      <td>0.457416</td>\n      <td>5.359858</td>\n      <td>11.496418</td>\n      <td>0.093820</td>\n      <td>3.608267</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>010100_3</td>\n      <td>2016</td>\n      <td>1680.00</td>\n      <td>723.6</td>\n      <td>4250.0</td>\n      <td>4463.2</td>\n      <td>2220.0</td>\n      <td>159000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>3.551347</td>\n      <td>9.663567</td>\n      <td>0.015610</td>\n      <td>1.908427</td>\n      <td>13.403585</td>\n      <td>0.508246</td>\n      <td>5.492098</td>\n      <td>11.629241</td>\n      <td>0.106461</td>\n      <td>3.737716</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>010200_1</td>\n      <td>2010</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>990</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>901</th>\n      <td>980900_1</td>\n      <td>2016</td>\n      <td>697.50</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>113960</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>3.980938</td>\n      <td>10.431795</td>\n      <td>0.033358</td>\n      <td>2.593826</td>\n      <td>14.164770</td>\n      <td>0.882948</td>\n      <td>6.251087</td>\n      <td>12.718832</td>\n      <td>0.288165</td>\n      <td>4.811385</td>\n    </tr>\n    <tr>\n      <th>902</th>\n      <td>980900_2</td>\n      <td>2010</td>\n      <td>8822.50</td>\n      <td>5738.6</td>\n      <td>14492.5</td>\n      <td>17972.5</td>\n      <td>8790.0</td>\n      <td>651250</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>15.942779</td>\n      <td>2.239849</td>\n      <td>8.027492</td>\n      <td>13.398592</td>\n      <td>0.506259</td>\n      <td>5.487126</td>\n    </tr>\n    <tr>\n      <th>903</th>\n      <td>980900_2</td>\n      <td>2016</td>\n      <td>8822.50</td>\n      <td>5738.6</td>\n      <td>14492.5</td>\n      <td>17972.5</td>\n      <td>8790.0</td>\n      <td>651250</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5.627785</td>\n      <td>10.995623</td>\n      <td>0.057902</td>\n      <td>3.124928</td>\n      <td>16.230872</td>\n      <td>2.500929</td>\n      <td>8.315504</td>\n      <td>13.686685</td>\n      <td>0.630807</td>\n      <td>5.774182</td>\n    </tr>\n    <tr>\n      <th>904</th>\n      <td>980900_3</td>\n      <td>2010</td>\n      <td>4215.00</td>\n      <td>3484.4</td>\n      <td>6332.5</td>\n      <td>8287.5</td>\n      <td>4105.0</td>\n      <td>538750</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>15.719732</td>\n      <td>2.043060</td>\n      <td>7.804526</td>\n      <td>12.242554</td>\n      <td>0.188494</td>\n      <td>4.340057</td>\n    </tr>\n    <tr>\n      <th>905</th>\n      <td>980900_3</td>\n      <td>2016</td>\n      <td>4215.00</td>\n      <td>3484.4</td>\n      <td>6332.5</td>\n      <td>8287.5</td>\n      <td>4105.0</td>\n      <td>538750</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5.456614</td>\n      <td>9.890819</td>\n      <td>0.019555</td>\n      <td>2.105077</td>\n      <td>15.955437</td>\n      <td>2.251167</td>\n      <td>8.040146</td>\n      <td>12.478258</td>\n      <td>0.233145</td>\n      <td>4.573022</td>\n    </tr>\n  </tbody>\n</table>\n<p>906 rows × 136 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmerged = dfmerged.sort_index(axis=1).sort_values(by=[\"category_tractce10\",\"Accident_Year\"])\n",
    "dfmerged.reset_index(drop=True,inplace=True)\n",
    "dfmerged"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "dfmerged[[\"tractce10\", \"category\"]] = dfmerged[\"category_tractce10\"].str.split(\"_\",1,expand=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "dfmerged.insert(1,\"tractce10\",dfmerged.pop(\"tractce10\"))\n",
    "dfmerged.insert(3,\"category\",dfmerged.pop(\"category\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfmerged = dfmerged.assign(cat_1=0,cat_2=0,cat_3=0,vision_zero=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}